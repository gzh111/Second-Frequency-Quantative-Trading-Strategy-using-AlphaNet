{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8325518a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_ENABLE_ONEDNN_OPTS=0\n"
     ]
    }
   ],
   "source": [
    "%env TF_ENABLE_ONEDNN_OPTS 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f7ec64-69af-4cc9-884c-f61023d07132",
   "metadata": {
    "executionInfo": {
     "elapsed": 2863,
     "status": "ok",
     "timestamp": 1666274947208,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "75f7ec64-69af-4cc9-884c-f61023d07132",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c70190e4-1903-41ed-8897-040f6808a020",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274947208,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "c70190e4-1903-41ed-8897-040f6808a020",
    "outputId": "248fa78f-5e9d-4db2-d134-2762d69323b5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HRKbR8ZA9tIu",
   "metadata": {
    "id": "HRKbR8ZA9tIu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DSA5205 Project\n",
    "\n",
    "Ref:\n",
    "- [AlphaNet：因子挖掘神经网络 - 华泰证券](https://crm.htsc.com.cn/doc/2020/10750101/74856806-a2e3-41cb-be4c-695dc6cc1341.pdf)\n",
    "- [再探AlphaNet：结构和特征优化 - 华泰证券](https://crm.htsc.com.cn/doc/2020/10750101/74619658-f648-4001-a255-5b78174b073a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9f103-0a81-4cc1-8e8c-226d40f546cf",
   "metadata": {
    "id": "31b9f103-0a81-4cc1-8e8c-226d40f546cf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe3037f-3941-4da6-87ff-c5fffc8ac6de",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947208,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "8fe3037f-3941-4da6-87ff-c5fffc8ac6de",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#combination c^2_N\n",
    "def generate(N):\n",
    "    col = []\n",
    "    col_rev = []\n",
    "    for i in range(1,N):\n",
    "        for j in range(0,i):\n",
    "            col.append([i,j])\n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98797082-b05f-4949-97e3-02b52bb03d79",
   "metadata": {
    "id": "98797082-b05f-4949-97e3-02b52bb03d79",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51408d77-3d75-4d0c-b64e-850a12d322e1",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947208,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "51408d77-3d75-4d0c-b64e-850a12d322e1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_corr(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    H = Matrix.shape[1]\n",
    "    num=generate(H)\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*len(num)*2*stride\n",
    "        data1 = tf.gather(Matrix,axis=1,indices=num)[:,:,:,start_index:end_index]\n",
    "        mean1 = tf.reduce_mean(data1, axis=-1)\n",
    "        spread1 = data1 - tf.expand_dims(mean1, axis=-1)\n",
    "        #N*len(num)*2*1\n",
    "        std=(tf.reduce_sum(spread1*spread1,axis=-1, keepdims=True)/(spread1.shape[-1]-1))**0.5\n",
    "        #N*len(num)*1\n",
    "        cov=tf.reduce_sum(tf.reduce_prod(spread1, axis=-2), axis=-1, keepdims=True)/(data1.shape[-1]-1)\n",
    "        corr=cov/tf.reduce_prod(std,axis=-2)\n",
    "\n",
    "        l.append(corr)\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*len(num)*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f42aa7-a80f-44fe-9186-fbb2f9f21b1b",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947208,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "36f42aa7-a80f-44fe-9186-fbb2f9f21b1b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_cov(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    H = Matrix.shape[1]\n",
    "    num=generate(H)\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*len(num)*2*stride\n",
    "        data1 = tf.gather(Matrix,axis=1,indices=num)[:,:,:,start_index:end_index]\n",
    "        mean1 = tf.reduce_mean(data1, axis=-1)\n",
    "        spread1 = data1 - tf.expand_dims(mean1, axis=-1)\n",
    "        #N*len(num)*1\n",
    "        cov=tf.reduce_sum(tf.reduce_prod(spread1, axis=-2), axis=-1, keepdims=True)/(data1.shape[-1]-1)\n",
    "\n",
    "        l.append(cov)\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*len(num)*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qI2v4XeD9qiK",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947208,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "qI2v4XeD9qiK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_stdde(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*stride\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        mean1 = tf.reduce_mean(data1, axis=-1)\n",
    "        spread1 = data1 - tf.expand_dims(mean1, axis=-1)\n",
    "        #N*H*1\n",
    "        std=(tf.reduce_sum(spread1*spread1,axis=-1, keepdims=True)/(spread1.shape[-1]-1))**0.5\n",
    "\n",
    "        l.append(std)\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "LjC9Q6Wf9qiK",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947209,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "LjC9Q6Wf9qiK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_zscore(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*stride\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        mean1 = tf.reduce_mean(data1, axis=-1)\n",
    "        spread1 = data1 - tf.expand_dims(mean1, axis=-1)\n",
    "        #N*H*1\n",
    "        std=(tf.reduce_sum(spread1*spread1,axis=-1, keepdims=True)/(spread1.shape[-1]-1))**0.5\n",
    "\n",
    "        l.append(tf.expand_dims(mean1, axis=-1)/std)\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1xqDKFSK9qiL",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947209,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "1xqDKFSK9qiL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_return(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*1\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        return_ = (data1[:,:,-1]-data1[:,:,0])/(data1[:,:,0]+0.0001)-1\n",
    "        l.append(tf.expand_dims(return_, axis=-1))\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2CvdvxDV9qiL",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947209,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "2CvdvxDV9qiL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_decaylinear(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*stride\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        range_ = end_index-start_index\n",
    "        weight = tf.range(1,range_+1,dtype=Matrix.dtype)\n",
    "        weight = weight/tf.reduce_sum(weight)\n",
    "        #N*H*1\n",
    "        wd = tf.reduce_sum(data1*weight, axis=-1, keepdims=True)\n",
    "        l.append(wd)\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "jpFyYUOZ9qiL",
   "metadata": {
    "executionInfo": {
     "elapsed": 1216,
     "status": "ok",
     "timestamp": 1666274948422,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "jpFyYUOZ9qiL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_min(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*stride\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        #N*H*1\n",
    "        l.append(tf.reduce_min(data1,axis=-1, keepdims=True))\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vLzWttfT9qiM",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666274948423,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "vLzWttfT9qiM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_max(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*stride\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        #N*H*1\n",
    "        l.append(tf.reduce_max(data1,axis=-1, keepdims=True))\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Yq9APQKB9qiM",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274948423,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "Yq9APQKB9qiM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_mean(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*stride\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        #N*H*1\n",
    "        l.append(tf.reduce_mean(data1,axis=-1, keepdims=True))\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7lv5tBBV9qiM",
   "metadata": {
    "collapsed": false,
    "id": "7lv5tBBV9qiM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.1 check the output of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "HhoWAOWE9qiM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666274948424,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "HhoWAOWE9qiM",
    "outputId": "6da160ed-fa8c-4587-d707-ab10f73217b3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGdCAYAAAA1/PiZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/QklEQVR4nO3df3Bc9X3/+9fZlSUtjqVIq0SykPAPITVN7MDguJaXUgJCJr78aEsmQDzTcb6TOEkzRLEFk8YlHQwNEPJDooqbcGNoQpK6JvcmZDoNaREigcAiYhxnakhmJEsmlsCKr1ay14B+oN3P/UM6x7vS2siyzlnt0fMxs4N292jP8Rkjvfz5vD/vj2WMMQIAAMgBgWxfAAAAwGwRXAAAQM4guAAAgJxBcAEAADmD4AIAAHIGwQUAAOQMggsAAMgZBBcAAJAz8rJ9AecrmUzq9ddf17Jly2RZVrYvBwAAzIIxRqdOnVJlZaUCgdmPo+R8cHn99ddVXV2d7csAAABz0NfXp6qqqlkfn/PBZdmyZZIm/+BFRUVZvhoAADAb8Xhc1dXVzu/x2cr54GJPDxUVFRFcAADIMeda5kFxLgAAyBlzDi7PPvusbrjhBlVWVsqyLP3sZz9Le/8Tn/iELMtKe9TX16cdMzY2ps9//vMqKyvT0qVLdeONN6q/v3+ulwQAAHxuzsHlzTff1CWXXKLdu3ef8ZiPfOQjOnbsmPN44okn0t7fvn27Hn/8ce3bt0/PPfec3njjDV1//fVKJBJzvSwAAOBjc65x2bx5szZv3nzWYwoKClRRUZHxvZMnT+qRRx7RD3/4Q11zzTWSpB/96Eeqrq7WU089pWuvvXaulwYAAHzK1RqXX/3qV3rve9+ruro6bdu2TcePH3feO3DggN5++21t2rTJea2yslJr1qxRNBo942eOjY0pHo+nPQAAwOLgWnDZvHmz/v3f/11PP/20vvnNb2r//v26+uqrNTY2JkkaGBhQfn6+SkpK0r6vvLxcAwMDZ/zc+++/X8XFxc6DHi4AACweri2HvuWWW5yv16xZow996ENasWKFfv7zn+umm2464/cZY866NGrnzp1qbm52ntvrwAEAgP95thx6+fLlWrFihbq7uyVJFRUVGh8f1/DwcNpxx48fV3l5+Rk/p6CgwOnZQu8WAAAWF8+CSywWU19fn5YvXy5JWrdunZYsWaL29nbnmGPHjunll19WJBLx6rIAAEAOmfNU0RtvvKHDhw87z48cOaLf/e53Ki0tVWlpqXbt2qWPfvSjWr58uV599VX94z/+o8rKyvS3f/u3kqTi4mJ98pOf1O23365wOKzS0lLdcccdWrt2rbPKCMDi1drepWDAUlND7Yz32jq6lUga7Wisy8KVAcimOQeXl156SVdddZXz3K472bp1q77zne/o0KFD+sEPfqATJ05o+fLluuqqq/TYY4+l7UnQ2tqqvLw83XzzzRoZGVFDQ4O+//3vKxgMnscfCYAfBAOWWtq7JCktvLR1dKulvUvNhBZgUbKMMSbbF3E+4vG4iouLdfLkSepdAB9IHWlJDSlNDbXasqdT0Z6Y8xxA7prr7++c32QRgL9kGmlpae/SvzzVrYQxitSECS3AIkZwAbCgpIYV+7kdWoKWpb3b6s/27QB8juACYMHJNNIStCwljFFbRzcjLsAi5tlyaAA4F00NtU5YCVqWeu7/v9TcWKeW9i61dXRn+/IAZAkjLgAWpC17OpUwRpY0Y6Slpb1Lnb0xrV9ZypJoYJEhuABYcNo6uhXtiam6JKS+4RFFasJpNS+dvTFFe2KqXx3O8pUC8BrBBcCCMn0JtP3cDi92aGFJNLA4EVwALCiJpEkLJanTQ0HLIrQAixwN6ADkhLo7f6HxRFL5wYC67t2c7csBcJ7m+vubVUUAFry2jm4ntIwnkqwqAhYxpooALGhnqnmRxHQRsAgRXAAsWNNDi5S5sy6AxYPgAmDBml6oa7OfJ5I5XaIHYA4ozgUAAJ6jOBcAAPgewQUAAOQMggsAAMgZBBcAAJAzCC4AckJre9cZG8+1dXSrdWp5NAB/I7gAyAnBgKWW9i7d+t0X0gKM3eslGLAIMMAiQB8XADkhtfFcZ++Q87rdoG761wD8ieACIOtap0ZMMnXBbevoViJptCOlEV1Le5fTOXd6aKGTLuBvTBUByDp7Gmh6DUvqNJCtqaFW+cHTP7p2P32Y0AIsIoy4AMi6TPsPZdqnSJq5U7T9NaEFWBwILgAWhNTwsvvpwxpPJDOGluk1LZI0nkiqraOb8AIsAkwVAVgw7GmgTKMomUJLc2Nd2vMzLZcG4B+MuABYMKZPA6WOotg7RUuZC3GjPYNpU00A/IngAmBBmF7TYj+P9gwqUlOmHVOhpXXaMakrjuznAPyL4AIg6zIV4mbq29LUUOsEmOlTR6nfA8C/CC4Ass6eBpoePOzn06eBzrTiCID/EVwALCh2M7pE0jj9WzasCitSU6aW9i61tnfJSKpfXTqjeNeeNgLgXwQXAFlnN6BL/bqqJKT+4RFJk91xoz2DkqRMFSxb9nQq2hOj3T+wCLAcGkDWNTXUqrmxLq2Nvx1aJKmzN5a2P5EkvTb1vh1aIjVhpo2ARcAyxuR0CX48HldxcbFOnjypoqKibF8OgPNg167Yy6HPpKgwT/HRCVmaHIGJ1IS1d1u9Z9cJ4PzN9fc3Iy4AFozUBnRBy1LQsmYcE6kJKz46Ien0tNH6laUeXiWAbCK4AFgwUhvQJYxRYpYDwvtfHXrngwD4AsW5ABYEe5qoqiSki0ovULQnlvG46a9Xl4QU7Yk57f5ZWQT4G8EFQFa1tndp/6tDToFttCem/uERVZeE1JdSoJtJUWGe+oZHFKkJpxX2AvAvpooAZFUwYDmhZe+2etWvnqxXeafQIklrLiyeWio9OQozvbcLAP8huADIKnsptD3ds+/TG1VdEprV90Z7Ytr99GFJk6Fl36c3unmpABYApooAZF3qvkS7nz6s8UTSWeosKe1raXJlUf3qyekhu5iX0AIsDoy4AFgQpi+FTg0q09cWRXti+vFLfc7z8UTSKc4F4G+MuABYEOyl0JZ0xmXQqSMv/cMjTiHu9E0YAfgXwQVA1tlLoe1VRbaCvIDGJk530J058jKYNkVEeAH8j+ACIKvs0NLcWKdE0ihpjALW5Eqj1NBiy88LqHxZgfqGR9Q/PJL2/ZGaMiWSOb2LCYB3QHABkFWJpFFzY13aKElbR3fGBnSWpPGJpNO7JWmME1oYZQEWB4ILgKzK1OU2kTSqX12q/UeG0+pdUsdSOntjShoRWoBFhlVFABacHY11ClhWWmiZvuFi0kgBi3oWYLEhuABYcG797gtpU0VFhXlKGKPpe0UnjVgGDSwyTBUBWFDaOrrV2Tu523OkJqy+obec9v+pU0XLCvO09sJiZyWRdHqDxbaObjZbBHyKERcAC4pdrGtvA5Bpz6LqkpBOjU4o2hNTdUlILe1damnvUjBgOauMgoHp4zMA/IDgAmBB2TFVbJtIGlVl2LPI0uQGjFUlIS2b2h1akrM5o73KSJrceRqAvxBcACwIre1dTr1Ka3uX9r86pP4Moy2pnXNPjU44r//2jyfSQgujLoA/UeMCYEF48UjMqW0JBqyMfVyKCvMUTwkr0uRqo4QxzmaLkujtAvgYIy4AFoRITZmk0237M00TTQ8t0/c1Gk8kCS2AzzHiAmBBsIOGXWg7G8tSRmBSN2AE4F8EFwBZ1zpVj5IaXt6JPW1UXRJS3/CIjE5PG7HZIuBfBBcAWWUX4kZ7Yor2DCpgza6g9v2VRXpteMRZVRSpCWv9ylIFA5Za2rsU7RkkuAA+RI0LgKyyC3EjNWF19g6dsSh3uhd7h9KWQu/dVq9gwFIiaZzPoqsu4D8EFwBZ1dRQq6p3hzIGFtv0olzpdD1LpCasfZ/e6DSes0dvIjVhJZJUvQB+w1QRgKy7KHyB+k/M7NmSSUFeQGMTSed5tCemv3zgafUPjyhSE1a0J8aqIsDHGHEBkHV7t9VrWYbpoExSQ4utf3hElkRoARYBRlwAeC51FZH99btDS9I64Z4ro6m+LkwPAb5GcAHgOXvlz/Svp08DzUZq/xYjaf+rQ/N3oQAWnDlPFT377LO64YYbVFlZKcuy9LOf/SztfWOMdu3apcrKSoVCIX34wx/WK6+8knbM2NiYPv/5z6usrExLly7VjTfeqP7+/rleEoAc0dRQq+bGOrW0d6mz93RR7rmGFmkyrOQHJ5dQV5dMFvmymgjwrzkHlzfffFOXXHKJdu/enfH9r33ta2ppadHu3bu1f/9+VVRUqLGxUadOnXKO2b59ux5//HHt27dPzz33nN544w1df/31SiQSc70sADnCDi/RnpiCs+zdcibjickl0H1TBbotKRs2AvCXOU8Vbd68WZs3b874njFGDz74oO68807ddNNNkqRHH31U5eXl2rt3rz7zmc/o5MmTeuSRR/TDH/5Q11xzjSTpRz/6kaqrq/XUU0/p2muvneulAcgBdm1LfjCg8cS5j7RMt35lqepXTy6Btv8LwH9cWVV05MgRDQwMaNOmTc5rBQUFuvLKKxWNRiVJBw4c0Ntvv512TGVlpdasWeMck8nY2Jji8XjaA0DusWtbxhNJnd94y6TvPX/EmXZqaqjVjsY65722jm61znL/IwALmyvBZWBgQJJUXl6e9np5ebnz3sDAgPLz81VSUnLGYzK5//77VVxc7Dyqq6vn+eoBeO3CDDtBn4ugNdmkLtoT0/5Xh9KCit2YLhiYj3gEINtc7eNiTZu3NsbMeG26dzpm586dOnnypPPo6+ubl2sF4B07TDQ31ilSE1b/8Oyaz53J0oLTs97Rnpha2rv04pFY2nno7QL4gyvBpaKiQpJmjJwcP37cGYWpqKjQ+Pi4hoeHz3hMJgUFBSoqKkp7AMgtiaRxwsT6laXnXZxrbwlQnTJys//IMKEF8CFXgsuqVatUUVGh9vZ257Xx8XE988wzikQikqR169ZpyZIlacccO3ZML7/8snMMAP9JbT4nTda6JIw57/BiSc6mi5akhDHKDwYILYDPzHlV0RtvvKHDhw87z48cOaLf/e53Ki0t1UUXXaTt27frvvvuU21trWpra3Xffffpggsu0JYtWyRJxcXF+uQnP6nbb79d4XBYpaWluuOOO7R27VpnlREA/0ltOBftGVRn75Czx9D5sNcQVZeE1Dc84qxW2rKnU+tXlqYV6wLIXXMOLi+99JKuuuoq53lzc7MkaevWrfr+97+vL37xixoZGdHnPvc5DQ8Pa8OGDXryySe1bNky53taW1uVl5enm2++WSMjI2poaND3v/99BYPB8/gjAVjI7BGQlpRVPn1Db83b5/cNjzjTQ1v2dJ53IAKwsFjGmJxudhCPx1VcXKyTJ09S7wLkkFu/+4I6e0+35w9aUmKefho1T42utLR3sWM0sEDN9fc3exUByIp9n96oujt/4TSfm6/QIp0ezbHDSltHNw3pAJ8guADIiraObo0nkgpYkhuZwtLpaSlGWgD/ILgA8Iy9ouix/Uf12onRtCmd+WTvGN3W0U1oAXyG4ALAM6kriiSpszem+tXheT1HpCY82Rsm5VyEF8A/XO2cCwCpmhpqVb+6VNLksmW7y+18aW6s095t9ZO9Yaaa3LFTNOAvjLgA8NS+T290WvHPt4d/3ev0hkldRURhLuAfLIcGkBWpK4rmG0ufgYWP5dAAFqTpLf6l0yuK3FBUmPnHWltHt6I9g9qwKkwXXSCHUeMCwFV2kaxdZ2JPExXkufPjJz46MaOuxT5nZ++QgoHz2xMJQHYx4gLAVakt/jt7Y4r2xFSQF9DYRNL573yqKgmpf3gkrYZmekM6ALmL4ALAdanhJWhZTlgZm0iqqDBP8dGJ8z5HUWGe3l9ZpM7eoYzhhdAC+ANTRQA80dRQq/xgQAljlB8MOMui5yO02J8TsCangfqHR9Leyw8GCC2ATxBcAHjCLsjNDwamWv3Pf61JtCem6pLQjNfHE0l6uQA+QXAB4Dq7ODZSE1bXvZtVv7pU0Z7YGVcAnY++lNGW+tWladsKEF6A3EdwAeCq1NAS7YmpraNbkZoySfM3TXQmkZoyNTXUEl4AHyG4AHCV3Xp/77Z6pwW/NLn6x02pxbh2eKlfXUoXXSDH0TkXgKfcavc/nb3ZIs3mgIVprr+/GXEB4Kmmhlq53QOuIC+gaE9M+18dSnu9raNbrR6EJgDuIbgA8FRbR7eSRgq6sKrINjaRVHVJSOtXlqadt2Vq+wEAuYvgAsAzdnhobqzT+lUlrp6rb3jECSmp56WfC5Db6JwLwBOp4UGSOnuHlB+0NJ6Y3zI7S5L9iS3tXfqXp7qVMIbQAvgEIy4APGGvLmpqqFUiaVRVEpr30CJNhpaqkpAiNeHJ8xqjoGURWgCfYMQFgKtap+pKUlf37Gis04tHYjNa88+X/uER57MtTYaXto5uwgvgA4y4AHBVMGDNaPzW2t6l11wKLanyg5aOfPU6p38MzeeA3MeICwBXpe4M3dkb095t9dr/6lBaa363XLaiZMY1pD4HkHsILgBc19RQq87emKI9MdXsfGKy7iRgud7F1t5aoK2j26mxoXMukNsILgBc19repfrVYb3YO6TEVLNutwNEUWGeM8oT7YmxqgjwCYILAFe1tnfpJ7/tTyuW9WLMIz46oaLCPEV7YorUhAktgE9QnAvAVcGAlbZ6aOPUMmUvxEcnZnTQBZDbCC4APBXtiXl6vpsuq2KjRcBHCC4AXGUXxTZnKTy8eMTboATAXQQXAL5TVJjndM7t7B2ifwvgIxTnAnCV3YCuqNC7Hzfx0QlnJZFE/xbATxhxAeCqpoZaVZeEFB+dkCQ1N9YpaHlz7mjPoJoaaunfAvgIIy4AXNXW0Z3WJXdyt2Zvr4GRFsA/GHEB4KpE0qh+damaG+uUHww4uzVXl4RcP3ekpkxtHd1qnZoqApD7CC4AXLWjsU77Pr1RkjSeSDrhxU1VU6Hoxy/1qWVqd2oA/sBUEQDXtXV0q6W9S/WrJxvB/f71uPqGR1zbr8gOLv3DI3TNBXyG4ALAVXZomb7CpyAvoLGJpCvn7OwdkiRFasJ0zQV8huACwFXPHx5MG/X48Ut96h8ecS202CxJe7fVu3oOAN6jxgWAqy6/uEzRnphu/e4Lauvo1kcvq1JBnvs/eoykKx54msJcwGcYcQHgKnukpaW9S529Q4rUhF0fbWlurNP/81Kf+oZHtP/VIVfPBcBbjLgAcJ3dBE5yf5PFosI8RXsG1TdVmBvtidHyH/ARggsATzQ11Co/6P6PnPjohDp7h9TcWKe92+rpmgv4DFNFADzR1tGt8YS7U0Spoj2DkiYb4O3I0s7UAOYfIy4AXGcvib7w3YWeFOZK0m+ODNF8DvAhggsAV6X2cVkRXqqxiaQsD7JE0kz2cZHEyiLARwguAFyVSJq0wtyiwjy53PFf0mT33GhPjFEXwGcILgBctWOqODbaM6jmxjr9n8tXeTLiYqtfXUrLf8BHCC4AXBcMWE4b/mDA8mTEpX94RPWrS50NHgH4A6uKALiidWqKpqmhVomkUaQmrJb2LhUVevdjp394xLNzAfAGwQWAK4IBy9lQMRiwnPqW+OiEZ9fQPzyiLXs6tX5lKUuiAZ9gqgiAK+xuuXZ4idSEPQ0tdhlNtCdGcS7gI4y4AHBN6j5FXnTNTbUkaGk8QcdcwG8ILgBc1dRQq91PH/a0a64kjSdOL8Om5T/gHwQXAK7yutV/qucPD+qxz7CqCPATalwAuMbumlu/utTpYuul3/Wd8PycANxFcAHgitRW/5GaMkV7Yp7tU2Qbm0iqraPb03MCcBfBBYAr7Fb/dh+XqpKQxia8mzIqKsxT/epStbR3EV4AHyG4AHDFjqnQYn9dVRLy9PzvryzSvk9vVKQmrOcPD3p6bgDuIbgAcF1re5ciNWWeh5e2jm5Fe2K6/OIyT88LwD2sKgLguhePxNTZO+RpcPn963F19g4501UA/IERFwCui9RMjnh4uXdQfHRC1SWhGaGlraNbrVPdfAHkHleDy65du2RZVtqjoqLCed8Yo127dqmyslKhUEgf/vCH9corr7h5SQCyoKmhNivLoU+OvJ323F7pxBYAQO5yfcTlAx/4gI4dO+Y8Dh065Lz3ta99TS0tLdq9e7f279+viooKNTY26tSpU25fFgCP7d1Wr3yPl0PHRyecFUWpy7OZOgJyl+s1Lnl5eWmjLDZjjB588EHdeeeduummmyRJjz76qMrLy7V371595jOfcfvSAHioraNb4x4uh7a1tHc5Ww4QWoDc5/o/f7q7u1VZWalVq1bp1ltvVW9vryTpyJEjGhgY0KZNm5xjCwoKdOWVVyoajbp9WQA8Fu3xdklypCbs7FU0nkgqPxggtAA+4Gpw2bBhg37wgx/of/7nf7Rnzx4NDAwoEokoFotpYGBAklReXp72PeXl5c57mYyNjSkej6c9ACxsbR3d6uwd8ux8kZqw9m6rV2dvTJJkaTK80IgOyH2uThVt3rzZ+Xrt2rXauHGjampq9Oijj6q+vl6SZFnpRXLGmBmvpbr//vt19913u3PBAFzh9WhL/eqw/vKBp9U/PKKqkpCe+4ernRoXSYy8ADnM00q5pUuXau3ateru7nbqXqaPrhw/fnzGKEyqnTt36uTJk86jr6/P1WsGMHetU+32jZlswe+VlvYuZ+n1zR+qljQZVpob69gCAMhxngaXsbEx/eEPf9Dy5cu1atUqVVRUqL293Xl/fHxczzzzjCKRyBk/o6CgQEVFRWkPAAtHa0owCAYstbR36fUTI4qPTni+DDlSE04bXbHDSyJpPL0OAPPH1X8C3XHHHbrhhht00UUX6fjx4/rKV76ieDyurVu3yrIsbd++Xffdd59qa2tVW1ur++67TxdccIG2bNni5mUBcJEdVqTJoNDZG1O0J6aiwjzFRyeyfHVMEwG5ztXg0t/fr49//OMaHBzUe97zHtXX16uzs1MrVqyQJH3xi1/UyMiIPve5z2l4eFgbNmzQk08+qWXLlrl5WQBcZAeDlvYuRXsGFakpU9/QW+rzsGuuJBXkBRTtiamto5uwAviIZYzJ6THTeDyu4uJinTx5kmkjYAG59bsveLqSaLr61aWK1JTRdA5YoOb6+5u9igC4wt6fKBuqS0JOaKKmBfAXggsAXykqzFPf8IiqS0JOrc2OqUZ0AHIfwQXAvLN7pmRjY0V7V+i+4RFFasKMtgA+Q3ABMK9SNzNcv7I0K+HFHnE5GnvL83MDcJd3HaEALAqJpEkrhv3LB5729PyWJRkjZxXTi0dinp4fgLsILgDmVWo9yZY9neofHvG0h8v0dZLZLBIGMP+YKgLgiraO7rTGcx43zZU0s3MugNzHiAsAV9hTRomk0U9/2+95A7qqkpD2bqv39JwA3EdwAeCK6UuQv/f8EU9b/l9UeoFn5wLgHaaKALguG31U7Hb/traObrVO9XUBkLsYcQHguraObk9HW+yVRS0pQcVeog0gtxFcALgu2jMoSVpWmKdTHgQYYyRLkpH045f61D88wn5FgE8wVQTAVW0d3ersHVKkJqy1FxZ7dl57VTShBfAXRlwAuCqRNKoqCSna420jOHvEJT8YILQAPsKICwBX7WisU1VJyPPz2qFlPJFMK9IFkNsILgB8qX51qbru3azmxjq1tHcRXgCfYKoIgC9tWDW5uaM9TWSvMGLaCMhtBBcArmht79L/e6BPK8JLFakpU2fvkKfn/97zR5z+MXZYSSTN2b4FQA4guABwRTBg6bUTo3rtxKgkqbok5Gnb//johNo6up3QwkgL4A/UuABwRVNDrdPwLdoT8zS0FOQFVFUS0vOHBz07JwBvEFwAuKapoVaRmrDn583PC6h/eESXX1zmvEbLf8AfCC4A5l1ryiqebOzQfGp0QvWrS53pobaObrW0dykYsDy/FgDzixoXAPMuGLDS9gnKht8ePSHpdGihey7gDwQXAPNu+hLkbBifSKruzl9oPJEktAA+wlQRAN8aTyRp+Q/4DMEFgCsSSaML312obFaVBC2Llv+AzxBcALhiR2OdLFnKZsu3DatLFakJ0/If8BGCCwBXtHV0q//EZO+W6pKQ58uiIzVhJY1RtCdGeAF8hOJcAPMudSVPZ29M0Z6Y+j1sQGfr7B1yCnPbOrpp+Q/4ACMuAOZdImmcwLB+ZamCAe+njKI9sbTVRE0NtQoGLJrQATmO4AJg3u1ICQzBgJW1kY7U1UQ0oQP8geACwFVNDbXKD2YnLGzZ0ymJJnSAn1DjAsBVW/Z0ajyRnRGXaE9MNTufUMIYQgvgE4y4AHDNlj2dzqqeokLv/p1UVRJSc2OdLEkJYxS0LEIL4BMEFwCuSSSNIjVhzzdavKj0AkV7BmU02YQuYYy27OmkMBfwAaaKALjmsc9slCRd8cDTio9OeHbeaE9MkpzpIXvkB0DuY8QFgGta27u0ZU+n+oZHPJ0qkqSiwjynf4s9XRXtidGEDshxjLgAcE0wYDmhIWmMOnuHPDt3fHRixu7QNKEDch/BBYBr7ILYlvYuVZWEPD23pZm7Q1OgC+Q+pooAuKqpoVbNjXWetvwvyAvISMoPBtgdGvAZgguAedWaYTNDr0c6xiYmp4duu/pi1a8uZYNFwEeYKgIwr4IBSy1Ty47twJKN0GBv7tjcWKdITdmMawKQmwguAOZVal2LrSUL/VOmb7IoicJcwAcILgDmXabwkp8X0PhE0rNrqCoJpYUWRloAf6DGBYArmhpqZW/EHLQsT0OLJem5f7jas/MB8A7BBYAr2jq6lTSTK3sSxqjaw+XQRjPrato6umn5D/gAwQXAvGvr6FZLe5fqV5fqtqsvVqQmrD4Pl0PnB620lUT29QTtISAAOYsaFwDzyg4JzY11ivYMqqW9Sxe+u1CWJkdCvDCemNzcsaW9Sz9+qU/9wyMzCnUB5CaCC4B5lUiatJDQ2Tuk106MenoN1SUhZ1NFQgvgLwQXAPNqR2Od83Wm1UVeSJ2WSm35DyD3UeMCwFVNDbWK1ISzcu6gZdHyH/AZRlwAuKa1vUs/OdCv+OjbWTn/F65JH/Fh5AXIfQQXAK5obe/S/leH1H/Cu9VEqaqnNaAjvAD+wFQRAFcEA5ZTIOu1osI89Q2P6NbvviDp9A7VtPwHch/BBYArmhpqVTXVdM7r/invryySNLmiya5vaWqoTSscBpCbmCoC4JoL3x3S/3dqTGMetvuXJgNL81RIYZQF8BdGXAC45vKLyzwPLbZoz6CaGmoVDFi0+gd8hOACwDVNDbUqKszOwG5n75C27Omk1T/gMwQXAK7ZsqdT8dGJrJzb7p5L11zAXwguAFzR1tGtaE9MywrzstKAro9W/4AvEVwAuCKRNKpfXaptV6z29LxMCgH+RnAB4IodjXWK1JSppb3L034uRpP7E0mTBboA/IXgAsB3brv6YjU31qX1cQHgDwQXAK55/vCgCvK8/TFTVRJy2vtHasJqae8ivAA+siCCy7e//W2tWrVKhYWFWrdunX79619n+5IAzAMv+7gELam5sU5VJSHVry5VtGdQ0Z6YIjVhmtABPpL14PLYY49p+/btuvPOO3Xw4EFdccUV2rx5s44ePZrtSwNwnuw9gryQMFJnb0ydvUMKWJY6e4dUVRLS+pWltPoHfCTrwaWlpUWf/OQn9alPfUp//ud/rgcffFDV1dX6zne+k+1LA5Bj7CJg+7/9wyM0nwN8JqvBZXx8XAcOHNCmTZvSXt+0aZOi0WjG7xkbG1M8Hk97AFiY2jq61dLepebGOgUt7wMEfVwA/8lqcBkcHFQikVB5eXna6+Xl5RoYGMj4Pffff7+Ki4udR3V1tReXCmAOEkmj5sY6RXsGlTDe1plEasKEFsCHsj5VJEnWtH+JGWNmvGbbuXOnTp486Tz6+vq8uEQAc2DXlnT2Dnl6XkuT00WsJgL8Jzu7n00pKytTMBicMbpy/PjxGaMwtoKCAhUUFHhxeQDOkz1V5DWj00uhJTHyAvhIVkdc8vPztW7dOrW3t6e93t7erkgkkqWrAjBf7Lb/VSUhz8/NUmjAn7I+VdTc3KyHH35Y//Zv/6Y//OEP2rFjh44eParPfvaz2b40APMgUlPmaXApKsxT/epSSZPhhVVFgL9kdapIkm655RbFYjHdc889OnbsmNasWaMnnnhCK1asyPalAThPwYCllvYuT3eHjo9OKGBZp4uCGXEBfMUyxuNS/3kWj8dVXFyskydPqqioKNuXA2Aau86lIC/gSRfd6pKQKt8d0mOf2ej6uQDM3Vx/f2d9qgiAvzU11Kq6JORZ6/8LSwgtgJ8RXAC47kKPalyKCvPYERrwOYILANf1D494cp6i0BJVlYT0/OFBT84HwHtZL84F4G9tHd2eBRf7PDd/iI7agF8x4gLANXZhrpfLoWn1D/gbwQWAa+y9ij56WZUn4aWoMI/lz4DPMVUEwDX2XkVeTRfFRyd0+cVlrp8HQPYw4gLAFa3tXc7qnkTSaFmh+/9OKirMY5oI8DmCCwBX2F1z2zq6taOxTsWFS1w/Z3x0Qn/5wNNqTdnYsa2jO+05gNzGVBEAV9gjHy3tXfrxS326KHyB+k+4P13UPzyi/a8OSTpdHNw8NWUFIPcRXAC4JpE0qi4JqW94xLteLoV5Wr+yNC20MH0E+AfBBYBrggFLfR4FFmkytMRHJ9TW0S0jEVoAH6LGBYBr7H2KvBIfnZAkGUn5wQChBfAhggsA17R1dKtveMTT8CJJQcvSeCLJnkWADxFcALgitcbk1/9wtWfnLcgL6EMrS9TcWOesagLgH9S4AHCF3TW3qaFWW/Z0enbesYmkXj8xkraqSRLTRoBPEFwAuMLumrtlT6eiPTHPzltUmKebLquSdDqssA0A4B8EFwCuaevoVrQnpoK8gMYmkmpurNPDv+51imjdsObCYic0SYy0AH5DjQsA19h9XMYmkqouCamzN+ZqaJGkaE/M06kpAN5ixAWAa+yRj5/+tl99wyPqGx7RssI8vTE6ITcmb4KWlDDS0dhbLnw6gIWAERcArtoxtaooP2/yx40boaV+damKCvOUMFJ1SUgfXVc1z2cAsFAQXAB4ousrmyXJlZEWSfrfXdc62wvYexUB8B+migB4wq1+KlUlIXX2Dmntrv/RqdEJVZeElEgatXV0K5E0aYW6AHIfIy4AXGc3o4vUhOf1cwvyAs7mjadGJxSpCevX/3C1Lr+4TC3tXQoGrHk9H4DsY8QFgCtu+b9f0GsnRnRR6QWK9sQUqQmnLY2eD6mfU10SYldoYBEguABwRTBgqX94RP1TexVFe2KqKgk5IyTzei5rchfq7/yqR+OJJKEF8DGmigC4Yu+2emdqyN5o0Y3Qkh8MKGGMs7Eiu0ID/kZwAeCa6eHFDZY1WaCbMEaWxK7QgM8RXADMu9aUXZn3bqt39VxjE0lnJMdIitSE2RUa8DFqXADMu2DAcnZl7uz1ZoPFosI8feqK1UokjepXh9kVGvApgguAeWeHBTs8SHJWFbmlqHDJjJDCrtCA/xBcALgidaQlaFlpS6LnW6ZVRIy0AP5EjQsAVySSRlUlIQUtSwljlB8MqH71/DagA7D4EFwAuOKxz2zUzR+qVsIYBazJ1T4P/7p33s9TVRKiGBdYRAguAFyR2sF2+zWT+wXFRyfm/Tz9wyOsJAIWEWpcAMy7TG33f7y/T/0n5reXS1FhnuKjE3r5tZOqX11KMS6wCDDiAmDeJZJmRsHszeur5/UcRYV5+j+Xr1J1SUjx0QkFLIudoIFFgBEXAPMuU4BoaqjVj1/qc5rFWZpsGDdX8dEJ7X91SH1TU0XrV5aex6cByBUEFwCua23vUjBgpW2yOB+TOtGeGBsqAosMwQWA61I76VaVhPTa8Mi8BJegZRFagEWGGhcArov2DDpf989TaJGkhDHasqdznj4NQC4guADwTHVJaN4/L9oTYxk0sIgwVQTAFXZdS1NDrfZ9eqO27OlUtCd23kW5tmWFebrpsqq0aSimjQD/I7gAcMX0QLF+Zan6ht5S3/D89HI5NTrhBCOJDRWBxYLgAsAV03eIDgaseQst0uQ0kR1WGGkBFg+CCwDXTA8v053PtNGFJSHtaKxTW0e3oj2D2rAqTAM6YBGgOBeAq5oaamVleD1SE5aRZGV68yyKCif/vdU/POJsLdDZO6Rg4Bw/CEBOIrgAcFVbR3faqEpVSUjNjXXau61+Mryc45BLfHRC1SUhGWOckZxITZgaF2CRYKoIgGvsEZHUjrn9wyPq7I1Jmux8OxeptTKRmrCiPTHVrw6f/wUDWPAYcQHgitQdop/7h6vVnFJ/Eu2JnbHu5VwELYu2/8AiQ3AB4IrpO0Q3NdSq6gwN6OZanZIwRpGaMKEFWEQILgBcsSPDKMhFpRfMOO58G9LRORdYXKhxAeC61vYu7X91KGNNy7mGluqSkPqGR9ICD51zgcWDERcArksNLc2NdWn1LufKLszdWHO6GLeoMI9VRcAiQXAB4LrktDXP9qqiubJXEkWmwkt8qv0/AP9jqgiA6zasCitSUybpzF10z4UdWuwVRRJ7FQGLBcEFgOvsVvzzWUTLMmhgcWKqCIBnfvxSX9rzqpLQGZdI26393+nzWudhBAdA7iC4APBEW0e3+odHFKkJ6wsNtYrUhNU/PJJxiXRVSUjx0YkZr08PM/3DI9S2AIsMU0UAXJfaRTd1asd+3V7ibOtP+TpVfHRCRYV5aaGmszfGdBGwiBBcALhuehddabK3SzBgqbmxTomkOWOfF3uUxQ4r9n+XFeZp7YXFaQ3oEknj1NMA8CemigC4LlMX3WDAUkt7lzp7Y9rRWHfGTRLjoxN6f2XRjNfXXlis9StL1dxYp2jPoFqmghAAf2PEBYBn7FGWpoZaJZJG1SUhRXtiuuKBp9Omiqbr7B2a8VrqjtCdvUOsMAIWCYILAM/Yoyz2133DIyoqzDtraJkutR7mX57qVsLMnIYC4F+uThWtXLlSlmWlPb70pS+lHXP06FHdcMMNWrp0qcrKytTU1KTx8XE3LwtAljQ11Kq5sc4JL82NdRlXD0Vqwk5X3On6plYmSZO7Qwcti9ACLCKuj7jcc8892rZtm/P8Xe96l/N1IpHQddddp/e85z167rnnFIvFtHXrVhlj9K1vfcvtSwOQBXbIaGnvUn5w5r+dLEl7t9WrraM7Y7GuJL382knn64QxauvoVlNDrdo6uinQBXzO9eCybNkyVVRUZHzvySef1O9//3v19fWpsrJSkvTNb35Tn/jEJ3TvvfeqqGhmQR6A3GbXuQQtS+OJ5Iz3jaQP7vqfjCMx9o7Q9rLo/911rbOkurM3lrYFAAB/cn1V0QMPPKBwOKxLL71U9957b9o00AsvvKA1a9Y4oUWSrr32Wo2NjenAgQMZP29sbEzxeDztASB32HUuCZN5b6FgwJrVponx0QlnpCV100WmjQB/c3XE5Qtf+IIuu+wylZSU6De/+Y127typI0eO6OGHH5YkDQwMqLy8PO17SkpKlJ+fr4GBgYyfef/99+vuu+9287IBeKQgL6CxifRRF3uzxNRNEyM1Yb3YO6SEMbI02Vm3b3hELe1devCpLiXN5DHrV5Z6efkAsuCcR1x27do1o+B2+uOll16SJO3YsUNXXnmlPvjBD+pTn/qUHnroIT3yyCOKxU7PW1vWzH9VGWMyvi5JO3fu1MmTJ51HX19fxuMALDypHXQjNeEZoSUTe8m0XYhrJGc1kiQljZQfDGjvtnpqW4BF4JxHXG677TbdeuutZz1m5cqVGV+vr6+XJB0+fFjhcFgVFRV68cUX044ZHh7W22+/PWMkxlZQUKCCgoJzvWwAC0BqB91E0uiFnphSJ4wClpQXCKTVvqQuld6wutQp2LVrYPKDk8fb00YA/O2cg0tZWZnKysrmdLKDBw9KkpYvXy5J2rhxo+69914dO3bMee3JJ59UQUGB1q1bN6dzAFi4UkdEgoHJ0RO74FaaHD3JVLCrqeOmrzKK1ISdFUj2EmvCC+BvrtW4vPDCC+rs7NRVV12l4uJi7d+/Xzt27NCNN96oiy66SJK0adMmvf/979ff/d3f6etf/7qGhoZ0xx13aNu2bawoAnymtb1LLx6JKVIz+Q+flvYup6h2+saJkZqw+obeShttmV7Km1rTkrrEOvU5AP9xLbgUFBToscce0913362xsTGtWLFC27Zt0xe/+EXnmGAwqJ///Of63Oc+p8svv1yhUEhbtmzRN77xDbcuC0CWBAOWOnuHnPb9qSuBpo+k2M+nBxrp9AhN/er0FUT216lFvQD8x7Xgctlll6mzs/Mdj7vooov0X//1X25dBoAFYvqoyNGhtzKGllSZerkYTRbsMroCLE7sDg3AM00NtapfPTm90z88MiO02CuFMonUhFVVEpI0WbBrh5e2jm5Jp1cssUM04G8EFwCesmtcMsk0wiKdXhJ984eqnc649p5FiaRJW2bNCAzgb+wODWDB6xsemRFKoj2DivbE9NKrwxpPJAktwCLBiAsAz6QuW66emvaxZZriCZ6hEWVTQ632fXqj08MlPxggtACLBMEFgCdSQ0ukJqy+4ZG0YJJpNVDCGOUHA2purJvxfltHtxNa7AZ0APyPqSIAnkgkjepXlypgWWnLoO3gkWnfotQdpFOb102vaaEBHbB4MOICwBM7Guu0YVVYSWOc0NLcWKeuezen7VtUlTKFlJg6NtPqodSalqaGWjU31qUdB8CfGHEB4Bm7CZ2ktNGS1GXR/VOFuNJkzxd7dMYeUUnd7ygVDeiAxYHgAsAzTQ21ivYMOuFFmlwdJE3WvRyNvSVZmrF6yG7t//zhQT32mY1n/XwA/sZUEQBPtE5N4+z79EZnWqfuzl+os3dIVSUhJY3Rzeur1T884kz3NDXUKlJTphePxBTtienyi+e2wSsA/2DEBYAnggErrYB299OHNZ5IKmhZ6h8eUf/wiCI1ZU6osdlf06cFgERwAeCR1L2KOntjTmixC3CTxjhFt9PDi90hFwCYKgLgmcmpn8kVRamhJdoTU2Cqp0tLe5d2P33Y+Z7g1PJp9iACIBFcAHjIXkFkh5b8YEB7t9WrubHOWT0kyendkhpumCYCIBFcAHgokZwMIXZosTve2n1YkiZ9Oih1RIb+LAAkalwAeCgYsJzGc9M73kpylknbIy2SVL86rPrVYTrjApBEcAHgkTN1vJVOrxxK3RLAHpFJLdilQBcAU0UAPHG2jrf1q0tVv7pUkZqytK0A7A660Z5BNTXUpu1XBGBxsowxOf1PmHg8ruLiYp08eVJFRUXZvhwAc5RpRGb664mkUTBgZZwuauvoViJpCDdAjpjr72+migAsCLPZg2h6EztbargB4G8EFwALwtlGSqaHmdTwcqaRGgD+RHAB4LnW9q45T/mkFvTa2wacqc8L00eA/1CcC8Bz9pTP9N4s9ujJO3XJbWqodVYd2Z115/pZAHILIy4APDd9GfS5Tvm0dXRrPJF0wkukJsz0EbBIsKoIQNbYAcMOILMNLamhxH5ud9g9l88CkD1z/f3NVBGArEmd8skPBs45tNifYe91FLSsWX8WgNxEcAGQNdOnfN5pP6KzLZnOtAcSAP+hxgVAVpxpykdKX/6cugJp+uoge9XQ2fZAYuQF8BeCCwDPzWbfIvv5OzWds2tbZvNZAHIfwQWA554/PJix90pTQ606e2N6/vDgWUPI9G0A6ldn/ixJbMwI+AzBBYDnLr+4zOnjMn0UxR49SZWp6dxsVg0x0gL4D8EFgOfm0selqaHWCS2sGgIWL4ILgKw411GUTCuQCC/A4sNyaABZM9s+LqmjMV33blZzY13GLQMA+B8jLgA8lbq8efooypY9nVq/sjRt2fO5rEAC4H8EFwCespc3d/bG0pYxb9nTqWhPbMbxZ2s6Z78PYPFgryIAnrNDSqQmrL3b6s/akwWAP8319zcjLgA8t35lqSQp2hNT3Z2/SCvMtbvhAkAmjLgAyBo7tOQHA+q6d3O2LweAh9gdGkBOOdcNFgFAYqoIQBbMdoNFAJiO4ALAUyxvBnA+CC4APMXyZgDng+JcAADgOYpzAQCA7xFcAABAziC4AACAnEFwAQAAOYPgAgAAcgbBBQAA5AyCCwAAyBkEFwAAkDMILgAAIGcQXAAAQM7I+b2K7B0L4vF4lq8EAADMlv17+1x3Hsr54HLq1ClJUnV1dZavBAAAnKtTp06puLh41sfn/CaLyWRSr7/+upYtWybLsrJ9OWni8biqq6vV19fHBpDvgHs1O9yn2eNezQ73afa4V7M3m3tljNGpU6dUWVmpQGD2lSs5P+ISCARUVVWV7cs4q6KiIv6SzxL3ana4T7PHvZod7tPsca9m753u1bmMtNgozgUAADmD4AIAAHIGwcVFBQUFuuuuu1RQUJDtS1nwuFezw32aPe7V7HCfZo97NXtu3qucL84FAACLByMuAAAgZxBcAABAziC4AACAnEFwAQAAOYPgMg/uvfdeRSIRXXDBBXr3u9+d8RjLsmY8HnroobRjDh06pCuvvFKhUEgXXnih7rnnnnPew2Ghm829Onr0qG644QYtXbpUZWVlampq0vj4eNoxi+FeTbdy5coZf4e+9KUvpR0zm3u3GHz729/WqlWrVFhYqHXr1unXv/51ti8p63bt2jXj709FRYXzvjFGu3btUmVlpUKhkD784Q/rlVdeyeIVe+PZZ5/VDTfcoMrKSlmWpZ/97Gdp78/mvoyNjenzn/+8ysrKtHTpUt14443q7+/38E/hjXe6V5/4xCdm/B2rr69PO2Y+7hXBZR6Mj4/rYx/7mP7+7//+rMd973vf07Fjx5zH1q1bnffi8bgaGxtVWVmp/fv361vf+pa+8Y1vqKWlxe3L99Q73atEIqHrrrtOb775pp577jnt27dPP/nJT3T77bc7xyyWe5XJPffck/Z36Mtf/rLz3mzu3WLw2GOPafv27brzzjt18OBBXXHFFdq8ebOOHj2a7UvLug984ANpf38OHTrkvPe1r31NLS0t2r17t/bv36+Kigo1NjY6+8H51ZtvvqlLLrlEu3fvzvj+bO7L9u3b9fjjj2vfvn167rnn9MYbb+j6669XIpHw6o/hiXe6V5L0kY98JO3v2BNPPJH2/rzcK4N5873vfc8UFxdnfE+Sefzxx8/4vd/+9rdNcXGxGR0ddV67//77TWVlpUkmk/N8pdl3pnv1xBNPmEAgYF577TXntf/4j/8wBQUF5uTJk8aYxXevbCtWrDCtra1nfH82924x+Iu/+Avz2c9+Nu21973vfeZLX/pSlq5oYbjrrrvMJZdckvG9ZDJpKioqzFe/+lXntdHRUVNcXGweeughj64w+6b/nJ7NfTlx4oRZsmSJ2bdvn3PMa6+9ZgKBgPnv//5vz67da5l+p23dutX89V//9Rm/Z77uFSMuHrrttttUVlam9evX66GHHlIymXTee+GFF3TllVemNeu59tpr9frrr+vVV1/NwtVmxwsvvKA1a9aosrLSee3aa6/V2NiYDhw44ByzWO/VAw88oHA4rEsvvVT33ntv2jTQbO6d342Pj+vAgQPatGlT2uubNm1SNBrN0lUtHN3d3aqsrNSqVat06623qre3V5J05MgRDQwMpN23goICXXnllYv6vs3mvhw4cEBvv/122jGVlZVas2bNorx3v/rVr/Te975XdXV12rZtm44fP+68N1/3Kuc3WcwV//zP/6yGhgaFQiF1dHTo9ttv1+DgoDPUPzAwoJUrV6Z9T3l5ufPeqlWrvL7krBgYGHD+3LaSkhLl5+drYGDAOWYx3qsvfOELuuyyy1RSUqLf/OY32rlzp44cOaKHH35Y0uzund8NDg4qkUjMuA/l5eWL5h6cyYYNG/SDH/xAdXV1+tOf/qSvfOUrikQieuWVV5x7k+m+/fGPf8zG5S4Is7kvAwMDys/PV0lJyYxjFtvfuc2bN+tjH/uYVqxYoSNHjuif/umfdPXVV+vAgQMqKCiYt3vFiMsZZCpkm/546aWXZv15X/7yl7Vx40Zdeumluv3223XPPffo61//etoxlmWlPTdTxabTX19o5vteZfrzGmPSXs/VezXdudy7HTt26Morr9QHP/hBfepTn9JDDz2kRx55RLFYzPm82dy7xSDT34/Fdg+m27x5sz760Y9q7dq1uuaaa/Tzn/9ckvToo486x3DfMpvLfVmM9+6WW27RddddpzVr1uiGG27QL37xC3V1dTl/187kXO8VIy5ncNttt+nWW2896zHT/9V/Lurr6xWPx/WnP/1J5eXlqqiomJE47SG26Wl/oZnPe1VRUaEXX3wx7bXh4WG9/fbbzn3I5Xs13fncO7ta//DhwwqHw7O6d35XVlamYDCY8e/HYrkHs7V06VKtXbtW3d3d+pu/+RtJk6MHy5cvd45Z7PfNXnV1tvtSUVGh8fFxDQ8Pp40kHD9+XJFIxNsLXmCWL1+uFStWqLu7W9L83StGXM6grKxM73vf+876KCwsnPPnHzx4UIWFhc6S4I0bN+rZZ59Nq1l48sknVVlZeV4ByQvzea82btyol19+WceOHXNee/LJJ1VQUKB169Y5x+TqvZrufO7dwYMHJcn5gTqbe+d3+fn5Wrdundrb29Neb29vX/S/RKYbGxvTH/7wBy1fvlyrVq1SRUVF2n0bHx/XM888s6jv22zuy7p167RkyZK0Y44dO6aXX355Ud87SYrFYurr63N+Rs3bvZp1GS/O6I9//KM5ePCgufvuu8273vUuc/DgQXPw4EFz6tQpY4wx//mf/2m++93vmkOHDpnDhw+bPXv2mKKiItPU1OR8xokTJ0x5ebn5+Mc/bg4dOmR++tOfmqKiIvONb3wjW38sV7zTvZqYmDBr1qwxDQ0N5re//a156qmnTFVVlbntttucz1gs9ypVNBo1LS0t5uDBg6a3t9c89thjprKy0tx4443OMbO5d4vBvn37zJIlS8wjjzxifv/735vt27ebpUuXmldffTXbl5ZVt99+u/nVr35lent7TWdnp7n++uvNsmXLnPvy1a9+1RQXF5uf/vSn5tChQ+bjH/+4Wb58uYnH41m+cnedOnXK+Tkkyfn/7I9//KMxZnb35bOf/aypqqoyTz31lPntb39rrr76anPJJZeYiYmJbP2xXHG2e3Xq1Clz++23m2g0ao4cOWJ++ctfmo0bN5oLL7xw3u8VwWUebN261Uia8fjlL39pjDHmF7/4hbn00kvNu971LnPBBReYNWvWmAcffNC8/fbbaZ/zv//7v+aKK64wBQUFpqKiwuzatct3y3vf6V4ZMxlurrvuOhMKhUxpaam57bbb0pY+G7M47lWqAwcOmA0bNpji4mJTWFho/uzP/szcdddd5s0330w7bjb3bjH413/9V7NixQqTn59vLrvsMvPMM89k+5Ky7pZbbjHLly83S5YsMZWVleamm24yr7zyivN+Mpk0d911l6moqDAFBQXmr/7qr8yhQ4eyeMXe+OUvf5nxZ9LWrVuNMbO7LyMjI+a2224zpaWlJhQKmeuvv94cPXo0C38ad53tXr311ltm06ZN5j3veY9ZsmSJueiii8zWrVtn3If5uFeWMT5vNwoAAHyDGhcAAJAzCC4AACBnEFwAAEDOILgAAICcQXABAAA5g+ACAAByBsEFAADkDIILAADIGQQXAACQMwguAAAgZxBcAABAziC4AACAnPH/A8fcYXAqV/s1AAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mean = [-10, 30]\n",
    "cov = [[10, 20], [20, 1000]]\n",
    "x, y = np.random.multivariate_normal(mean, cov, 5000).T\n",
    "plt.plot(x, y, 'x')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "JoMmFYsY9qiM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274948424,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "JoMmFYsY9qiM",
    "outputId": "b5d36d87-2b96-4bde-fe86-690691e4d64f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 30000), dtype=float64, numpy=\narray([[[-16.90305237, -10.42008847,  -7.99352076, ...,  -9.3876178 ,\n          -9.42813894,  -5.48572102],\n        [-24.2944417 , -31.24858685,  54.93427032, ...,  53.04379639,\n           8.78127999, -12.6407996 ]],\n\n       [[ -9.37620705,  -6.88851091, -15.16714768, ...,  -9.27224863,\n         -15.42330593, -19.6812529 ],\n        [ 47.53693771,  32.83899792,  50.03523055, ...,  78.10374506,\n          47.73524195,  28.97111049]]])>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just for test, so the dimension may be different with real dataset\n",
    "testdata=tf.transpose(tf.constant(np.random.multivariate_normal(mean, cov, [2,30000])), perm=[0, 2, 1])\n",
    "testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "t8H8J8NO9qiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274948424,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "t8H8J8NO9qiN",
    "outputId": "5d6386d6-18ed-486c-f977-991679db727f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 1, 3), dtype=float64, numpy=\narray([[[0.18641518, 0.18916416, 0.1823912 ]],\n\n       [[0.21570571, 0.21699157, 0.19679823]]])>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20/(10*1000)**0.5=0.2\n",
    "ts_corr(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "uLUulnfU9qiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274948424,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "uLUulnfU9qiN",
    "outputId": "c1b41baa-7b62-4316-a424-a13316874fa0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 1, 3), dtype=float64, numpy=\narray([[[18.84705515, 18.71665573, 17.88085835]],\n\n       [[21.88986137, 21.65356242, 19.86808809]]])>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20\n",
    "ts_cov(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "VAZ3yh4e9qiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1029,
     "status": "ok",
     "timestamp": 1666274949450,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "VAZ3yh4e9qiN",
    "outputId": "9d4936a3-bb78-46d5-8866-e12a539b00d1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[ 3.17411291,  3.14160383,  3.12336552],\n        [31.85223074, 31.49473643, 31.38785763]],\n\n       [[ 3.1818418 ,  3.16483701,  3.17610641],\n        [31.89354377, 31.53081092, 31.78629018]]])>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_std=10**0.5 about 3.162;y_std=1000**0.5\n",
    "ts_stdde(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ntGpHdeF9qiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274949450,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "ntGpHdeF9qiN",
    "outputId": "7f121e5b-ddc3-427d-8bdb-da50436238a4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[-3.14351236, -3.19049342, -3.1942247 ],\n        [ 0.93824227,  0.97753709,  0.97382519]],\n\n       [[-3.14810563, -3.13746817, -3.15751681],\n        [ 0.93272024,  0.95643252,  0.93745234]]])>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_mean/x_std=-10**0.5 ; y_mean/y_std=0.3*10**0.5\n",
    "ts_zscore(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "x0dSeqrb9qiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274949450,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "x0dSeqrb9qiN",
    "outputId": "96ce5bd7-f633-40fd-9d7b-11e14fa613f5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[-1.27977411, -1.15881654, -1.47819652],\n        [-3.05258477, -2.40012274, -1.03650302]],\n\n       [[-1.0302163 , -1.05209317, -0.86420791],\n        [-1.09490593, -0.46013833, -0.95665654]]])>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_return(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9GLIPEiU9qiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274949450,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "9GLIPEiU9qiN",
    "outputId": "cf914e80-f84f-4241-849b-d87fe23fc25c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[-10.00730588, -10.00571928,  -9.98013103],\n        [ 29.89701911,  30.76479969,  30.72770231]],\n\n       [[-10.02805402,  -9.9195615 , -10.02264738],\n        [ 29.6061017 ,  30.36118034,  29.65341016]]])>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_decaylinear(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "zTrKR_7-9qiO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274949451,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "zTrKR_7-9qiO",
    "outputId": "cbe9b968-09dc-4293-a23f-6abc3f99efed",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[ -22.52626183,  -22.34290289,  -22.95208512],\n        [ -90.72019474, -115.58623272,  -93.48377318]],\n\n       [[ -22.58301063,  -22.21321229,  -23.57184258],\n        [ -80.86111071,  -91.24790674,  -89.28648657]]])>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_min(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "PtqAumv-9qiO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274949451,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "PtqAumv-9qiO",
    "outputId": "4f980e54-4456-4f2a-e49d-3dad4e3b75cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[  1.64873887,   1.86506671,   2.34379988],\n        [146.91143933, 135.25593111, 144.84853975]],\n\n       [[  1.57247817,   1.77302705,   1.15114149],\n        [151.34434873, 142.59452249, 139.38090044]]])>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_max(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "lF7GJrp39qiO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274949451,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "lF7GJrp39qiO",
    "outputId": "41485dde-38d9-4730-e5ec-c0fc2cff7aa7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[ -9.97786316, -10.02326633,  -9.9767313 ],\n        [ 29.88510938,  30.78727301,  30.56628628]],\n\n       [[-10.01677408,  -9.9295754 , -10.02860938],\n        [ 29.74775368,  30.15709302,  29.79813212]]])>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_mean(testdata,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gX7BY77z9qiO",
   "metadata": {
    "collapsed": false,
    "id": "gX7BY77z9qiO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From the results above, the dimensions of outputs and statistics properties are right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PbUAsSs89qiO",
   "metadata": {
    "collapsed": false,
    "id": "PbUAsSs89qiO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. block construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab53c7b3-b0a1-4404-ade6-1d3f51b3aac3",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274949451,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "ab53c7b3-b0a1-4404-ade6-1d3f51b3aac3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class alphanet_feature_Block(tf.keras.Model):\n",
    "    def __init__(self, stride):\n",
    "        super(alphanet_feature_Block, self).__init__(name='')\n",
    "        #stride: tensorflow/array(2,)\n",
    "        self.stride_feature, self.stride_pool = stride\n",
    "\n",
    "        #pooling layers\n",
    "        self.max_pool = tf.keras.layers.MaxPooling2D(pool_size=(1, 3),strides=(1, self.stride_pool), padding='valid')\n",
    "        self.mean_pool = tf.keras.layers.AveragePooling2D(pool_size=(1, 3),strides=(1, self.stride_pool), padding='valid')\n",
    "        #different in input and output\n",
    "        self.min_pool = tf.keras.layers.MaxPooling2D(pool_size=(1, 3),strides=(1, self.stride_pool), padding='valid')\n",
    "\n",
    "        #batchnormal for feature input\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn5 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn6 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn7 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        #batchnormal for pooling layer\n",
    "        self.bn1mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn4mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn5mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn6mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn7mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn1max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn4max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn5max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn6max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn7max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn1min = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2min = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3min = tf.keras.layers.BatchNormalization()\n",
    "        self.bn4min = tf.keras.layers.BatchNormalization()\n",
    "        self.bn5min = tf.keras.layers.BatchNormalization()\n",
    "        self.bn6min = tf.keras.layers.BatchNormalization()\n",
    "        self.bn7min = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "\n",
    "\n",
    "    #output: N*H*[W/H]*1, the last is the number of channel\n",
    "    def ts_corr(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_corr(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_cov(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_cov(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_stdde(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_stdde(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_zscore(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_zscore(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_return(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_return(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_decaylinear(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_decaylinear(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    #below functions could be replaced with pooling layers\n",
    "    def ts_min(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_min(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_max(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_max(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_mean(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_mean(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        #input_tensor tensorflow:(N,9,30)\n",
    "        #feature layer\n",
    "        x1 = self.ts_corr(input_tensor,self.stride_feature)\n",
    "        x1 = self.bn1(x1, training=training)\n",
    "        x1f = tf.keras.layers.Flatten()(x1)\n",
    "\n",
    "        x2 = self.ts_cov(input_tensor,self.stride_feature)\n",
    "        x2 = self.bn2(x2, training=training)\n",
    "        x2f = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "        x3 = self.ts_stdde(input_tensor,self.stride_feature)\n",
    "        x3 = self.bn3(x3, training=training)\n",
    "        x3f = tf.keras.layers.Flatten()(x3)\n",
    "\n",
    "        x4 = self.ts_zscore(input_tensor,self.stride_feature)\n",
    "        x4 = self.bn4(x4, training=training)\n",
    "        x4f = tf.keras.layers.Flatten()(x4)\n",
    "\n",
    "        x5 = self.ts_return(input_tensor,self.stride_feature)\n",
    "        x5 = self.bn5(x5, training=training)\n",
    "        x5f = tf.keras.layers.Flatten()(x5)\n",
    "\n",
    "        x6 = self.ts_decaylinear(input_tensor,self.stride_feature)\n",
    "        x6 = self.bn6(x6, training=training)\n",
    "        x6f = tf.keras.layers.Flatten()(x6)\n",
    "\n",
    "        x7 = self.ts_mean(input_tensor,self.stride_feature)\n",
    "        x7 = self.bn7(x7, training=training)\n",
    "        x7f = tf.keras.layers.Flatten()(x7)\n",
    "\n",
    "        #feature output\n",
    "        feature_output = tf.concat([x1f, x2f, x3f, x4f, x5f, x6f, x7f],axis=-1)\n",
    "\n",
    "        #pooling layer\n",
    "        x1mean = self.mean_pool(x1)\n",
    "        x1mean = self.bn1mean(x1mean, training=training)\n",
    "        x1max = self.max_pool(x1)\n",
    "        x1max = self.bn1max(x1max, training=training)\n",
    "        x1min = -self.min_pool(-x1)\n",
    "        x1min = self.bn1min(x1min, training=training)\n",
    "        x1p = tf.concat([tf.keras.layers.Flatten()(x1mean),tf.keras.layers.Flatten()(x1max),tf.keras.layers.Flatten()(x1min)],axis=-1)\n",
    "\n",
    "        x2mean = self.mean_pool(x2)\n",
    "        x2mean = self.bn2mean(x2mean, training=training)\n",
    "        x2max = self.max_pool(x2)\n",
    "        x2max = self.bn2max(x2max, training=training)\n",
    "        x2min = -self.min_pool(-x2)\n",
    "        x2min = self.bn2min(x2min, training=training)\n",
    "        x2p = tf.concat([tf.keras.layers.Flatten()(x2mean),tf.keras.layers.Flatten()(x2max),tf.keras.layers.Flatten()(x2min)],axis=-1)\n",
    "\n",
    "        x3mean = self.mean_pool(x3)\n",
    "        x3mean = self.bn3mean(x3mean, training=training)\n",
    "        x3max = self.max_pool(x3)\n",
    "        x3max = self.bn3max(x3max, training=training)\n",
    "        x3min = -self.min_pool(-x3)\n",
    "        x3min = self.bn3min(x3min, training=training)\n",
    "        x3p = tf.concat([tf.keras.layers.Flatten()(x3mean),tf.keras.layers.Flatten()(x3max),tf.keras.layers.Flatten()(x3min)],axis=-1)\n",
    "\n",
    "        x4mean = self.mean_pool(x4)\n",
    "        x4mean = self.bn4mean(x4mean, training=training)\n",
    "        x4max = self.max_pool(x4)\n",
    "        x4max = self.bn4max(x4max, training=training)\n",
    "        x4min = -self.min_pool(-x4)\n",
    "        x4min = self.bn4min(x4min, training=training)\n",
    "        x4p = tf.concat([tf.keras.layers.Flatten()(x4mean),tf.keras.layers.Flatten()(x4max),tf.keras.layers.Flatten()(x4min)],axis=-1)\n",
    "\n",
    "        x5mean = self.mean_pool(x5)\n",
    "        x5mean = self.bn5mean(x5mean, training=training)\n",
    "        x5max = self.max_pool(x5)\n",
    "        x5max = self.bn5max(x5max, training=training)\n",
    "        x5min = -self.min_pool(-x5)\n",
    "        x5min = self.bn5min(x5min, training=training)\n",
    "        x5p = tf.concat([tf.keras.layers.Flatten()(x5mean),tf.keras.layers.Flatten()(x5max),tf.keras.layers.Flatten()(x5min)],axis=-1)\n",
    "\n",
    "        x6mean = self.mean_pool(x6)\n",
    "        x6mean = self.bn6mean(x6mean, training=training)\n",
    "        x6max = self.max_pool(x6)\n",
    "        x6max = self.bn6max(x6max, training=training)\n",
    "        x6min = -self.min_pool(-x6)\n",
    "        x6min = self.bn6min(x6min, training=training)\n",
    "        x6p = tf.concat([tf.keras.layers.Flatten()(x6mean),tf.keras.layers.Flatten()(x6max),tf.keras.layers.Flatten()(x6min)],axis=-1)\n",
    "\n",
    "        x7mean = self.mean_pool(x7)\n",
    "        x7mean = self.bn7mean(x7mean, training=training)\n",
    "        x7max = self.max_pool(x7)\n",
    "        x7max = self.bn7max(x7max, training=training)\n",
    "        x7min = -self.min_pool(-x7)\n",
    "        x7min = self.bn7min(x7min, training=training)\n",
    "        x7p = tf.concat([tf.keras.layers.Flatten()(x7mean),tf.keras.layers.Flatten()(x7max),tf.keras.layers.Flatten()(x7min)],axis=-1)\n",
    "\n",
    "        #pooling output\n",
    "        pooling_output = tf.concat([x1p,x2p,x3p,x4p,x5p,x6p,x7p],axis=-1)\n",
    "\n",
    "        #output\n",
    "        output = tf.concat([feature_output,pooling_output],axis=-1)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_AlphaNet(feature_block, input_shape=(9, 30), output_shape=10, name=\"AlphaNet\", dropout_seed=None, version=1):\n",
    "    r\"\"\"\n",
    "    Build AlphaNet model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_block: alphanet_feature_Block\n",
    "        Feature extraction model.\n",
    "    input_shape: int or tuple. optional, default=(9, 30)\n",
    "        Input data shape. (NOT include batch size)\n",
    "    output_shape: int, optional, default=10\n",
    "        Output data shape, i.e. the length of predicted days.\n",
    "    name: str, optional, default=\"AlphaNet\"\n",
    "        Model name.\n",
    "    dropout_seed: None or int, optional, default=None\n",
    "        Random seed in \"Dropout\" layer.\n",
    "    version: {1, 2, 3}, optional, default=1\n",
    "        Version of AlphaNet. Details can be found in papers. Not supported for 2/3.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: tf.keras.Model\n",
    "        AlphaNet model.\n",
    "    \"\"\"\n",
    "    # input\n",
    "    inputs = keras.layers.Input(shape=input_shape)\n",
    "    # feature layers\n",
    "    x = feature_block(inputs)\n",
    "    # the rest\n",
    "    init = keras.initializers.TruncatedNormal(mean=0., stddev=1.)\n",
    "    if version == 1:   # AlphaNet-v1\n",
    "        x = keras.layers.Dropout(rate=0.5, seed=dropout_seed, name=\"Dropout1\")(x)\n",
    "        #x = keras.layers.Dense(30, activation=\"relu\", kernel_initializer=init, name=\"Dense\")(x)\n",
    "        #sigmoid\n",
    "        x = keras.layers.Dense(512, activation=\"sigmoid\", kernel_initializer=keras.initializers.TruncatedNormal(mean=0., stddev=1.), name=\"Dense1\")(x)\n",
    "        x = keras.layers.Dense(256, activation=\"sigmoid\", kernel_initializer=keras.initializers.TruncatedNormal(mean=0., stddev=1.), name=\"Dense2\")(x)\n",
    "        x = keras.layers.Dense(128, activation=\"sigmoid\", kernel_initializer=keras.initializers.TruncatedNormal(mean=0., stddev=1.), name=\"Dense3\")(x)\n",
    "        x = keras.layers.Dense(64, activation=\"sigmoid\", kernel_initializer=keras.initializers.TruncatedNormal(mean=0., stddev=1.), name=\"Dense4\")(x)\n",
    "        x = keras.layers.Dense(32, activation=\"sigmoid\", kernel_initializer=keras.initializers.TruncatedNormal(mean=0., stddev=1.), name=\"Dense5\")(x)\n",
    "    # elif version == 2:  # AlphaNet-v2\n",
    "    #     x = keras.layers.LSTM(30, activation=\"relu\", name=\"LSTM\")(x)\n",
    "    #     x = keras.layers.BatchNormalization(name=\"BN\")(x)\n",
    "    # elif version == 3:  # AlphaNet-v3\n",
    "    #     x = keras.layers.GRU(30, activation=\"relu\", name=\"GRU\")(x)\n",
    "    #     x = keras.layers.BatchNormalization(name=\"BN\")(x)\n",
    "    outputs = keras.layers.Dense(output_shape, activation=\"linear\", kernel_initializer=init, name=\"Output\")(x)\n",
    "\n",
    "    # create model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=name) \n",
    "\n",
    "    # complie setting\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    # if version == 1:\n",
    "    #     optimizer = keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "    # else:\n",
    "    #     optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    loss = keras.losses.MeanSquaredError()\n",
    "    metrics = [keras.metrics.MeanSquaredError(name=\"MSE\")]\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    # return\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([100, 9, 30])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = np.arange(1,10)\n",
    "cov = np.eye(9)*[2,3,6,7,2,9,3,5,3]\n",
    "testdata2 = tf.transpose(tf.constant(np.random.multivariate_normal(mean, cov, [100,30])), perm=[0, 2, 1])\n",
    "testdata2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "testb_feature = alphanet_feature_Block([10,3])\n",
    "testb = build_AlphaNet(testb_feature, dropout_seed=123, version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "modelb = testb(testdata2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(100, 10), dtype=float32, numpy=\narray([[-4.44954491e+00,  6.63651109e-01,  1.60305023e+00,\n         5.40556431e+00,  7.42941976e-01,  3.87378621e+00,\n         2.42731810e+00, -4.81731594e-01,  3.07331729e+00,\n        -4.88662195e+00],\n       [-4.28291225e+00,  3.47794592e-01,  3.10085511e+00,\n         5.06184292e+00,  1.59935105e+00,  2.90918231e+00,\n         2.07553840e+00, -9.40124035e-01,  3.32084370e+00,\n        -4.94252300e+00],\n       [-3.79847860e+00,  1.09543347e+00,  1.81403291e+00,\n         5.14057064e+00,  1.06107724e+00,  2.41810799e+00,\n         1.57561636e+00, -1.70758724e-01,  5.02442789e+00,\n        -4.05425787e+00],\n       [-4.93423176e+00,  1.17565250e+00,  2.02311444e+00,\n         4.41297770e+00,  4.01672244e-01,  2.53595781e+00,\n         1.72244239e+00, -3.31846565e-01,  4.18598223e+00,\n        -5.16961575e+00],\n       [-3.16069698e+00,  4.03118044e-01,  9.19850886e-01,\n         5.47121286e+00,  1.10889840e+00,  3.93278360e+00,\n         2.85367966e+00, -1.19515419e-01,  3.93332911e+00,\n        -4.08711863e+00],\n       [-4.45986176e+00,  2.61881948e-02,  1.60668409e+00,\n         4.59259892e+00,  1.54081857e+00,  6.37869239e-02,\n        -9.71954405e-01,  1.94316983e-01,  4.50370741e+00,\n        -5.31178141e+00],\n       [-4.36949539e+00, -2.96200126e-01,  1.86614466e+00,\n         5.60707378e+00,  3.94765675e-01,  4.71948862e+00,\n         2.96418166e+00, -2.41098404e-01,  1.99603808e+00,\n        -4.64139700e+00],\n       [-4.70133591e+00,  4.13929641e-01,  1.67072415e+00,\n         4.93612814e+00,  9.97368991e-02,  3.00856423e+00,\n         2.51981878e+00,  6.22224808e-01,  4.33771133e+00,\n        -5.12928486e+00],\n       [-4.89062500e+00,  3.71188521e-01,  3.33401632e+00,\n         4.30011845e+00,  8.10838819e-01,  2.59005046e+00,\n         2.18742943e+00, -8.30491304e-01,  3.43612790e+00,\n        -4.16140509e+00],\n       [-4.79698706e+00, -1.20190740e+00,  2.14403820e+00,\n         5.50763941e+00,  3.22076976e-01,  2.20240831e+00,\n         2.07288218e+00,  3.97989392e-01,  4.26152134e+00,\n        -4.79336452e+00],\n       [-3.66314268e+00,  1.41145265e+00,  7.90270627e-01,\n         5.09327459e+00,  1.04724526e+00,  3.12657571e+00,\n         2.21621704e+00,  1.29436076e-01,  3.02816319e+00,\n        -4.03614330e+00],\n       [-3.99122810e+00,  4.13088262e-01,  2.43635440e+00,\n         4.70593882e+00, -5.61590791e-01,  1.08992815e+00,\n         1.12322652e+00,  2.52896130e-01,  5.33379602e+00,\n        -3.07205248e+00],\n       [-3.27369142e+00,  1.71177924e-01,  2.49277425e+00,\n         4.41484976e+00,  3.55875969e-01,  5.82183897e-01,\n         2.24491835e-01, -5.30356586e-01,  5.21772242e+00,\n        -3.24112463e+00],\n       [-4.11036968e+00,  5.09912133e-01,  2.62896991e+00,\n         4.76918411e+00,  6.53708816e-01,  2.78003502e+00,\n         1.23737586e+00, -1.05955064e-01,  3.73310709e+00,\n        -4.96423197e+00],\n       [-4.62154627e+00,  8.42030346e-02,  2.04208279e+00,\n         4.32349443e+00,  5.89450419e-01,  2.85676479e+00,\n         2.54550028e+00, -1.66770995e-01,  4.25960016e+00,\n        -5.46501684e+00],\n       [-4.16489315e+00,  2.58703828e-01,  2.44690251e+00,\n         6.04933548e+00,  1.58697748e+00,  3.41574287e+00,\n         2.77924395e+00, -3.72588634e-02,  3.23458314e+00,\n        -5.11935091e+00],\n       [-4.14297104e+00,  5.95809758e-01,  3.32967448e+00,\n         4.30127859e+00,  1.36875165e+00,  3.39004183e+00,\n         1.51726663e+00, -8.06063890e-01,  3.78293228e+00,\n        -4.14571619e+00],\n       [-3.99221563e+00,  1.91455007e-01,  2.15891194e+00,\n         5.71637583e+00,  1.41871822e+00,  3.69291568e+00,\n         1.85380042e+00, -7.41605699e-01,  3.16685677e+00,\n        -4.98793554e+00],\n       [-5.20884085e+00, -3.19587916e-01,  1.59363413e+00,\n         4.08167934e+00,  3.48178446e-02,  2.25667167e+00,\n         2.15205026e+00,  1.31580055e-01,  4.44912148e+00,\n        -5.21272659e+00],\n       [-4.25066423e+00,  3.79870951e-01,  1.84647715e+00,\n         4.23125172e+00,  2.50228822e-01,  2.84157801e+00,\n         1.59801853e+00,  6.02423549e-01,  4.63233089e+00,\n        -3.96718025e+00],\n       [-3.78682852e+00, -4.35391366e-01,  2.12231946e+00,\n         6.04546833e+00,  1.22000635e+00,  3.31378388e+00,\n         2.83244538e+00,  8.38074327e-01,  2.91744351e+00,\n        -4.38266230e+00],\n       [-4.26561928e+00,  5.73346913e-01,  1.27558076e+00,\n         4.29886580e+00,  1.08582568e+00,  2.46799970e+00,\n         2.01401210e+00,  3.92028868e-01,  4.67684889e+00,\n        -5.73015642e+00],\n       [-4.57925558e+00, -8.69660735e-01,  1.46448505e+00,\n         4.75121498e+00,  1.75212920e-01,  3.50416350e+00,\n         3.28068972e+00,  5.68775356e-01,  3.65597177e+00,\n        -5.90905476e+00],\n       [-4.89076328e+00,  9.06435609e-01,  2.15776110e+00,\n         4.09131575e+00, -3.60224247e-02,  3.14459038e+00,\n         2.15109015e+00,  8.30695629e-02,  4.15706730e+00,\n        -4.69471359e+00],\n       [-4.06257677e+00,  2.76301324e-01,  1.72538197e+00,\n         4.78825283e+00,  8.72704804e-01,  3.57100177e+00,\n         2.26297545e+00,  4.30169463e-01,  3.53900647e+00,\n        -5.01751232e+00],\n       [-4.81585836e+00, -6.31025791e-01,  1.39154530e+00,\n         3.60386801e+00, -8.38169456e-03,  2.98533821e+00,\n         1.74807227e+00,  4.58814323e-01,  3.99468780e+00,\n        -5.32871389e+00],\n       [-4.58052397e+00, -2.41863489e-01,  1.69452286e+00,\n         5.34675932e+00,  1.45829928e+00,  4.35994923e-01,\n        -1.64787173e+00, -1.13747478e+00,  4.74091244e+00,\n        -4.91434097e+00],\n       [-4.55143976e+00,  1.36088371e-01,  1.17154348e+00,\n         5.39030266e+00,  4.58222777e-01,  3.49621344e+00,\n         2.82900524e+00,  9.40484405e-01,  3.86802101e+00,\n        -5.54762030e+00],\n       [-4.09533310e+00, -2.61216372e-01,  2.77238560e+00,\n         4.44215250e+00,  8.85474265e-01,  3.50382543e+00,\n         2.26316261e+00, -1.48119867e-01,  4.80869055e+00,\n        -4.92051935e+00],\n       [-4.26131678e+00,  8.18339705e-01,  3.59811997e+00,\n         3.54995155e+00,  2.17682838e-01,  1.49581695e+00,\n         1.04592705e+00, -2.22144365e-01,  5.72063732e+00,\n        -2.77003407e+00],\n       [-2.51716685e+00,  2.17692900e+00,  4.54361391e+00,\n         4.22326660e+00,  8.00388217e-01,  1.20744646e+00,\n        -2.33093667e+00, -9.24102843e-01,  5.09760237e+00,\n        -1.74764335e+00],\n       [-4.50545502e+00, -2.19073921e-01,  3.22352791e+00,\n         4.44486570e+00, -3.23394895e-01,  1.21341419e+00,\n         7.42626190e-01,  1.05186343e-01,  5.37647724e+00,\n        -2.10145855e+00],\n       [-4.19101810e+00,  1.07357323e+00,  1.33925486e+00,\n         5.33395767e+00,  1.91723990e+00,  1.04396725e+00,\n         4.07287002e-01, -1.94045126e-01,  4.73311853e+00,\n        -4.10698128e+00],\n       [-5.00103331e+00, -1.12987846e-01,  1.88112152e+00,\n         4.37247944e+00, -2.30523884e-01,  2.70158148e+00,\n         2.33121276e+00,  7.00811505e-01,  4.50927925e+00,\n        -4.81284952e+00],\n       [-4.26774740e+00,  6.31995857e-01,  2.11073852e+00,\n         4.28025675e+00,  3.77868116e-01,  2.80224419e+00,\n         8.53007078e-01,  8.26144814e-02,  3.82718277e+00,\n        -3.40925169e+00],\n       [-4.55910444e+00, -1.68580818e+00,  1.82399118e+00,\n         4.84247398e+00, -2.55352139e-01,  3.67200351e+00,\n         2.93305326e+00,  6.63279772e-01,  3.33834481e+00,\n        -5.15314293e+00],\n       [-4.35679531e+00,  9.18853641e-01,  2.34684277e+00,\n         4.94179249e+00,  8.01336050e-01,  2.74383426e+00,\n         2.18811369e+00, -5.69963574e-01,  4.22116661e+00,\n        -3.86861062e+00],\n       [-3.48613167e+00, -1.32311821e-01,  1.04923511e+00,\n         5.46400261e+00,  1.70636845e+00,  2.48609996e+00,\n         1.30495751e+00, -2.93931365e-01,  2.52338099e+00,\n        -5.34216642e+00],\n       [-5.05637074e+00, -2.40274072e-02,  3.04396796e+00,\n         4.13167524e+00, -1.67036831e-01,  2.85103226e+00,\n         2.56515336e+00, -2.19552338e-01,  4.59180975e+00,\n        -3.85200429e+00],\n       [-4.45720959e+00, -4.85391706e-01,  2.83157039e+00,\n         4.99253798e+00,  2.56492662e+00,  1.39655960e+00,\n         5.86325884e-01, -7.39516795e-01,  3.73381996e+00,\n        -5.23768711e+00],\n       [-3.91781712e+00, -5.23897111e-01,  2.96970725e+00,\n         6.06751108e+00,  9.90234494e-01,  2.44071007e+00,\n         1.77221620e+00, -2.13231564e-01,  4.32073355e+00,\n        -3.70664167e+00],\n       [-4.33716154e+00, -3.24796557e-01,  2.49252987e+00,\n         4.99622583e+00,  9.14883196e-01,  2.47003078e+00,\n         2.37418628e+00,  2.10229993e-01,  3.01051760e+00,\n        -3.62745929e+00],\n       [-4.28362799e+00,  2.13877976e-01,  2.06257081e+00,\n         6.06288147e+00,  1.35130847e+00,  3.31646156e+00,\n         3.03115845e+00,  3.12393367e-01,  2.96630478e+00,\n        -5.17207241e+00],\n       [-4.38897276e+00,  3.46977234e-01,  3.38240433e+00,\n         5.17455530e+00,  8.18974435e-01,  5.19842088e-01,\n        -6.93453729e-01, -6.45179689e-01,  5.78161430e+00,\n        -2.57945704e+00],\n       [-4.46045256e+00,  4.36518669e-01,  2.91515374e+00,\n         3.98374534e+00,  1.17778480e+00,  9.48721468e-01,\n         1.68209875e+00, -3.90484929e-01,  3.89046407e+00,\n        -3.56035066e+00],\n       [-4.83083677e+00, -1.50108129e-01,  2.60199022e+00,\n         4.35980034e+00,  5.91255844e-01,  2.99059510e+00,\n         2.41957211e+00, -5.13764501e-01,  3.64729905e+00,\n        -4.42049360e+00],\n       [-3.87179661e+00,  7.79427528e-01,  2.78846693e+00,\n         4.10249424e+00,  1.03174388e+00,  5.67260861e-01,\n        -7.15187907e-01,  1.19273686e+00,  5.69387627e+00,\n        -3.49280858e+00],\n       [-4.30755663e+00,  1.52096152e-02,  2.18685269e+00,\n         5.20411205e+00,  1.64075971e+00,  1.60394442e+00,\n         8.82579386e-01, -4.78077173e-01,  4.13535500e+00,\n        -4.87641859e+00],\n       [-4.17338371e+00,  3.00290585e-01,  3.85956526e+00,\n         5.17992401e+00,  1.76086736e+00,  1.96691430e+00,\n        -7.61640549e-01, -8.27039957e-01,  4.28147602e+00,\n        -3.28758240e+00],\n       [-4.86911917e+00,  1.70459211e-01,  2.73553085e+00,\n         5.08499050e+00,  1.29030848e+00,  2.88782716e+00,\n         1.29746878e+00, -1.12861919e+00,  3.12272477e+00,\n        -4.47390890e+00],\n       [-4.46953344e+00,  6.34681225e-01,  2.98978496e+00,\n         3.69727087e+00, -1.16107082e+00,  4.72955108e-01,\n        -9.34809387e-01, -4.58926409e-01,  6.40621519e+00,\n        -2.24334931e+00],\n       [-3.56445098e+00,  1.15474260e+00,  2.49309587e+00,\n         5.27005482e+00,  1.80318296e+00,  2.25165796e+00,\n         1.94061363e+00, -7.27554679e-01,  3.97417068e+00,\n        -4.24923992e+00],\n       [-4.36305094e+00, -4.05919015e-01,  1.60858369e+00,\n         6.23512840e+00,  9.96964335e-01,  3.78055120e+00,\n         3.06287336e+00, -1.23414576e-01,  2.65362763e+00,\n        -5.15790033e+00],\n       [-4.17420816e+00,  9.57242370e-01,  2.60550833e+00,\n         4.84564304e+00,  3.61567855e-01,  3.20989680e+00,\n         1.01865506e+00,  1.43746138e-01,  4.28289175e+00,\n        -4.81052828e+00],\n       [-4.54729939e+00, -1.55319560e+00,  2.02301145e+00,\n         5.73875618e+00,  1.84482336e+00,  8.36468995e-01,\n        -8.39497507e-01, -1.07659316e+00,  3.39490485e+00,\n        -4.05162382e+00],\n       [-3.39127779e+00,  8.98657084e-01,  2.30626202e+00,\n         4.96363783e+00,  1.48634219e+00,  1.77942920e+00,\n         4.02621686e-01,  2.40774274e-01,  4.09652758e+00,\n        -3.42125416e+00],\n       [-3.69650960e+00,  3.70044172e-01,  1.65098250e+00,\n         6.39413118e+00,  1.40935063e+00,  2.68603301e+00,\n         2.21277833e+00,  7.55440354e-01,  4.22505713e+00,\n        -3.49833536e+00],\n       [-4.38086987e+00,  3.13101947e-01,  3.23353648e+00,\n         5.20310163e+00,  2.13208938e+00,  2.49711442e+00,\n         7.19559908e-01, -1.56677437e+00,  3.61134720e+00,\n        -4.04454565e+00],\n       [-3.98415565e+00, -7.21770525e-02,  2.72796869e+00,\n         4.70245171e+00,  4.57003534e-01,  2.42292428e+00,\n        -3.68781567e-01, -2.71343350e-01,  5.55778694e+00,\n        -4.45734549e+00],\n       [-4.02751637e+00,  1.05844378e-01,  3.54948926e+00,\n         5.18120146e+00,  2.13130498e+00,  2.42396331e+00,\n         1.44157672e+00, -1.33090305e+00,  4.13071108e+00,\n        -4.27743053e+00],\n       [-4.60176420e+00, -1.69081807e-01,  2.15304422e+00,\n         5.85397387e+00,  1.82297778e+00,  1.95760965e+00,\n         1.13623095e+00, -3.42572391e-01,  3.41758752e+00,\n        -4.16474724e+00],\n       [-4.13181496e+00,  3.61620009e-01,  2.03566384e+00,\n         5.55766964e+00,  9.38321888e-01,  3.48928690e+00,\n         2.86393642e+00, -1.98694348e-01,  3.51775837e+00,\n        -4.88107777e+00],\n       [-4.39059401e+00, -8.52763653e-04,  2.55033898e+00,\n         5.28655005e+00,  8.96997154e-01,  2.75720119e+00,\n         2.03066921e+00,  2.80628026e-01,  3.42705941e+00,\n        -3.79309750e+00],\n       [-4.78732300e+00, -3.08939993e-01,  8.35187793e-01,\n         5.04403734e+00, -2.23872900e-01,  3.19943070e+00,\n         3.29011726e+00,  1.74524629e+00,  3.61434579e+00,\n        -4.88812399e+00],\n       [-4.15222645e+00, -7.36158907e-01,  2.63012862e+00,\n         4.79503632e+00,  1.25947273e+00,  2.44702911e+00,\n         2.32091951e+00, -2.22253799e-03,  4.68740320e+00,\n        -5.81701040e+00],\n       [-4.17730999e+00, -2.85886377e-01,  1.56823206e+00,\n         6.63119125e+00,  6.92648232e-01,  3.73192835e+00,\n         3.80497932e+00,  1.02392411e+00,  3.92103958e+00,\n        -5.64302111e+00],\n       [-4.18903780e+00, -1.99495482e+00,  3.58092737e+00,\n         4.11299992e+00,  1.35162830e-01,  3.00327086e+00,\n         1.74695528e+00,  7.78571606e-01,  3.77376556e+00,\n        -4.92112494e+00],\n       [-4.35519695e+00, -4.10226941e-01,  2.33560896e+00,\n         5.01629066e+00,  9.71631050e-01,  3.34605885e+00,\n         2.86110926e+00, -1.75137043e-01,  2.86116338e+00,\n        -5.24636745e+00],\n       [-4.04002714e+00,  4.13498282e-01,  2.89787292e+00,\n         4.74499846e+00,  7.00001061e-01,  2.23491263e+00,\n         8.63653958e-01, -3.29956710e-01,  3.59242702e+00,\n        -2.96242547e+00],\n       [-4.31696367e+00, -6.67743444e-01,  1.97476029e+00,\n         5.82999277e+00,  7.83148110e-01,  3.68874407e+00,\n         3.07059932e+00, -4.78594899e-02,  2.73323774e+00,\n        -4.61998701e+00],\n       [-3.61027956e+00,  1.94512904e-01,  3.34007287e+00,\n         5.83668375e+00,  1.70906949e+00,  3.78228474e+00,\n         1.89577818e+00, -5.32772779e-01,  2.91792703e+00,\n        -3.73310328e+00],\n       [-3.41211033e+00,  2.42058396e-01,  2.70970178e+00,\n         4.83908463e+00,  1.33529437e+00,  2.29933524e+00,\n         2.86415815e-01, -8.93225968e-01,  4.39391232e+00,\n        -3.12441111e+00],\n       [-4.49646139e+00,  5.01354694e-01,  3.56176186e+00,\n         3.80439663e+00, -3.76553416e-01,  1.97814083e+00,\n        -5.13740420e-01, -2.96238959e-01,  5.15716505e+00,\n        -2.39181828e+00],\n       [-4.35060883e+00,  1.41988766e+00,  2.26863527e+00,\n         4.05302858e+00, -7.93443739e-01,  2.26719332e+00,\n         1.03016853e+00,  5.55030525e-01,  5.42164993e+00,\n        -3.14902163e+00],\n       [-4.67679739e+00,  2.83010006e-02,  9.73169386e-01,\n         4.62948179e+00, -4.98915911e-01,  2.06367421e+00,\n         1.63212430e+00,  1.26787615e+00,  5.60451555e+00,\n        -4.27786589e+00],\n       [-3.57205868e+00, -3.28627348e-01,  3.77586198e+00,\n         3.23387718e+00,  2.22313666e+00,  2.77907395e+00,\n         5.31181633e-01, -4.41699803e-01,  3.08297062e+00,\n        -3.88822246e+00],\n       [-4.64803791e+00,  6.49725497e-01,  1.75938451e+00,\n         4.39172792e+00,  2.71429658e-01,  2.24050331e+00,\n         2.08964491e+00,  7.48494506e-01,  4.28517580e+00,\n        -4.71990824e+00],\n       [-2.97233653e+00,  5.16764700e-01,  8.09565842e-01,\n         4.18331861e+00,  4.90916431e-01,  2.03576016e+00,\n         1.56809723e+00,  8.19122791e-01,  4.39589739e+00,\n        -3.82921529e+00],\n       [-4.44074345e+00,  5.18157780e-02,  1.61601400e+00,\n         5.20162964e+00,  6.22001290e-01,  3.67391253e+00,\n         2.43319106e+00,  8.32109332e-01,  4.63036585e+00,\n        -5.76648951e+00],\n       [-3.10582352e+00, -4.28555787e-01,  2.46744275e+00,\n         7.46494436e+00,  3.07110405e+00,  3.61572766e+00,\n         1.52202559e+00,  3.19474339e-01,  3.92441773e+00,\n        -4.99520493e+00],\n       [-3.90785003e+00, -2.19095230e-01,  1.06287622e+00,\n         5.64407396e+00,  1.85512590e+00,  2.12493491e+00,\n         2.22164750e+00,  4.48895097e-01,  2.55418730e+00,\n        -5.36763382e+00],\n       [-3.19595003e+00,  4.26190197e-01,  1.38299954e+00,\n         5.30148363e+00,  2.47980785e+00,  3.38899374e+00,\n         8.20411384e-01, -7.31236398e-01,  2.13513994e+00,\n        -4.77875710e+00],\n       [-4.81801128e+00,  7.99221575e-01,  2.61464095e+00,\n         4.95042562e+00,  1.60973799e+00,  2.46514511e+00,\n         8.51475835e-01, -1.44099724e+00,  3.25667024e+00,\n        -4.42302227e+00],\n       [-3.90230823e+00,  1.10151100e+00,  4.12295961e+00,\n         4.63204098e+00,  9.95333195e-02,  2.44816422e+00,\n        -1.55119449e-01, -7.37324655e-01,  4.68612766e+00,\n        -2.61065364e+00],\n       [-4.39473677e+00,  9.52691913e-01,  3.80865335e+00,\n         2.64983296e+00,  5.15074730e-02,  1.86344159e+00,\n         4.61248398e-01, -8.90512109e-01,  4.03047323e+00,\n        -2.64191532e+00],\n       [-3.50324321e+00,  2.67731428e-01,  1.21983683e+00,\n         5.48498344e+00,  1.15600407e+00,  2.86372280e+00,\n         2.58539104e+00,  4.39551234e-01,  4.37905216e+00,\n        -5.41926765e+00],\n       [-4.38988209e+00, -8.84949565e-02,  2.62821841e+00,\n         4.47911549e+00,  9.81413066e-01,  2.78150201e+00,\n         1.80516541e+00, -2.77971506e-01,  4.58538389e+00,\n        -5.12093973e+00],\n       [-4.66021919e+00,  2.78656185e-02,  1.86712360e+00,\n         5.23305655e+00,  3.65262270e-01,  3.18185759e+00,\n         3.18252277e+00,  4.01853144e-01,  3.96768427e+00,\n        -5.83648539e+00],\n       [-4.72472429e+00,  3.99542451e-02,  3.08041596e+00,\n         3.78642440e+00,  4.75316048e-01,  3.01807141e+00,\n         2.04857445e+00, -6.06547594e-02,  3.34331679e+00,\n        -3.99119473e+00],\n       [-3.10674071e+00,  2.63801873e-01,  3.17128181e+00,\n         6.05495596e+00,  1.10004377e+00,  8.69265378e-01,\n         9.56378877e-01,  4.15986657e-01,  5.58836794e+00,\n        -2.10853171e+00],\n       [-4.54476595e+00,  2.83962309e-01,  2.84464288e+00,\n         5.21330261e+00,  6.24406695e-01,  3.36246777e+00,\n         2.33282614e+00, -1.76329136e-01,  4.24502611e+00,\n        -4.57167530e+00],\n       [-3.65047264e+00, -6.05863333e-03, -4.90512967e-01,\n         4.58256721e+00,  1.42751312e+00,  1.84564543e+00,\n         1.46199834e+00,  7.02814579e-01,  3.81548667e+00,\n        -5.40856504e+00],\n       [-3.53459430e+00,  4.61631954e-01,  1.39739442e+00,\n         3.99251342e+00,  7.24209547e-02,  1.78182268e+00,\n         1.73241913e+00,  8.09410095e-01,  5.07849550e+00,\n        -3.61820984e+00],\n       [-3.18621230e+00, -1.77172244e-01,  9.84296739e-01,\n         5.87397623e+00,  1.43678784e+00,  2.14020634e+00,\n         1.51076031e+00,  1.10664999e+00,  4.59038067e+00,\n        -4.16108274e+00],\n       [-3.84969401e+00,  3.72165740e-01,  3.59407258e+00,\n         4.35781622e+00,  4.25950825e-01,  1.08266842e+00,\n         4.21592891e-01, -3.74447405e-01,  5.38144732e+00,\n        -3.14696145e+00],\n       [-4.88105059e+00, -1.49669409e-01,  2.47262955e+00,\n         4.43435287e+00,  1.10171366e+00,  6.00433052e-01,\n         1.33166730e-01,  2.22570658e-01,  3.80730534e+00,\n        -3.22401786e+00],\n       [-4.01138258e+00,  3.13680470e-02,  2.27293396e+00,\n         5.51839161e+00,  7.21450031e-01,  4.37544441e+00,\n         2.90371418e+00,  6.63193464e-01,  2.85132933e+00,\n        -4.94086504e+00],\n       [-4.34256983e+00,  5.98762989e-01,  2.39012384e+00,\n         5.57503843e+00,  2.46415544e+00,  1.79507756e+00,\n        -1.05256617e+00, -8.83796930e-01,  4.67388153e+00,\n        -4.62921286e+00],\n       [-4.30681038e+00,  6.00126088e-02,  1.11333811e+00,\n         4.76538897e+00,  8.08362663e-01,  3.78251076e+00,\n         2.92338800e+00,  2.57001638e-01,  3.36256671e+00,\n        -6.07110405e+00],\n       [-4.38540602e+00,  2.17510641e-01,  3.47366118e+00,\n         4.38539410e+00,  1.19417620e+00,  2.72979188e+00,\n         2.27655506e+00, -6.64443731e-01,  4.04044390e+00,\n        -4.94791412e+00]], dtype=float32)>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[<keras.engine.input_layer.InputLayer at 0x24add4efee0>,\n <__main__.alphanet_feature_Block at 0x24add4efd60>,\n <keras.layers.core.Dropout at 0x24add593f10>,\n <keras.layers.core.Dense at 0x24af2670fd0>,\n <keras.layers.core.Dense at 0x24af2670310>,\n <keras.layers.core.Dense at 0x24af2678700>,\n <keras.layers.core.Dense at 0x24af2628130>,\n <keras.layers.core.Dense at 0x24acf96c040>,\n <keras.layers.core.Dense at 0x24addaad730>]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testb.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# 7+7*3+3=31\n",
    "print(len(testb_feature.layers))\n",
    "# input + feature + dropout + dense * 2\n",
    "print(len(testb.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AlphaNet\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 9, 30)]           0         \n",
      "_________________________________________________________________\n",
      "alphanet_feature__block (alp (None, 702)               112       \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 702)               0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 512)               359936    \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "Dense3 (Dense)               (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "Dense4 (Dense)               (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "Dense5 (Dense)               (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 534,938\n",
      "Trainable params: 534,882\n",
      "Non-trainable params: 56\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "testb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Variable 'alphanet_feature__block/batch_normalization/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_1/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_1/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_1/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_1/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_2/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_2/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_2/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_2/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_3/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_3/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_3/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_3/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_4/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_4/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_4/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_4/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_5/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_5/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_5/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_5/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_6/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_6/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_6/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_6/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_7/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_7/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_7/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_7/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_8/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_8/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_8/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_8/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_9/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_9/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_9/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_9/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_10/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_10/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_10/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_10/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_11/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_11/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_11/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_11/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_12/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_12/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_12/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_12/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_13/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_13/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_13/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_13/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_14/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_14/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_14/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_14/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_15/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_15/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_15/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_15/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_16/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_16/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_16/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_16/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_17/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_17/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_17/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_17/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_18/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_18/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_18/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_18/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_19/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_19/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_19/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_19/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_20/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_20/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_20/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_20/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_21/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_21/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_21/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_21/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_22/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_22/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_22/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_22/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_23/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_23/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_23/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_23/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_24/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_24/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_24/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_24/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_25/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_25/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_25/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_25/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_26/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_26/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_26/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_26/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_27/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_27/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_27/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_27/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'Dense1/kernel:0' shape=(702, 512) dtype=float32, numpy=\n array([[ 1.9912306 , -0.8221895 ,  0.8752249 , ..., -1.0255218 ,\n         -0.43931496,  0.9367493 ],\n        [-0.1387854 , -0.02847961, -1.0871227 , ..., -0.5592969 ,\n         -1.3062516 , -1.3545406 ],\n        [-0.459353  ,  0.5718273 ,  0.966423  , ..., -0.80841696,\n          0.08074316, -0.9889902 ],\n        ...,\n        [-1.2877312 , -0.48700872, -0.08941682, ...,  0.2521914 ,\n          0.6063366 , -0.02848599],\n        [ 0.01817947,  0.19908926,  1.5844264 , ..., -0.5560323 ,\n         -0.30574775, -0.19883345],\n        [-0.9412598 ,  0.9472968 , -1.1719016 , ..., -0.95788383,\n          0.07877224,  0.6875288 ]], dtype=float32)>,\n <tf.Variable 'Dense1/bias:0' shape=(512,) dtype=float32, numpy=\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.], dtype=float32)>,\n <tf.Variable 'Dense2/kernel:0' shape=(512, 256) dtype=float32, numpy=\n array([[ 1.0712322 , -0.6626173 ,  1.0761105 , ...,  0.17718318,\n          0.5934858 ,  0.8292505 ],\n        [-1.1712369 ,  0.15677051, -1.1606642 , ...,  0.39728877,\n         -0.91954064, -0.6592344 ],\n        [-0.5848861 ,  0.51836777, -0.5070846 , ..., -1.6122835 ,\n          0.26232138, -0.9206136 ],\n        ...,\n        [ 1.2727008 , -0.19932601,  0.21211709, ...,  0.50447965,\n         -1.6665744 ,  1.3144046 ],\n        [-0.1558052 , -0.9188282 ,  0.54051346, ...,  0.5116784 ,\n          1.4165523 , -0.25789654],\n        [-0.73250914, -0.51295984, -0.30320185, ..., -1.2011743 ,\n         -0.72439903, -1.9576212 ]], dtype=float32)>,\n <tf.Variable 'Dense2/bias:0' shape=(256,) dtype=float32, numpy=\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.], dtype=float32)>,\n <tf.Variable 'Dense3/kernel:0' shape=(256, 128) dtype=float32, numpy=\n array([[-1.6152960e-01,  4.5116916e-01, -9.8824406e-01, ...,\n          3.4625912e-01,  4.3180558e-01,  6.7272514e-01],\n        [-5.3302294e-01,  1.4619846e+00, -5.4328793e-01, ...,\n          1.2953635e+00,  9.2991167e-01,  5.3207964e-01],\n        [-1.2356176e+00, -9.6804792e-01, -4.5400238e-01, ...,\n         -1.6137132e-03,  1.7138315e+00,  2.1456882e-01],\n        ...,\n        [-5.0005132e-01,  2.1013731e-01, -8.3500600e-01, ...,\n          2.9046828e-01,  7.4975735e-01, -6.9298404e-01],\n        [-5.3192246e-01, -1.3055450e+00, -2.5892687e-01, ...,\n         -1.0380900e+00, -2.5730944e-01, -5.4789597e-01],\n        [ 1.5882020e-01, -3.9173317e-01, -5.4643029e-01, ...,\n         -3.3901402e-01,  3.5083517e-01, -9.9091685e-01]], dtype=float32)>,\n <tf.Variable 'Dense3/bias:0' shape=(128,) dtype=float32, numpy=\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n <tf.Variable 'Dense4/kernel:0' shape=(128, 64) dtype=float32, numpy=\n array([[ 0.1609259 , -1.5227389 ,  0.59372646, ...,  0.04508969,\n          1.1984236 , -0.5020886 ],\n        [ 0.6933104 , -0.60260797, -0.14671181, ...,  0.7453217 ,\n         -0.46787742,  0.46659842],\n        [-0.37700897,  1.4800557 , -1.0717852 , ..., -0.66954935,\n          0.37693986, -0.61539257],\n        ...,\n        [-0.6380992 , -0.91479594,  0.17298883, ..., -0.29906815,\n          0.10580677, -0.40482783],\n        [-1.075073  ,  0.26222977, -0.07193877, ...,  1.7659293 ,\n          0.48312804,  0.73978615],\n        [-0.9077992 , -0.01303987, -0.28933278, ...,  0.6231823 ,\n         -0.5513526 ,  0.66128075]], dtype=float32)>,\n <tf.Variable 'Dense4/bias:0' shape=(64,) dtype=float32, numpy=\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n <tf.Variable 'Dense5/kernel:0' shape=(64, 32) dtype=float32, numpy=\n array([[-0.12991585, -1.3328643 ,  0.818006  , ...,  0.9264998 ,\n          1.1279057 ,  1.5725018 ],\n        [ 0.34189954,  1.0760021 , -0.7487322 , ..., -0.15199566,\n         -0.00686615, -0.5583207 ],\n        [-0.8991091 , -0.9554086 , -0.07102153, ..., -0.92366904,\n         -0.8227351 , -0.9348092 ],\n        ...,\n        [-0.28967452, -0.906828  , -1.3760142 , ...,  0.5847466 ,\n          0.07535268,  1.0484593 ],\n        [-0.3935974 , -1.0153816 ,  1.0038232 , ..., -0.5490283 ,\n          1.3891667 ,  1.8285205 ],\n        [ 0.7502697 ,  0.06097689,  0.5377759 , ...,  0.92003495,\n          0.32617944,  0.37708747]], dtype=float32)>,\n <tf.Variable 'Dense5/bias:0' shape=(32,) dtype=float32, numpy=\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       dtype=float32)>,\n <tf.Variable 'Output/kernel:0' shape=(32, 10) dtype=float32, numpy=\n array([[ 4.08716738e-01,  1.42472029e+00, -9.87822771e-01,\n         -6.05120324e-02,  1.26514822e-01,  1.45256734e+00,\n         -1.31335771e+00,  1.43884325e+00,  4.58021015e-01,\n          2.16506496e-01],\n        [ 3.22677940e-01, -1.38743067e+00,  1.70281306e-01,\n         -8.36604983e-02, -1.91901386e+00,  1.40552485e+00,\n         -5.05896568e-01,  4.31099609e-02, -5.50548553e-01,\n         -9.74487185e-01],\n        [ 8.74779284e-01,  9.19646263e-01, -4.99054849e-01,\n          1.47638869e+00,  4.94607627e-01,  1.14005172e+00,\n          5.37645340e-01,  7.90497065e-02, -1.34905672e+00,\n          1.69922382e-01],\n        [ 6.83837712e-01, -3.60257566e-01,  1.14161360e+00,\n         -1.69480693e+00,  1.65584707e+00,  2.10975260e-01,\n         -8.58345389e-01,  2.23623648e-01,  2.93346077e-01,\n         -4.88253921e-01],\n        [-1.09837413e+00, -4.31499600e-01,  2.48015821e-01,\n         -5.69554627e-01, -2.90698558e-01,  5.89745104e-01,\n          6.37226045e-01,  1.20282888e+00, -4.85405065e-02,\n          1.27262667e-01],\n        [ 8.39208663e-01, -6.08810484e-01, -3.59799862e-01,\n          1.63342834e+00,  1.59785271e+00,  6.40456796e-01,\n         -1.16817534e+00,  2.99412966e-01, -4.10925418e-01,\n         -4.98011768e-01],\n        [-8.86658967e-01, -8.66774082e-01,  1.89120281e+00,\n          1.50254822e+00,  9.42270637e-01,  2.37717867e-01,\n          1.00977182e+00, -4.77093399e-01,  5.12457967e-01,\n         -1.13823026e-01],\n        [-1.78297496e+00,  1.20365500e+00, -1.15500057e+00,\n          1.19136417e+00, -6.76572263e-01,  1.23113644e+00,\n         -1.36140835e+00,  1.67336094e+00,  5.29579878e-01,\n         -5.47601104e-01],\n        [ 5.63436449e-01,  3.49699080e-01, -5.98132432e-01,\n          2.83530112e-02,  1.04851162e+00,  1.10353082e-02,\n         -1.14386618e+00, -4.85693157e-01,  2.47689545e-01,\n          6.83104917e-02],\n        [-4.75980639e-01, -3.57439041e-01, -7.01614916e-01,\n          7.97750771e-01,  6.89953744e-01, -1.30607283e+00,\n         -1.36265838e+00, -8.72898102e-01, -2.19639894e-02,\n          3.01651835e-01],\n        [-1.40810955e+00, -7.57242978e-01,  1.30021262e+00,\n         -1.27190864e+00, -1.13579047e+00,  8.27233672e-01,\n         -5.47646105e-01,  2.88988590e-01, -1.81834534e-01,\n         -1.25686157e+00],\n        [-1.60135198e+00,  5.19891500e-01,  1.65615022e+00,\n          2.13052630e-01, -1.47300258e-01, -1.48611438e+00,\n          7.42254198e-01, -5.57127833e-01, -3.30248117e-01,\n         -1.58635652e+00],\n        [-1.00694299e+00,  5.69276214e-01, -4.27888721e-01,\n         -5.19946635e-01, -8.49659801e-01, -1.68503985e-01,\n          1.35134423e+00, -9.81274962e-01,  8.63273621e-01,\n         -1.34163523e+00],\n        [-2.98780680e-01,  1.31577313e+00,  1.53241014e+00,\n         -2.39072964e-01, -2.86266536e-01,  5.93776405e-01,\n         -4.79540139e-01, -8.58599663e-01,  2.87562400e-01,\n          1.83562481e+00],\n        [-1.99533716e-01, -4.48159665e-01, -6.08129859e-01,\n         -7.50931680e-01,  1.38252258e-01, -7.65328169e-01,\n         -1.11649609e+00,  2.22029075e-01,  1.92161500e+00,\n         -1.93059456e+00],\n        [-4.76100206e-01,  6.45169854e-01, -5.31377733e-01,\n          9.34029162e-01,  5.05692422e-01, -1.35158610e+00,\n         -1.61778009e+00, -1.10502028e+00,  6.07826471e-01,\n         -4.65596139e-01],\n        [ 1.30135170e-03, -1.97440647e-02,  4.65191960e-01,\n          4.35936391e-01, -2.36237615e-01, -9.22790825e-01,\n          1.60167921e+00,  1.04372847e+00,  1.54104817e+00,\n         -6.23346090e-01],\n        [ 3.18013906e-01, -1.07407212e+00,  2.99355030e-01,\n          1.31936699e-01,  7.20782697e-01,  9.07217503e-01,\n          1.94197214e+00, -3.12909156e-01,  3.94404352e-01,\n          5.34350038e-01],\n        [ 6.57722592e-01, -5.91210663e-01, -9.01822448e-01,\n          5.93983769e-01,  5.88577330e-01, -2.96424150e-01,\n         -6.99915349e-01,  4.41465229e-01,  1.27465832e+00,\n          1.33621031e-02],\n        [-5.82908630e-01, -4.46499139e-01, -3.13445717e-01,\n         -1.33333936e-01, -4.06381845e-01,  1.19442940e+00,\n         -4.44640338e-01,  1.46261916e-01,  5.10779858e-01,\n         -2.74789423e-01],\n        [ 5.29932268e-02, -9.06596839e-01, -4.97475453e-02,\n          1.99891472e+00, -3.43652785e-01,  6.00751579e-01,\n          1.54352975e+00,  1.36487830e+00, -7.31981844e-02,\n          9.09060299e-01],\n        [-1.00917149e+00, -8.28436792e-01, -7.21816540e-01,\n         -9.95767474e-01, -1.16381478e+00, -4.65469420e-01,\n         -4.34447974e-01,  1.24028909e+00,  1.62259206e-01,\n          6.79349303e-01],\n        [-2.33413756e-01,  1.27831709e+00, -5.61340630e-01,\n          6.09372020e-01,  6.93694293e-01, -4.70378488e-01,\n         -4.14793491e-01,  1.28693566e-01, -6.83866918e-01,\n          5.60081303e-01],\n        [ 3.95922929e-01,  6.22764528e-01,  1.21377850e+00,\n          7.96232522e-01, -4.28655207e-01, -1.09514385e-01,\n          3.22754741e-01, -1.38983798e+00,  1.03082180e-01,\n          1.20466292e+00],\n        [-7.43213952e-01, -8.83546591e-01, -1.92056656e+00,\n          2.02355787e-01,  3.37938339e-01,  8.40511143e-01,\n          3.45824897e-01,  2.66132921e-01, -1.73910487e+00,\n         -1.53201044e+00],\n        [-2.16582596e-01,  7.84705400e-01,  2.18369011e-02,\n         -4.85284537e-01,  2.40014419e-01, -7.63243511e-02,\n         -9.15507078e-01,  9.76390719e-01,  5.69786251e-01,\n         -2.63004303e-01],\n        [ 3.13019633e-01,  8.58720899e-01, -1.93147734e-02,\n         -1.29737511e-01, -1.55705512e+00, -2.69185871e-01,\n         -1.96126747e+00,  2.61725396e-01,  3.62015307e-01,\n          1.19630373e+00],\n        [ 9.71630633e-01,  6.45223916e-01,  4.68439817e-01,\n         -5.97199192e-04,  3.85159999e-02,  1.39626908e+00,\n          1.45033598e-01,  1.77085549e-01,  9.43639517e-01,\n          9.07929301e-01],\n        [ 6.24654353e-01, -4.64469433e-01, -8.46630931e-01,\n          1.48120189e+00,  8.58684123e-01,  4.67134714e-01,\n         -8.82398188e-01, -1.28188956e+00,  2.45947212e-01,\n         -1.07204521e+00],\n        [ 3.55515212e-01,  4.52326126e-02, -8.42161536e-01,\n         -1.29259586e-01,  2.88701832e-01, -1.58081090e+00,\n         -3.61235023e-01, -8.41430202e-02, -6.72507226e-01,\n         -2.33259335e-01],\n        [ 5.83456397e-01, -1.21301281e+00,  1.50022471e+00,\n          6.62045538e-01,  1.54652452e+00, -3.62880290e-01,\n         -9.76262465e-02, -4.31273937e-01, -1.06337881e+00,\n          4.91343975e-01],\n        [-3.57696742e-01, -1.05359983e+00,  8.53118122e-01,\n         -2.01851130e-01, -3.53903413e-01,  4.54994261e-01,\n          8.26018453e-01, -8.57191682e-01, -1.37719408e-01,\n          8.08602273e-01]], dtype=float32)>,\n <tf.Variable 'Output/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testb.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Real Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TRAIN_DIR = \"train_30\"\n",
    "VALID_DIR = \"valid_30\"\n",
    "SAVE_DIR = \"saved_model\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "def load_hdf5(file):\n",
    "    r\"\"\" \n",
    "    Load HDF5 data file and preprocess.\n",
    "    \"\"\"\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        X, y = f[\"X\"][...], f[\"y\"][...]\n",
    "    # preprocess: log(1+x)\n",
    "    X[5, :] = np.log(X[5, :] + 1)\n",
    "    X[6, :] = np.log(X[6, :] + 1)\n",
    "    y = np.log(y + 1)\n",
    "    return X, y\n",
    "\n",
    "def load_dataset(train_dir, valid_dir):\n",
    "    r\"\"\"\n",
    "    Load preprocessed datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_dir: str\n",
    "        Fold contains training datasets.\n",
    "    valid_dir: str\n",
    "        Fold contains test datasets.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    value: generator\n",
    "        A generator for tuples like (name, train_data, valid_data)\n",
    "    \"\"\"\n",
    "    \n",
    "    for file in os.listdir(train_dir):\n",
    "        name = file.split(\"_\")[0]\n",
    "        train_dt = tf.data.Dataset.from_tensor_slices(\n",
    "            load_hdf5(f\"{train_dir}/{name}_train.hdf5\")\n",
    "        )\n",
    "        valid_dt = tf.data.Dataset.from_tensor_slices(\n",
    "            load_hdf5(f\"{valid_dir}/{name}_valid.hdf5\")\n",
    "        )\n",
    "        yield (name, train_dt, valid_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "473/473 [==============================] - 26s 49ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "Epoch 2/100\n",
      "473/473 [==============================] - 26s 55ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "Epoch 3/100\n",
      "473/473 [==============================] - 31s 66ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "Epoch 4/100\n",
      "473/473 [==============================] - 25s 54ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "Epoch 5/100\n",
      "473/473 [==============================] - 46s 97ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "INFO:tensorflow:Assets written to: saved_model/AR_30_log_s103_sigmoid_4layers\\assets\n",
      "Dataset AR has been trained and saved.\n",
      "Epoch 1/100\n",
      "366/366 [==============================] - 22s 47ms/step - loss: 2.8779 - MSE: 2.8779 - val_loss: 0.5099 - val_MSE: 0.5099\n",
      "Epoch 2/100\n",
      "366/366 [==============================] - 16s 43ms/step - loss: 0.4194 - MSE: 0.4194 - val_loss: 0.2658 - val_MSE: 0.2658\n",
      "Epoch 3/100\n",
      "366/366 [==============================] - 24s 66ms/step - loss: 0.2275 - MSE: 0.2275 - val_loss: 0.1483 - val_MSE: 0.1483\n",
      "Epoch 4/100\n",
      "366/366 [==============================] - 19s 52ms/step - loss: 0.1351 - MSE: 0.1351 - val_loss: 0.0879 - val_MSE: 0.0879\n",
      "Epoch 5/100\n",
      "366/366 [==============================] - 15s 42ms/step - loss: 0.0851 - MSE: 0.0851 - val_loss: 0.0565 - val_MSE: 0.0565\n",
      "Epoch 6/100\n",
      "366/366 [==============================] - 11s 30ms/step - loss: 0.0566 - MSE: 0.0566 - val_loss: 0.0387 - val_MSE: 0.0387\n",
      "Epoch 7/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 0.0405 - MSE: 0.0405 - val_loss: 0.0275 - val_MSE: 0.0275\n",
      "Epoch 8/100\n",
      "366/366 [==============================] - 14s 39ms/step - loss: 0.0280 - MSE: 0.0280 - val_loss: 0.0193 - val_MSE: 0.0193\n",
      "Epoch 9/100\n",
      "366/366 [==============================] - 16s 44ms/step - loss: 0.0199 - MSE: 0.0199 - val_loss: 0.0135 - val_MSE: 0.0135\n",
      "Epoch 10/100\n",
      "366/366 [==============================] - 18s 50ms/step - loss: 0.0136 - MSE: 0.0136 - val_loss: 0.0089 - val_MSE: 0.0089\n",
      "Epoch 11/100\n",
      "366/366 [==============================] - 16s 43ms/step - loss: 0.0091 - MSE: 0.0091 - val_loss: 0.0059 - val_MSE: 0.0059\n",
      "Epoch 12/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 0.0065 - MSE: 0.0065 - val_loss: 0.0040 - val_MSE: 0.0040\n",
      "Epoch 13/100\n",
      "366/366 [==============================] - 43s 118ms/step - loss: 0.0047 - MSE: 0.0047 - val_loss: 0.0028 - val_MSE: 0.0028\n",
      "Epoch 14/100\n",
      "366/366 [==============================] - 11s 31ms/step - loss: 0.0034 - MSE: 0.0034 - val_loss: 0.0020 - val_MSE: 0.0020\n",
      "Epoch 15/100\n",
      "366/366 [==============================] - 15s 40ms/step - loss: 0.0024 - MSE: 0.0024 - val_loss: 0.0014 - val_MSE: 0.0014\n",
      "Epoch 16/100\n",
      "366/366 [==============================] - 21s 59ms/step - loss: 0.0018 - MSE: 0.0018 - val_loss: 0.0011 - val_MSE: 0.0011\n",
      "Epoch 17/100\n",
      "366/366 [==============================] - 18s 49ms/step - loss: 0.0014 - MSE: 0.0014 - val_loss: 7.7214e-04 - val_MSE: 7.7214e-04\n",
      "Epoch 18/100\n",
      "366/366 [==============================] - 26s 70ms/step - loss: 0.0010 - MSE: 0.0010 - val_loss: 5.8456e-04 - val_MSE: 5.8456e-04\n",
      "Epoch 19/100\n",
      "366/366 [==============================] - 27s 74ms/step - loss: 8.0718e-04 - MSE: 8.0718e-04 - val_loss: 4.4795e-04 - val_MSE: 4.4795e-04\n",
      "Epoch 20/100\n",
      "366/366 [==============================] - 34s 94ms/step - loss: 6.3042e-04 - MSE: 6.3042e-04 - val_loss: 3.3653e-04 - val_MSE: 3.3653e-04\n",
      "Epoch 21/100\n",
      "366/366 [==============================] - 12s 32ms/step - loss: 4.6898e-04 - MSE: 4.6898e-04 - val_loss: 2.5331e-04 - val_MSE: 2.5331e-04\n",
      "Epoch 22/100\n",
      "366/366 [==============================] - 23s 63ms/step - loss: 3.8763e-04 - MSE: 3.8763e-04 - val_loss: 1.9160e-04 - val_MSE: 1.9160e-04\n",
      "Epoch 23/100\n",
      "366/366 [==============================] - 19s 52ms/step - loss: 2.9406e-04 - MSE: 2.9406e-04 - val_loss: 1.4936e-04 - val_MSE: 1.4936e-04\n",
      "Epoch 24/100\n",
      "366/366 [==============================] - 12s 33ms/step - loss: 2.4394e-04 - MSE: 2.4394e-04 - val_loss: 1.1280e-04 - val_MSE: 1.1280e-04\n",
      "Epoch 25/100\n",
      "366/366 [==============================] - 19s 51ms/step - loss: 1.8111e-04 - MSE: 1.8111e-04 - val_loss: 8.9001e-05 - val_MSE: 8.9001e-05\n",
      "Epoch 26/100\n",
      "366/366 [==============================] - 12s 33ms/step - loss: 1.4437e-04 - MSE: 1.4437e-04 - val_loss: 6.8951e-05 - val_MSE: 6.8951e-05\n",
      "Epoch 27/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 1.1745e-04 - MSE: 1.1745e-04 - val_loss: 5.4268e-05 - val_MSE: 5.4268e-05\n",
      "Epoch 28/100\n",
      "366/366 [==============================] - 12s 32ms/step - loss: 9.7852e-05 - MSE: 9.7852e-05 - val_loss: 4.3680e-05 - val_MSE: 4.3680e-05\n",
      "Epoch 29/100\n",
      "366/366 [==============================] - 19s 51ms/step - loss: 8.0567e-05 - MSE: 8.0567e-05 - val_loss: 3.4361e-05 - val_MSE: 3.4361e-05\n",
      "Epoch 30/100\n",
      "366/366 [==============================] - 16s 43ms/step - loss: 6.6790e-05 - MSE: 6.6790e-05 - val_loss: 2.7051e-05 - val_MSE: 2.7051e-05\n",
      "Epoch 31/100\n",
      "366/366 [==============================] - 12s 33ms/step - loss: 5.6626e-05 - MSE: 5.6626e-05 - val_loss: 2.3585e-05 - val_MSE: 2.3585e-05\n",
      "Epoch 32/100\n",
      "366/366 [==============================] - 16s 43ms/step - loss: 4.4257e-05 - MSE: 4.4257e-05 - val_loss: 1.7913e-05 - val_MSE: 1.7913e-05\n",
      "Epoch 33/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 3.6147e-05 - MSE: 3.6147e-05 - val_loss: 1.7685e-05 - val_MSE: 1.7685e-05\n",
      "Epoch 34/100\n",
      "366/366 [==============================] - 15s 42ms/step - loss: 3.0321e-05 - MSE: 3.0321e-05 - val_loss: 1.4001e-05 - val_MSE: 1.4001e-05\n",
      "Epoch 35/100\n",
      "366/366 [==============================] - 15s 40ms/step - loss: 2.4329e-05 - MSE: 2.4329e-05 - val_loss: 1.0409e-05 - val_MSE: 1.0409e-05\n",
      "Epoch 36/100\n",
      "366/366 [==============================] - 16s 43ms/step - loss: 1.9740e-05 - MSE: 1.9740e-05 - val_loss: 8.3435e-06 - val_MSE: 8.3435e-06\n",
      "Epoch 37/100\n",
      "366/366 [==============================] - 15s 42ms/step - loss: 1.8384e-05 - MSE: 1.8384e-05 - val_loss: 6.9819e-06 - val_MSE: 6.9819e-06\n",
      "Epoch 38/100\n",
      "366/366 [==============================] - 15s 42ms/step - loss: 1.3866e-05 - MSE: 1.3866e-05 - val_loss: 6.0645e-06 - val_MSE: 6.0645e-06\n",
      "Epoch 39/100\n",
      "366/366 [==============================] - 14s 39ms/step - loss: 1.2223e-05 - MSE: 1.2223e-05 - val_loss: 5.1170e-06 - val_MSE: 5.1170e-06\n",
      "Epoch 40/100\n",
      "366/366 [==============================] - 11s 31ms/step - loss: 1.0369e-05 - MSE: 1.0369e-05 - val_loss: 4.9640e-06 - val_MSE: 4.9640e-06\n",
      "Epoch 41/100\n",
      "366/366 [==============================] - 18s 49ms/step - loss: 9.5066e-06 - MSE: 9.5066e-06 - val_loss: 5.8013e-06 - val_MSE: 5.8013e-06\n",
      "Epoch 42/100\n",
      "366/366 [==============================] - 11s 30ms/step - loss: 7.2828e-06 - MSE: 7.2828e-06 - val_loss: 3.3387e-06 - val_MSE: 3.3387e-06\n",
      "Epoch 43/100\n",
      "366/366 [==============================] - 18s 49ms/step - loss: 7.1582e-06 - MSE: 7.1582e-06 - val_loss: 4.1538e-06 - val_MSE: 4.1538e-06\n",
      "Epoch 44/100\n",
      "366/366 [==============================] - 21s 58ms/step - loss: 5.6326e-06 - MSE: 5.6326e-06 - val_loss: 2.4765e-06 - val_MSE: 2.4765e-06\n",
      "Epoch 45/100\n",
      "366/366 [==============================] - 12s 32ms/step - loss: 4.7943e-06 - MSE: 4.7943e-06 - val_loss: 2.1997e-06 - val_MSE: 2.1997e-06\n",
      "Epoch 46/100\n",
      "366/366 [==============================] - 14s 39ms/step - loss: 4.2976e-06 - MSE: 4.2976e-06 - val_loss: 1.9509e-06 - val_MSE: 1.9509e-06\n",
      "Epoch 47/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 4.0822e-06 - MSE: 4.0822e-06 - val_loss: 1.7694e-06 - val_MSE: 1.7694e-06\n",
      "Epoch 48/100\n",
      "366/366 [==============================] - 16s 43ms/step - loss: 3.5796e-06 - MSE: 3.5796e-06 - val_loss: 1.8071e-06 - val_MSE: 1.8071e-06\n",
      "Epoch 49/100\n",
      "366/366 [==============================] - 12s 32ms/step - loss: 3.2103e-06 - MSE: 3.2103e-06 - val_loss: 1.7176e-06 - val_MSE: 1.7176e-06\n",
      "Epoch 50/100\n",
      "366/366 [==============================] - 11s 29ms/step - loss: 2.7542e-06 - MSE: 2.7542e-06 - val_loss: 1.4082e-06 - val_MSE: 1.4082e-06\n",
      "Epoch 51/100\n",
      "366/366 [==============================] - 10s 29ms/step - loss: 2.6360e-06 - MSE: 2.6360e-06 - val_loss: 1.3962e-06 - val_MSE: 1.3962e-06\n",
      "Epoch 52/100\n",
      "366/366 [==============================] - 15s 40ms/step - loss: 2.4846e-06 - MSE: 2.4846e-06 - val_loss: 1.2514e-06 - val_MSE: 1.2514e-06\n",
      "Epoch 53/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 2.2456e-06 - MSE: 2.2456e-06 - val_loss: 1.6028e-06 - val_MSE: 1.6028e-06\n",
      "Epoch 54/100\n",
      "366/366 [==============================] - 15s 40ms/step - loss: 2.0920e-06 - MSE: 2.0920e-06 - val_loss: 1.1332e-06 - val_MSE: 1.1332e-06\n",
      "Epoch 55/100\n",
      "366/366 [==============================] - 18s 50ms/step - loss: 2.0469e-06 - MSE: 2.0469e-06 - val_loss: 1.1845e-06 - val_MSE: 1.1845e-06\n",
      "Epoch 56/100\n",
      "366/366 [==============================] - 21s 56ms/step - loss: 2.0144e-06 - MSE: 2.0144e-06 - val_loss: 1.0164e-06 - val_MSE: 1.0164e-06\n",
      "Epoch 57/100\n",
      "366/366 [==============================] - 24s 65ms/step - loss: 1.9245e-06 - MSE: 1.9245e-06 - val_loss: 1.1080e-06 - val_MSE: 1.1080e-06\n",
      "Epoch 58/100\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 1.7841e-06 - MSE: 1.7841e-06 - val_loss: 9.7500e-07 - val_MSE: 9.7500e-07\n",
      "Epoch 59/100\n",
      "366/366 [==============================] - 16s 43ms/step - loss: 1.6759e-06 - MSE: 1.6759e-06 - val_loss: 9.6393e-07 - val_MSE: 9.6393e-07\n",
      "Epoch 60/100\n",
      "366/366 [==============================] - 18s 48ms/step - loss: 1.7476e-06 - MSE: 1.7476e-06 - val_loss: 1.3374e-06 - val_MSE: 1.3374e-06\n",
      "Epoch 61/100\n",
      "366/366 [==============================] - 22s 61ms/step - loss: 1.6818e-06 - MSE: 1.6818e-06 - val_loss: 1.0894e-06 - val_MSE: 1.0894e-06\n",
      "Epoch 62/100\n",
      "366/366 [==============================] - 20s 55ms/step - loss: 1.6022e-06 - MSE: 1.6022e-06 - val_loss: 9.2658e-07 - val_MSE: 9.2658e-07\n",
      "Epoch 63/100\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 1.6688e-06 - MSE: 1.6688e-06 - val_loss: 1.3130e-06 - val_MSE: 1.3130e-06\n",
      "Epoch 64/100\n",
      "366/366 [==============================] - 20s 55ms/step - loss: 1.5900e-06 - MSE: 1.5900e-06 - val_loss: 8.8409e-07 - val_MSE: 8.8409e-07\n",
      "Epoch 65/100\n",
      "366/366 [==============================] - 24s 65ms/step - loss: 1.7660e-06 - MSE: 1.7660e-06 - val_loss: 1.2866e-06 - val_MSE: 1.2866e-06\n",
      "Epoch 66/100\n",
      "366/366 [==============================] - 19s 52ms/step - loss: 1.7007e-06 - MSE: 1.7007e-06 - val_loss: 9.1829e-07 - val_MSE: 9.1829e-07\n",
      "Epoch 67/100\n",
      "366/366 [==============================] - 20s 54ms/step - loss: 1.5112e-06 - MSE: 1.5112e-06 - val_loss: 8.6315e-07 - val_MSE: 8.6315e-07\n",
      "Epoch 68/100\n",
      "366/366 [==============================] - 20s 54ms/step - loss: 1.4986e-06 - MSE: 1.4986e-06 - val_loss: 8.8382e-07 - val_MSE: 8.8382e-07\n",
      "Epoch 69/100\n",
      "366/366 [==============================] - 16s 45ms/step - loss: 1.4964e-06 - MSE: 1.4964e-06 - val_loss: 8.8546e-07 - val_MSE: 8.8546e-07\n",
      "Epoch 70/100\n",
      "366/366 [==============================] - 15s 42ms/step - loss: 1.4571e-06 - MSE: 1.4571e-06 - val_loss: 8.6018e-07 - val_MSE: 8.6018e-07\n",
      "Epoch 71/100\n",
      "366/366 [==============================] - 8s 22ms/step - loss: 1.4148e-06 - MSE: 1.4148e-06 - val_loss: 9.1148e-07 - val_MSE: 9.1148e-07\n",
      "Epoch 72/100\n",
      "366/366 [==============================] - 9s 24ms/step - loss: 1.4435e-06 - MSE: 1.4435e-06 - val_loss: 8.3795e-07 - val_MSE: 8.3795e-07\n",
      "Epoch 73/100\n",
      "366/366 [==============================] - 11s 30ms/step - loss: 1.5155e-06 - MSE: 1.5155e-06 - val_loss: 8.4012e-07 - val_MSE: 8.4012e-07\n",
      "Epoch 74/100\n",
      "366/366 [==============================] - 12s 31ms/step - loss: 1.4359e-06 - MSE: 1.4359e-06 - val_loss: 8.4747e-07 - val_MSE: 8.4747e-07\n",
      "Epoch 75/100\n",
      "366/366 [==============================] - 11s 31ms/step - loss: 1.4323e-06 - MSE: 1.4323e-06 - val_loss: 8.3493e-07 - val_MSE: 8.3493e-07\n",
      "Epoch 76/100\n",
      "366/366 [==============================] - 17s 45ms/step - loss: 1.4311e-06 - MSE: 1.4311e-06 - val_loss: 8.4669e-07 - val_MSE: 8.4669e-07\n",
      "Epoch 77/100\n",
      "366/366 [==============================] - 12s 33ms/step - loss: 1.4156e-06 - MSE: 1.4156e-06 - val_loss: 8.4371e-07 - val_MSE: 8.4371e-07\n",
      "Epoch 78/100\n",
      "366/366 [==============================] - 15s 40ms/step - loss: 1.4806e-06 - MSE: 1.4806e-06 - val_loss: 8.3235e-07 - val_MSE: 8.3235e-07\n",
      "Epoch 79/100\n",
      "366/366 [==============================] - 11s 31ms/step - loss: 1.4734e-06 - MSE: 1.4734e-06 - val_loss: 1.1353e-06 - val_MSE: 1.1353e-06\n",
      "Epoch 80/100\n",
      "366/366 [==============================] - 11s 31ms/step - loss: 1.4407e-06 - MSE: 1.4407e-06 - val_loss: 8.8644e-07 - val_MSE: 8.8644e-07\n",
      "Epoch 81/100\n",
      "366/366 [==============================] - 15s 40ms/step - loss: 1.4080e-06 - MSE: 1.4080e-06 - val_loss: 8.4772e-07 - val_MSE: 8.4772e-07\n",
      "Epoch 82/100\n",
      "366/366 [==============================] - 12s 33ms/step - loss: 1.4559e-06 - MSE: 1.4559e-06 - val_loss: 8.2931e-07 - val_MSE: 8.2931e-07\n",
      "Epoch 83/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 1.3821e-06 - MSE: 1.3821e-06 - val_loss: 8.2285e-07 - val_MSE: 8.2285e-07\n",
      "Epoch 84/100\n",
      "366/366 [==============================] - 12s 32ms/step - loss: 1.3998e-06 - MSE: 1.3998e-06 - val_loss: 8.2574e-07 - val_MSE: 8.2574e-07\n",
      "Epoch 85/100\n",
      "366/366 [==============================] - 12s 32ms/step - loss: 1.3818e-06 - MSE: 1.3818e-06 - val_loss: 8.2392e-07 - val_MSE: 8.2392e-07\n",
      "Epoch 86/100\n",
      "366/366 [==============================] - 12s 32ms/step - loss: 1.4325e-06 - MSE: 1.4325e-06 - val_loss: 8.8548e-07 - val_MSE: 8.8548e-07\n",
      "Epoch 87/100\n",
      "366/366 [==============================] - 12s 32ms/step - loss: 1.4312e-06 - MSE: 1.4312e-06 - val_loss: 8.2664e-07 - val_MSE: 8.2664e-07\n",
      "Epoch 88/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 1.4081e-06 - MSE: 1.4081e-06 - val_loss: 9.2417e-07 - val_MSE: 9.2417e-07\n",
      "INFO:tensorflow:Assets written to: saved_model/BAH_30_log_s103_sigmoid_4layers\\assets\n",
      "Dataset BAH has been trained and saved.\n",
      "Epoch 1/100\n",
      "460/460 [==============================] - 21s 33ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "Epoch 2/100\n",
      "460/460 [==============================] - 19s 41ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "Epoch 3/100\n",
      "460/460 [==============================] - 15s 33ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "Epoch 4/100\n",
      "460/460 [==============================] - 24s 52ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "Epoch 5/100\n",
      "460/460 [==============================] - 34s 73ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "INFO:tensorflow:Assets written to: saved_model/FTI_30_log_s103_sigmoid_4layers\\assets\n",
      "Dataset FTI has been trained and saved.\n",
      "Epoch 1/100\n",
      "229/229 [==============================] - 21s 76ms/step - loss: 1.5379 - MSE: 1.5379 - val_loss: 0.1194 - val_MSE: 0.1194\n",
      "Epoch 2/100\n",
      "229/229 [==============================] - 11s 50ms/step - loss: 0.1714 - MSE: 0.1714 - val_loss: 0.0516 - val_MSE: 0.0516\n",
      "Epoch 3/100\n",
      "229/229 [==============================] - 15s 64ms/step - loss: 0.0784 - MSE: 0.0784 - val_loss: 0.0279 - val_MSE: 0.0279\n",
      "Epoch 4/100\n",
      "229/229 [==============================] - 20s 87ms/step - loss: 0.0441 - MSE: 0.0441 - val_loss: 0.0170 - val_MSE: 0.0170\n",
      "Epoch 5/100\n",
      "229/229 [==============================] - 20s 88ms/step - loss: 0.0290 - MSE: 0.0290 - val_loss: 0.0116 - val_MSE: 0.0116\n",
      "Epoch 6/100\n",
      "229/229 [==============================] - 20s 86ms/step - loss: 0.0209 - MSE: 0.0209 - val_loss: 0.0087 - val_MSE: 0.0087\n",
      "Epoch 7/100\n",
      "229/229 [==============================] - 20s 89ms/step - loss: 0.0161 - MSE: 0.0161 - val_loss: 0.0069 - val_MSE: 0.0069\n",
      "Epoch 8/100\n",
      "229/229 [==============================] - 20s 90ms/step - loss: 0.0125 - MSE: 0.0125 - val_loss: 0.0058 - val_MSE: 0.0058\n",
      "Epoch 9/100\n",
      "229/229 [==============================] - 19s 85ms/step - loss: 0.0107 - MSE: 0.0107 - val_loss: 0.0049 - val_MSE: 0.0049\n",
      "Epoch 10/100\n",
      "229/229 [==============================] - 20s 88ms/step - loss: 0.0090 - MSE: 0.0090 - val_loss: 0.0042 - val_MSE: 0.0042\n",
      "Epoch 11/100\n",
      "229/229 [==============================] - 20s 88ms/step - loss: 0.0077 - MSE: 0.0077 - val_loss: 0.0036 - val_MSE: 0.0036\n",
      "Epoch 12/100\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 0.0065 - MSE: 0.0065 - val_loss: 0.0031 - val_MSE: 0.0031\n",
      "Epoch 13/100\n",
      "229/229 [==============================] - 14s 62ms/step - loss: 0.0054 - MSE: 0.0054 - val_loss: 0.0026 - val_MSE: 0.0026\n",
      "Epoch 14/100\n",
      "229/229 [==============================] - 11s 48ms/step - loss: 0.0046 - MSE: 0.0046 - val_loss: 0.0022 - val_MSE: 0.0022\n",
      "Epoch 15/100\n",
      "229/229 [==============================] - 20s 89ms/step - loss: 0.0040 - MSE: 0.0040 - val_loss: 0.0018 - val_MSE: 0.0018\n",
      "Epoch 16/100\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 0.0033 - MSE: 0.0033 - val_loss: 0.0015 - val_MSE: 0.0015\n",
      "Epoch 17/100\n",
      "229/229 [==============================] - 18s 79ms/step - loss: 0.0029 - MSE: 0.0029 - val_loss: 0.0013 - val_MSE: 0.0013\n",
      "Epoch 18/100\n",
      "229/229 [==============================] - 23s 99ms/step - loss: 0.0024 - MSE: 0.0024 - val_loss: 0.0011 - val_MSE: 0.0011\n",
      "Epoch 19/100\n",
      "229/229 [==============================] - 14s 60ms/step - loss: 0.0021 - MSE: 0.0021 - val_loss: 9.0215e-04 - val_MSE: 9.0215e-04\n",
      "Epoch 20/100\n",
      "229/229 [==============================] - 12s 52ms/step - loss: 0.0018 - MSE: 0.0018 - val_loss: 7.7795e-04 - val_MSE: 7.7795e-04\n",
      "Epoch 21/100\n",
      "229/229 [==============================] - 7s 32ms/step - loss: 0.0015 - MSE: 0.0015 - val_loss: 6.6554e-04 - val_MSE: 6.6554e-04\n",
      "Epoch 22/100\n",
      "229/229 [==============================] - 16s 68ms/step - loss: 0.0013 - MSE: 0.0013 - val_loss: 5.7500e-04 - val_MSE: 5.7500e-04\n",
      "Epoch 23/100\n",
      "229/229 [==============================] - 23s 103ms/step - loss: 0.0011 - MSE: 0.0011 - val_loss: 4.9533e-04 - val_MSE: 4.9533e-04\n",
      "Epoch 24/100\n",
      "229/229 [==============================] - 17s 76ms/step - loss: 9.7288e-04 - MSE: 9.7288e-04 - val_loss: 4.2226e-04 - val_MSE: 4.2226e-04\n",
      "Epoch 25/100\n",
      "229/229 [==============================] - 20s 89ms/step - loss: 8.5357e-04 - MSE: 8.5357e-04 - val_loss: 3.6726e-04 - val_MSE: 3.6726e-04\n",
      "Epoch 26/100\n",
      "229/229 [==============================] - 22s 97ms/step - loss: 7.5274e-04 - MSE: 7.5274e-04 - val_loss: 3.2364e-04 - val_MSE: 3.2364e-04\n",
      "Epoch 27/100\n",
      "229/229 [==============================] - 17s 76ms/step - loss: 6.4012e-04 - MSE: 6.4012e-04 - val_loss: 2.8376e-04 - val_MSE: 2.8376e-04\n",
      "Epoch 28/100\n",
      "229/229 [==============================] - 20s 89ms/step - loss: 5.6559e-04 - MSE: 5.6559e-04 - val_loss: 2.3578e-04 - val_MSE: 2.3578e-04\n",
      "Epoch 29/100\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 4.8132e-04 - MSE: 4.8132e-04 - val_loss: 2.0452e-04 - val_MSE: 2.0452e-04\n",
      "Epoch 30/100\n",
      "229/229 [==============================] - 19s 83ms/step - loss: 4.2373e-04 - MSE: 4.2373e-04 - val_loss: 1.7878e-04 - val_MSE: 1.7878e-04\n",
      "Epoch 31/100\n",
      "229/229 [==============================] - 20s 88ms/step - loss: 3.5131e-04 - MSE: 3.5131e-04 - val_loss: 1.5213e-04 - val_MSE: 1.5213e-04\n",
      "Epoch 32/100\n",
      "229/229 [==============================] - 16s 69ms/step - loss: 3.3603e-04 - MSE: 3.3603e-04 - val_loss: 1.2535e-04 - val_MSE: 1.2535e-04\n",
      "Epoch 33/100\n",
      "229/229 [==============================] - 12s 51ms/step - loss: 2.7576e-04 - MSE: 2.7576e-04 - val_loss: 1.1089e-04 - val_MSE: 1.1089e-04\n",
      "Epoch 34/100\n",
      "229/229 [==============================] - 8s 37ms/step - loss: 2.4955e-04 - MSE: 2.4955e-04 - val_loss: 9.4649e-05 - val_MSE: 9.4649e-05\n",
      "Epoch 35/100\n",
      "229/229 [==============================] - 12s 51ms/step - loss: 2.2597e-04 - MSE: 2.2597e-04 - val_loss: 8.9569e-05 - val_MSE: 8.9569e-05\n",
      "Epoch 36/100\n",
      "229/229 [==============================] - 11s 46ms/step - loss: 1.8665e-04 - MSE: 1.8665e-04 - val_loss: 6.9343e-05 - val_MSE: 6.9343e-05\n",
      "Epoch 37/100\n",
      "229/229 [==============================] - 10s 42ms/step - loss: 1.6327e-04 - MSE: 1.6327e-04 - val_loss: 6.1764e-05 - val_MSE: 6.1764e-05\n",
      "Epoch 38/100\n",
      "229/229 [==============================] - 12s 52ms/step - loss: 1.4305e-04 - MSE: 1.4305e-04 - val_loss: 5.3249e-05 - val_MSE: 5.3249e-05\n",
      "Epoch 39/100\n",
      "229/229 [==============================] - 11s 47ms/step - loss: 1.2535e-04 - MSE: 1.2535e-04 - val_loss: 4.5134e-05 - val_MSE: 4.5134e-05\n",
      "Epoch 40/100\n",
      "229/229 [==============================] - 12s 51ms/step - loss: 1.1354e-04 - MSE: 1.1354e-04 - val_loss: 4.1366e-05 - val_MSE: 4.1366e-05\n",
      "Epoch 41/100\n",
      "229/229 [==============================] - 9s 41ms/step - loss: 9.7763e-05 - MSE: 9.7763e-05 - val_loss: 3.4612e-05 - val_MSE: 3.4612e-05\n",
      "Epoch 42/100\n",
      "229/229 [==============================] - 12s 51ms/step - loss: 8.5091e-05 - MSE: 8.5091e-05 - val_loss: 3.0382e-05 - val_MSE: 3.0382e-05\n",
      "Epoch 43/100\n",
      "229/229 [==============================] - 15s 65ms/step - loss: 7.6369e-05 - MSE: 7.6369e-05 - val_loss: 2.5295e-05 - val_MSE: 2.5295e-05\n",
      "Epoch 44/100\n",
      "229/229 [==============================] - 19s 85ms/step - loss: 6.4765e-05 - MSE: 6.4765e-05 - val_loss: 2.3118e-05 - val_MSE: 2.3118e-05\n",
      "Epoch 45/100\n",
      "229/229 [==============================] - 20s 89ms/step - loss: 5.4868e-05 - MSE: 5.4868e-05 - val_loss: 1.8538e-05 - val_MSE: 1.8538e-05\n",
      "Epoch 46/100\n",
      "229/229 [==============================] - 13s 58ms/step - loss: 5.0074e-05 - MSE: 5.0074e-05 - val_loss: 1.6444e-05 - val_MSE: 1.6444e-05\n",
      "Epoch 47/100\n",
      "229/229 [==============================] - 12s 52ms/step - loss: 4.4649e-05 - MSE: 4.4649e-05 - val_loss: 1.3921e-05 - val_MSE: 1.3921e-05\n",
      "Epoch 48/100\n",
      "229/229 [==============================] - 12s 52ms/step - loss: 3.6675e-05 - MSE: 3.6675e-05 - val_loss: 1.2415e-05 - val_MSE: 1.2415e-05\n",
      "Epoch 49/100\n",
      "229/229 [==============================] - 11s 46ms/step - loss: 3.0986e-05 - MSE: 3.0986e-05 - val_loss: 1.0981e-05 - val_MSE: 1.0981e-05\n",
      "Epoch 50/100\n",
      "229/229 [==============================] - 10s 42ms/step - loss: 3.0048e-05 - MSE: 3.0048e-05 - val_loss: 9.1377e-06 - val_MSE: 9.1377e-06\n",
      "Epoch 51/100\n",
      "229/229 [==============================] - 12s 42ms/step - loss: 2.5388e-05 - MSE: 2.5388e-05 - val_loss: 8.1567e-06 - val_MSE: 8.1567e-06\n",
      "Epoch 52/100\n",
      "229/229 [==============================] - 11s 48ms/step - loss: 2.2405e-05 - MSE: 2.2405e-05 - val_loss: 7.4618e-06 - val_MSE: 7.4618e-06\n",
      "Epoch 53/100\n",
      "229/229 [==============================] - 9s 41ms/step - loss: 2.0396e-05 - MSE: 2.0396e-05 - val_loss: 6.2845e-06 - val_MSE: 6.2845e-06\n",
      "Epoch 54/100\n",
      "229/229 [==============================] - 12s 52ms/step - loss: 1.8414e-05 - MSE: 1.8414e-05 - val_loss: 5.7787e-06 - val_MSE: 5.7787e-06\n",
      "Epoch 55/100\n",
      "229/229 [==============================] - 12s 53ms/step - loss: 1.5607e-05 - MSE: 1.5607e-05 - val_loss: 5.3114e-06 - val_MSE: 5.3114e-06\n",
      "Epoch 56/100\n",
      "229/229 [==============================] - 11s 48ms/step - loss: 1.4024e-05 - MSE: 1.4024e-05 - val_loss: 4.6516e-06 - val_MSE: 4.6516e-06\n",
      "Epoch 57/100\n",
      "229/229 [==============================] - 9s 41ms/step - loss: 1.2328e-05 - MSE: 1.2328e-05 - val_loss: 5.4003e-06 - val_MSE: 5.4003e-06\n",
      "Epoch 58/100\n",
      "229/229 [==============================] - 12s 51ms/step - loss: 1.0284e-05 - MSE: 1.0284e-05 - val_loss: 3.8927e-06 - val_MSE: 3.8927e-06\n",
      "Epoch 59/100\n",
      "229/229 [==============================] - 12s 52ms/step - loss: 9.9240e-06 - MSE: 9.9240e-06 - val_loss: 3.7587e-06 - val_MSE: 3.7587e-06\n",
      "Epoch 60/100\n",
      "229/229 [==============================] - 12s 53ms/step - loss: 8.4013e-06 - MSE: 8.4013e-06 - val_loss: 3.4629e-06 - val_MSE: 3.4629e-06\n",
      "Epoch 61/100\n",
      "229/229 [==============================] - 6s 26ms/step - loss: 7.6838e-06 - MSE: 7.6838e-06 - val_loss: 3.1686e-06 - val_MSE: 3.1686e-06\n",
      "Epoch 62/100\n",
      "229/229 [==============================] - 6s 26ms/step - loss: 7.3976e-06 - MSE: 7.3976e-06 - val_loss: 3.1654e-06 - val_MSE: 3.1654e-06\n",
      "Epoch 63/100\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 6.6069e-06 - MSE: 6.6069e-06 - val_loss: 2.8290e-06 - val_MSE: 2.8290e-06\n",
      "Epoch 64/100\n",
      "229/229 [==============================] - 3s 13ms/step - loss: 6.6062e-06 - MSE: 6.6062e-06 - val_loss: 2.6931e-06 - val_MSE: 2.6931e-06\n",
      "Epoch 65/100\n",
      "229/229 [==============================] - 8s 33ms/step - loss: 5.9229e-06 - MSE: 5.9229e-06 - val_loss: 2.5945e-06 - val_MSE: 2.5945e-06\n",
      "Epoch 66/100\n",
      "229/229 [==============================] - 8s 33ms/step - loss: 5.2108e-06 - MSE: 5.2108e-06 - val_loss: 2.8010e-06 - val_MSE: 2.8010e-06\n",
      "Epoch 67/100\n",
      "229/229 [==============================] - 7s 33ms/step - loss: 4.9907e-06 - MSE: 4.9907e-06 - val_loss: 3.6102e-06 - val_MSE: 3.6102e-06\n",
      "Epoch 68/100\n",
      "229/229 [==============================] - 8s 34ms/step - loss: 5.1309e-06 - MSE: 5.1309e-06 - val_loss: 2.6081e-06 - val_MSE: 2.6081e-06\n",
      "Epoch 69/100\n",
      "229/229 [==============================] - 8s 33ms/step - loss: 4.5445e-06 - MSE: 4.5445e-06 - val_loss: 2.2660e-06 - val_MSE: 2.2660e-06\n",
      "Epoch 70/100\n",
      "229/229 [==============================] - 8s 33ms/step - loss: 4.0186e-06 - MSE: 4.0186e-06 - val_loss: 2.2150e-06 - val_MSE: 2.2150e-06\n",
      "Epoch 71/100\n",
      "229/229 [==============================] - 8s 33ms/step - loss: 3.8285e-06 - MSE: 3.8285e-06 - val_loss: 2.5065e-06 - val_MSE: 2.5065e-06\n",
      "Epoch 72/100\n",
      "229/229 [==============================] - 8s 34ms/step - loss: 3.7075e-06 - MSE: 3.7075e-06 - val_loss: 2.1495e-06 - val_MSE: 2.1495e-06\n",
      "Epoch 73/100\n",
      "229/229 [==============================] - 8s 34ms/step - loss: 3.2896e-06 - MSE: 3.2896e-06 - val_loss: 2.2626e-06 - val_MSE: 2.2626e-06\n",
      "Epoch 74/100\n",
      "229/229 [==============================] - 8s 33ms/step - loss: 3.3323e-06 - MSE: 3.3323e-06 - val_loss: 2.1332e-06 - val_MSE: 2.1332e-06\n",
      "Epoch 75/100\n",
      "229/229 [==============================] - 6s 25ms/step - loss: 3.0097e-06 - MSE: 3.0097e-06 - val_loss: 2.3005e-06 - val_MSE: 2.3005e-06\n",
      "Epoch 76/100\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 2.9632e-06 - MSE: 2.9632e-06 - val_loss: 2.0309e-06 - val_MSE: 2.0309e-06\n",
      "Epoch 77/100\n",
      "229/229 [==============================] - 4s 19ms/step - loss: 2.9504e-06 - MSE: 2.9504e-06 - val_loss: 2.2540e-06 - val_MSE: 2.2540e-06\n",
      "Epoch 78/100\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 2.7842e-06 - MSE: 2.7842e-06 - val_loss: 2.2212e-06 - val_MSE: 2.2212e-06\n",
      "Epoch 79/100\n",
      "229/229 [==============================] - 7s 32ms/step - loss: 2.7321e-06 - MSE: 2.7321e-06 - val_loss: 2.0019e-06 - val_MSE: 2.0019e-06\n",
      "Epoch 80/100\n",
      "229/229 [==============================] - 7s 33ms/step - loss: 2.6533e-06 - MSE: 2.6533e-06 - val_loss: 1.9954e-06 - val_MSE: 1.9954e-06\n",
      "Epoch 81/100\n",
      "229/229 [==============================] - 5s 21ms/step - loss: 2.7198e-06 - MSE: 2.7198e-06 - val_loss: 2.1200e-06 - val_MSE: 2.1200e-06\n",
      "Epoch 82/100\n",
      "229/229 [==============================] - 7s 33ms/step - loss: 2.6322e-06 - MSE: 2.6322e-06 - val_loss: 1.9833e-06 - val_MSE: 1.9833e-06\n",
      "Epoch 83/100\n",
      "229/229 [==============================] - 8s 34ms/step - loss: 2.5057e-06 - MSE: 2.5057e-06 - val_loss: 2.1941e-06 - val_MSE: 2.1941e-06\n",
      "Epoch 84/100\n",
      "229/229 [==============================] - 8s 34ms/step - loss: 2.5274e-06 - MSE: 2.5274e-06 - val_loss: 1.9494e-06 - val_MSE: 1.9494e-06\n",
      "Epoch 85/100\n",
      "229/229 [==============================] - 8s 34ms/step - loss: 2.4610e-06 - MSE: 2.4610e-06 - val_loss: 1.9446e-06 - val_MSE: 1.9446e-06\n",
      "Epoch 86/100\n",
      "229/229 [==============================] - 8s 33ms/step - loss: 2.4765e-06 - MSE: 2.4765e-06 - val_loss: 2.3065e-06 - val_MSE: 2.3065e-06\n",
      "Epoch 87/100\n",
      "229/229 [==============================] - 8s 34ms/step - loss: 2.4087e-06 - MSE: 2.4087e-06 - val_loss: 2.2443e-06 - val_MSE: 2.2443e-06\n",
      "Epoch 88/100\n",
      "229/229 [==============================] - 8s 33ms/step - loss: 2.3507e-06 - MSE: 2.3507e-06 - val_loss: 2.2272e-06 - val_MSE: 2.2272e-06\n",
      "Epoch 89/100\n",
      "229/229 [==============================] - 7s 33ms/step - loss: 2.3414e-06 - MSE: 2.3414e-06 - val_loss: 2.0268e-06 - val_MSE: 2.0268e-06\n",
      "Epoch 90/100\n",
      "229/229 [==============================] - 7s 33ms/step - loss: 2.2899e-06 - MSE: 2.2899e-06 - val_loss: 1.9261e-06 - val_MSE: 1.9261e-06\n",
      "Epoch 91/100\n",
      "229/229 [==============================] - 7s 33ms/step - loss: 2.2895e-06 - MSE: 2.2895e-06 - val_loss: 1.9652e-06 - val_MSE: 1.9652e-06\n",
      "Epoch 92/100\n",
      "229/229 [==============================] - 7s 31ms/step - loss: 2.2834e-06 - MSE: 2.2834e-06 - val_loss: 1.9479e-06 - val_MSE: 1.9479e-06\n",
      "Epoch 93/100\n",
      "229/229 [==============================] - 7s 31ms/step - loss: 2.2748e-06 - MSE: 2.2748e-06 - val_loss: 1.9444e-06 - val_MSE: 1.9444e-06\n",
      "Epoch 94/100\n",
      "229/229 [==============================] - 7s 33ms/step - loss: 2.2658e-06 - MSE: 2.2658e-06 - val_loss: 2.2525e-06 - val_MSE: 2.2525e-06\n",
      "Epoch 95/100\n",
      "229/229 [==============================] - 7s 33ms/step - loss: 2.2692e-06 - MSE: 2.2692e-06 - val_loss: 2.1452e-06 - val_MSE: 2.1452e-06\n",
      "INFO:tensorflow:Assets written to: saved_model/HII_30_log_s103_sigmoid_4layers\\assets\n",
      "Dataset HII has been trained and saved.\n",
      "Epoch 1/100\n",
      "413/413 [==============================] - 15s 30ms/step - loss: 2.1593 - MSE: 2.1593 - val_loss: 0.2688 - val_MSE: 0.2688\n",
      "Epoch 2/100\n",
      "413/413 [==============================] - 13s 31ms/step - loss: 0.1522 - MSE: 0.1522 - val_loss: 0.0621 - val_MSE: 0.0621\n",
      "Epoch 3/100\n",
      "413/413 [==============================] - 14s 35ms/step - loss: 0.0607 - MSE: 0.0607 - val_loss: 0.0315 - val_MSE: 0.0315\n",
      "Epoch 4/100\n",
      "413/413 [==============================] - 49s 119ms/step - loss: 0.0344 - MSE: 0.0344 - val_loss: 0.0173 - val_MSE: 0.0173\n",
      "Epoch 5/100\n",
      "413/413 [==============================] - 15s 37ms/step - loss: 0.0207 - MSE: 0.0207 - val_loss: 0.0101 - val_MSE: 0.0101\n",
      "Epoch 6/100\n",
      "413/413 [==============================] - 11s 26ms/step - loss: 0.0125 - MSE: 0.0125 - val_loss: 0.0066 - val_MSE: 0.0066\n",
      "Epoch 7/100\n",
      "413/413 [==============================] - 18s 45ms/step - loss: 0.0083 - MSE: 0.0083 - val_loss: 0.0046 - val_MSE: 0.0046\n",
      "Epoch 8/100\n",
      "413/413 [==============================] - 18s 43ms/step - loss: 0.0058 - MSE: 0.0058 - val_loss: 0.0032 - val_MSE: 0.0032\n",
      "Epoch 9/100\n",
      "413/413 [==============================] - 23s 55ms/step - loss: 0.0041 - MSE: 0.0041 - val_loss: 0.0024 - val_MSE: 0.0024\n",
      "Epoch 10/100\n",
      "413/413 [==============================] - 21s 51ms/step - loss: 0.0030 - MSE: 0.0030 - val_loss: 0.0017 - val_MSE: 0.0017\n",
      "Epoch 11/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 0.0022 - MSE: 0.0022 - val_loss: 0.0012 - val_MSE: 0.0012\n",
      "Epoch 12/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 0.0017 - MSE: 0.0017 - val_loss: 9.0231e-04 - val_MSE: 9.0231e-04\n",
      "Epoch 13/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 0.0013 - MSE: 0.0013 - val_loss: 6.5928e-04 - val_MSE: 6.5928e-04\n",
      "Epoch 14/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 9.5384e-04 - MSE: 9.5384e-04 - val_loss: 4.7232e-04 - val_MSE: 4.7232e-04\n",
      "Epoch 15/100\n",
      "413/413 [==============================] - 35s 84ms/step - loss: 7.2593e-04 - MSE: 7.2593e-04 - val_loss: 3.5112e-04 - val_MSE: 3.5112e-04\n",
      "Epoch 16/100\n",
      "413/413 [==============================] - 38s 91ms/step - loss: 5.3637e-04 - MSE: 5.3637e-04 - val_loss: 2.5696e-04 - val_MSE: 2.5696e-04\n",
      "Epoch 17/100\n",
      "413/413 [==============================] - 38s 88ms/step - loss: 4.4921e-04 - MSE: 4.4921e-04 - val_loss: 1.9587e-04 - val_MSE: 1.9587e-04\n",
      "Epoch 18/100\n",
      "413/413 [==============================] - 37s 91ms/step - loss: 3.2838e-04 - MSE: 3.2838e-04 - val_loss: 1.4590e-04 - val_MSE: 1.4590e-04\n",
      "Epoch 19/100\n",
      "413/413 [==============================] - 38s 93ms/step - loss: 2.6763e-04 - MSE: 2.6763e-04 - val_loss: 1.0759e-04 - val_MSE: 1.0759e-04\n",
      "Epoch 20/100\n",
      "413/413 [==============================] - 40s 97ms/step - loss: 2.0748e-04 - MSE: 2.0748e-04 - val_loss: 7.6534e-05 - val_MSE: 7.6534e-05\n",
      "Epoch 21/100\n",
      "413/413 [==============================] - 40s 97ms/step - loss: 1.5711e-04 - MSE: 1.5711e-04 - val_loss: 6.0623e-05 - val_MSE: 6.0623e-05\n",
      "Epoch 22/100\n",
      "413/413 [==============================] - 23s 55ms/step - loss: 1.2741e-04 - MSE: 1.2741e-04 - val_loss: 4.5754e-05 - val_MSE: 4.5754e-05\n",
      "Epoch 23/100\n",
      "413/413 [==============================] - 34s 82ms/step - loss: 9.4849e-05 - MSE: 9.4849e-05 - val_loss: 3.3930e-05 - val_MSE: 3.3930e-05\n",
      "Epoch 24/100\n",
      "413/413 [==============================] - 39s 93ms/step - loss: 7.5376e-05 - MSE: 7.5376e-05 - val_loss: 2.4976e-05 - val_MSE: 2.4976e-05\n",
      "Epoch 25/100\n",
      "413/413 [==============================] - 29s 70ms/step - loss: 5.7990e-05 - MSE: 5.7990e-05 - val_loss: 1.9113e-05 - val_MSE: 1.9113e-05\n",
      "Epoch 26/100\n",
      "413/413 [==============================] - 22s 54ms/step - loss: 4.6985e-05 - MSE: 4.6985e-05 - val_loss: 1.5082e-05 - val_MSE: 1.5082e-05\n",
      "Epoch 27/100\n",
      "413/413 [==============================] - 12s 29ms/step - loss: 3.6000e-05 - MSE: 3.6000e-05 - val_loss: 1.1077e-05 - val_MSE: 1.1077e-05\n",
      "Epoch 28/100\n",
      "413/413 [==============================] - 11s 27ms/step - loss: 3.0048e-05 - MSE: 3.0048e-05 - val_loss: 8.8815e-06 - val_MSE: 8.8815e-06\n",
      "Epoch 29/100\n",
      "413/413 [==============================] - 15s 36ms/step - loss: 2.2283e-05 - MSE: 2.2283e-05 - val_loss: 6.4116e-06 - val_MSE: 6.4116e-06\n",
      "Epoch 30/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 1.6996e-05 - MSE: 1.6996e-05 - val_loss: 5.0810e-06 - val_MSE: 5.0810e-06\n",
      "Epoch 31/100\n",
      "413/413 [==============================] - 20s 49ms/step - loss: 1.5409e-05 - MSE: 1.5409e-05 - val_loss: 4.0142e-06 - val_MSE: 4.0142e-06\n",
      "Epoch 32/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 1.1412e-05 - MSE: 1.1412e-05 - val_loss: 3.2712e-06 - val_MSE: 3.2712e-06\n",
      "Epoch 33/100\n",
      "413/413 [==============================] - 20s 49ms/step - loss: 9.2948e-06 - MSE: 9.2948e-06 - val_loss: 2.6829e-06 - val_MSE: 2.6829e-06\n",
      "Epoch 34/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 8.0456e-06 - MSE: 8.0456e-06 - val_loss: 2.4520e-06 - val_MSE: 2.4520e-06\n",
      "Epoch 35/100\n",
      "413/413 [==============================] - 21s 52ms/step - loss: 6.8003e-06 - MSE: 6.8003e-06 - val_loss: 2.1103e-06 - val_MSE: 2.1103e-06\n",
      "Epoch 36/100\n",
      "413/413 [==============================] - 19s 46ms/step - loss: 5.4820e-06 - MSE: 5.4820e-06 - val_loss: 1.6106e-06 - val_MSE: 1.6106e-06\n",
      "Epoch 37/100\n",
      "413/413 [==============================] - 21s 52ms/step - loss: 4.6708e-06 - MSE: 4.6708e-06 - val_loss: 1.4152e-06 - val_MSE: 1.4152e-06\n",
      "Epoch 38/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 4.0056e-06 - MSE: 4.0056e-06 - val_loss: 1.2522e-06 - val_MSE: 1.2522e-06\n",
      "Epoch 39/100\n",
      "413/413 [==============================] - 20s 49ms/step - loss: 3.8827e-06 - MSE: 3.8827e-06 - val_loss: 1.1529e-06 - val_MSE: 1.1529e-06\n",
      "Epoch 40/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 2.9104e-06 - MSE: 2.9104e-06 - val_loss: 1.0469e-06 - val_MSE: 1.0469e-06\n",
      "Epoch 41/100\n",
      "413/413 [==============================] - 20s 49ms/step - loss: 2.8412e-06 - MSE: 2.8412e-06 - val_loss: 9.8281e-07 - val_MSE: 9.8281e-07\n",
      "Epoch 42/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 2.3829e-06 - MSE: 2.3829e-06 - val_loss: 9.0940e-07 - val_MSE: 9.0940e-07\n",
      "Epoch 43/100\n",
      "413/413 [==============================] - 20s 49ms/step - loss: 2.0923e-06 - MSE: 2.0923e-06 - val_loss: 8.8813e-07 - val_MSE: 8.8813e-07\n",
      "Epoch 44/100\n",
      "413/413 [==============================] - 20s 49ms/step - loss: 2.0244e-06 - MSE: 2.0244e-06 - val_loss: 1.0107e-06 - val_MSE: 1.0107e-06\n",
      "Epoch 45/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 1.8706e-06 - MSE: 1.8706e-06 - val_loss: 8.1243e-07 - val_MSE: 8.1243e-07\n",
      "Epoch 46/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 1.6806e-06 - MSE: 1.6806e-06 - val_loss: 8.8437e-07 - val_MSE: 8.8437e-07\n",
      "Epoch 47/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 1.7283e-06 - MSE: 1.7283e-06 - val_loss: 8.1223e-07 - val_MSE: 8.1223e-07\n",
      "Epoch 48/100\n",
      "413/413 [==============================] - 25s 60ms/step - loss: 1.5521e-06 - MSE: 1.5521e-06 - val_loss: 7.6116e-07 - val_MSE: 7.6116e-07\n",
      "Epoch 49/100\n",
      "413/413 [==============================] - 14s 33ms/step - loss: 1.4416e-06 - MSE: 1.4416e-06 - val_loss: 7.7954e-07 - val_MSE: 7.7954e-07\n",
      "Epoch 50/100\n",
      "413/413 [==============================] - 8s 21ms/step - loss: 1.4070e-06 - MSE: 1.4070e-06 - val_loss: 7.4101e-07 - val_MSE: 7.4101e-07\n",
      "Epoch 51/100\n",
      "413/413 [==============================] - 10s 24ms/step - loss: 1.3764e-06 - MSE: 1.3764e-06 - val_loss: 9.1653e-07 - val_MSE: 9.1653e-07\n",
      "Epoch 52/100\n",
      "413/413 [==============================] - 18s 45ms/step - loss: 1.3624e-06 - MSE: 1.3624e-06 - val_loss: 7.2000e-07 - val_MSE: 7.2000e-07\n",
      "Epoch 53/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 1.3260e-06 - MSE: 1.3260e-06 - val_loss: 7.1723e-07 - val_MSE: 7.1723e-07\n",
      "Epoch 54/100\n",
      "413/413 [==============================] - 22s 53ms/step - loss: 1.2709e-06 - MSE: 1.2709e-06 - val_loss: 7.1201e-07 - val_MSE: 7.1201e-07\n",
      "Epoch 55/100\n",
      "413/413 [==============================] - 23s 55ms/step - loss: 1.2878e-06 - MSE: 1.2878e-06 - val_loss: 7.0660e-07 - val_MSE: 7.0660e-07\n",
      "Epoch 56/100\n",
      "413/413 [==============================] - 21s 51ms/step - loss: 1.2564e-06 - MSE: 1.2564e-06 - val_loss: 7.0752e-07 - val_MSE: 7.0752e-07\n",
      "Epoch 57/100\n",
      "413/413 [==============================] - 20s 48ms/step - loss: 1.2372e-06 - MSE: 1.2372e-06 - val_loss: 7.0111e-07 - val_MSE: 7.0111e-07\n",
      "Epoch 58/100\n",
      "413/413 [==============================] - 23s 55ms/step - loss: 1.2327e-06 - MSE: 1.2327e-06 - val_loss: 7.0261e-07 - val_MSE: 7.0261e-07\n",
      "Epoch 59/100\n",
      "413/413 [==============================] - 21s 51ms/step - loss: 1.2491e-06 - MSE: 1.2491e-06 - val_loss: 7.7525e-07 - val_MSE: 7.7525e-07\n",
      "Epoch 60/100\n",
      "413/413 [==============================] - 23s 55ms/step - loss: 1.2244e-06 - MSE: 1.2244e-06 - val_loss: 7.0880e-07 - val_MSE: 7.0880e-07\n",
      "Epoch 61/100\n",
      "413/413 [==============================] - 20s 49ms/step - loss: 1.2200e-06 - MSE: 1.2200e-06 - val_loss: 7.0529e-07 - val_MSE: 7.0529e-07\n",
      "Epoch 62/100\n",
      "413/413 [==============================] - 20s 49ms/step - loss: 1.2086e-06 - MSE: 1.2086e-06 - val_loss: 7.9414e-07 - val_MSE: 7.9414e-07\n",
      "INFO:tensorflow:Assets written to: saved_model/LMT_30_log_s103_sigmoid_4layers\\assets\n",
      "Dataset LMT has been trained and saved.\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 13s 49ms/step - loss: 0.5199 - MSE: 0.5199 - val_loss: 0.1950 - val_MSE: 0.1950\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.1500 - MSE: 0.1500 - val_loss: 0.0626 - val_MSE: 0.0626\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.0560 - MSE: 0.0560 - val_loss: 0.0241 - val_MSE: 0.0241\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 11s 43ms/step - loss: 0.0264 - MSE: 0.0264 - val_loss: 0.0124 - val_MSE: 0.0124\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 9s 46ms/step - loss: 0.0161 - MSE: 0.0161 - val_loss: 0.0075 - val_MSE: 0.0075\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 12s 59ms/step - loss: 0.0106 - MSE: 0.0106 - val_loss: 0.0051 - val_MSE: 0.0051\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.0075 - MSE: 0.0075 - val_loss: 0.0037 - val_MSE: 0.0037\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.0056 - MSE: 0.0056 - val_loss: 0.0027 - val_MSE: 0.0027\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.0043 - MSE: 0.0043 - val_loss: 0.0022 - val_MSE: 0.0022\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 0.0034 - MSE: 0.0034 - val_loss: 0.0017 - val_MSE: 0.0017\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.0026 - MSE: 0.0026 - val_loss: 0.0014 - val_MSE: 0.0014\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 0.0021 - MSE: 0.0021 - val_loss: 0.0012 - val_MSE: 0.0012\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 0.0016 - MSE: 0.0016 - val_loss: 9.4987e-04 - val_MSE: 9.4987e-04\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 0.0014 - MSE: 0.0014 - val_loss: 8.0003e-04 - val_MSE: 8.0003e-04\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 0.0013 - MSE: 0.0013 - val_loss: 6.8663e-04 - val_MSE: 6.8663e-04\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 9.9322e-04 - MSE: 9.9322e-04 - val_loss: 5.8343e-04 - val_MSE: 5.8343e-04\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 8.6579e-04 - MSE: 8.6579e-04 - val_loss: 4.9808e-04 - val_MSE: 4.9808e-04\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 7.1917e-04 - MSE: 7.1917e-04 - val_loss: 4.2125e-04 - val_MSE: 4.2125e-04\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 5.9777e-04 - MSE: 5.9777e-04 - val_loss: 3.5475e-04 - val_MSE: 3.5475e-04\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 5.2167e-04 - MSE: 5.2167e-04 - val_loss: 3.0953e-04 - val_MSE: 3.0953e-04\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 4.5827e-04 - MSE: 4.5827e-04 - val_loss: 2.6639e-04 - val_MSE: 2.6639e-04\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 4.1830e-04 - MSE: 4.1830e-04 - val_loss: 2.2677e-04 - val_MSE: 2.2677e-04\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 3.5225e-04 - MSE: 3.5225e-04 - val_loss: 1.9262e-04 - val_MSE: 1.9262e-04\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 8s 42ms/step - loss: 2.9386e-04 - MSE: 2.9386e-04 - val_loss: 1.6947e-04 - val_MSE: 1.6947e-04\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 2.6139e-04 - MSE: 2.6139e-04 - val_loss: 1.4282e-04 - val_MSE: 1.4282e-04\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 2.2871e-04 - MSE: 2.2871e-04 - val_loss: 1.2383e-04 - val_MSE: 1.2383e-04\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 9s 46ms/step - loss: 1.9425e-04 - MSE: 1.9425e-04 - val_loss: 1.0673e-04 - val_MSE: 1.0673e-04\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.6424e-04 - MSE: 1.6424e-04 - val_loss: 9.3116e-05 - val_MSE: 9.3116e-05\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 9s 46ms/step - loss: 1.6182e-04 - MSE: 1.6182e-04 - val_loss: 8.0264e-05 - val_MSE: 8.0264e-05\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.2956e-04 - MSE: 1.2956e-04 - val_loss: 6.9059e-05 - val_MSE: 6.9059e-05\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 9s 46ms/step - loss: 1.0946e-04 - MSE: 1.0946e-04 - val_loss: 5.9735e-05 - val_MSE: 5.9735e-05\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 12s 59ms/step - loss: 9.8538e-05 - MSE: 9.8538e-05 - val_loss: 5.3143e-05 - val_MSE: 5.3143e-05\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 8s 42ms/step - loss: 8.8831e-05 - MSE: 8.8831e-05 - val_loss: 4.5975e-05 - val_MSE: 4.5975e-05\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 7.8474e-05 - MSE: 7.8474e-05 - val_loss: 4.1550e-05 - val_MSE: 4.1550e-05\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 6.8755e-05 - MSE: 6.8755e-05 - val_loss: 3.5271e-05 - val_MSE: 3.5271e-05\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 6.1555e-05 - MSE: 6.1555e-05 - val_loss: 3.2103e-05 - val_MSE: 3.2103e-05\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 5.6827e-05 - MSE: 5.6827e-05 - val_loss: 2.8642e-05 - val_MSE: 2.8642e-05\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 10s 53ms/step - loss: 4.5928e-05 - MSE: 4.5928e-05 - val_loss: 2.4120e-05 - val_MSE: 2.4120e-05\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 4.5049e-05 - MSE: 4.5049e-05 - val_loss: 2.1266e-05 - val_MSE: 2.1266e-05\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 12s 61ms/step - loss: 4.6830e-05 - MSE: 4.6830e-05 - val_loss: 2.2118e-05 - val_MSE: 2.2118e-05\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 3.3134e-05 - MSE: 3.3134e-05 - val_loss: 1.7404e-05 - val_MSE: 1.7404e-05\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 12s 58ms/step - loss: 3.2008e-05 - MSE: 3.2008e-05 - val_loss: 1.4728e-05 - val_MSE: 1.4728e-05\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 9s 48ms/step - loss: 2.8519e-05 - MSE: 2.8519e-05 - val_loss: 1.3907e-05 - val_MSE: 1.3907e-05\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 2.6411e-05 - MSE: 2.6411e-05 - val_loss: 1.2187e-05 - val_MSE: 1.2187e-05\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 2.2775e-05 - MSE: 2.2775e-05 - val_loss: 1.1228e-05 - val_MSE: 1.1228e-05\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 2.0923e-05 - MSE: 2.0923e-05 - val_loss: 1.0158e-05 - val_MSE: 1.0158e-05\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 2.1467e-05 - MSE: 2.1467e-05 - val_loss: 9.9131e-06 - val_MSE: 9.9131e-06\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 1.8631e-05 - MSE: 1.8631e-05 - val_loss: 1.0241e-05 - val_MSE: 1.0241e-05\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 9s 43ms/step - loss: 1.5727e-05 - MSE: 1.5727e-05 - val_loss: 9.8618e-06 - val_MSE: 9.8618e-06\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 1.5169e-05 - MSE: 1.5169e-05 - val_loss: 9.1983e-06 - val_MSE: 9.1983e-06\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.5866e-05 - MSE: 1.5866e-05 - val_loss: 6.6918e-06 - val_MSE: 6.6918e-06\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 12s 58ms/step - loss: 1.2974e-05 - MSE: 1.2974e-05 - val_loss: 5.8711e-06 - val_MSE: 5.8711e-06\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 1.1721e-05 - MSE: 1.1721e-05 - val_loss: 5.9659e-06 - val_MSE: 5.9659e-06\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 1.1869e-05 - MSE: 1.1869e-05 - val_loss: 5.2445e-06 - val_MSE: 5.2445e-06\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 12s 59ms/step - loss: 9.8721e-06 - MSE: 9.8721e-06 - val_loss: 4.9417e-06 - val_MSE: 4.9417e-06\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 9.8809e-06 - MSE: 9.8809e-06 - val_loss: 6.2950e-06 - val_MSE: 6.2950e-06\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 9.0501e-06 - MSE: 9.0501e-06 - val_loss: 4.4779e-06 - val_MSE: 4.4779e-06\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 9s 43ms/step - loss: 8.8619e-06 - MSE: 8.8619e-06 - val_loss: 4.1960e-06 - val_MSE: 4.1960e-06\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 9s 41ms/step - loss: 7.8871e-06 - MSE: 7.8871e-06 - val_loss: 4.0544e-06 - val_MSE: 4.0544e-06\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 12s 59ms/step - loss: 8.3712e-06 - MSE: 8.3712e-06 - val_loss: 3.9519e-06 - val_MSE: 3.9519e-06\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 7.7759e-06 - MSE: 7.7759e-06 - val_loss: 4.0747e-06 - val_MSE: 4.0747e-06\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 7.0130e-06 - MSE: 7.0130e-06 - val_loss: 4.1173e-06 - val_MSE: 4.1173e-06\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 6.5103e-06 - MSE: 6.5103e-06 - val_loss: 3.6351e-06 - val_MSE: 3.6351e-06\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 12s 59ms/step - loss: 6.4243e-06 - MSE: 6.4243e-06 - val_loss: 3.5113e-06 - val_MSE: 3.5113e-06\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 8s 42ms/step - loss: 6.2098e-06 - MSE: 6.2098e-06 - val_loss: 3.3921e-06 - val_MSE: 3.3921e-06\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 5.6217e-06 - MSE: 5.6217e-06 - val_loss: 3.3308e-06 - val_MSE: 3.3308e-06\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 12s 49ms/step - loss: 5.5134e-06 - MSE: 5.5134e-06 - val_loss: 3.8976e-06 - val_MSE: 3.8976e-06\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 5.4790e-06 - MSE: 5.4790e-06 - val_loss: 3.2482e-06 - val_MSE: 3.2482e-06\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 5.1571e-06 - MSE: 5.1571e-06 - val_loss: 3.4206e-06 - val_MSE: 3.4206e-06\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 5.1479e-06 - MSE: 5.1479e-06 - val_loss: 3.2883e-06 - val_MSE: 3.2883e-06\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 4.8688e-06 - MSE: 4.8688e-06 - val_loss: 3.1056e-06 - val_MSE: 3.1056e-06\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 9s 48ms/step - loss: 5.0722e-06 - MSE: 5.0722e-06 - val_loss: 3.0339e-06 - val_MSE: 3.0339e-06\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 4.8064e-06 - MSE: 4.8064e-06 - val_loss: 3.4525e-06 - val_MSE: 3.4525e-06\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 13s 65ms/step - loss: 5.6005e-06 - MSE: 5.6005e-06 - val_loss: 3.6747e-06 - val_MSE: 3.6747e-06\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 8s 42ms/step - loss: 4.5141e-06 - MSE: 4.5141e-06 - val_loss: 3.0278e-06 - val_MSE: 3.0278e-06\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 10s 53ms/step - loss: 4.4205e-06 - MSE: 4.4205e-06 - val_loss: 2.9860e-06 - val_MSE: 2.9860e-06\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 4.2788e-06 - MSE: 4.2788e-06 - val_loss: 2.9076e-06 - val_MSE: 2.9076e-06\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 10s 53ms/step - loss: 4.3173e-06 - MSE: 4.3173e-06 - val_loss: 2.9915e-06 - val_MSE: 2.9915e-06\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 4.3277e-06 - MSE: 4.3277e-06 - val_loss: 2.8676e-06 - val_MSE: 2.8676e-06\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 4.2621e-06 - MSE: 4.2621e-06 - val_loss: 2.9741e-06 - val_MSE: 2.9741e-06\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 4.2326e-06 - MSE: 4.2326e-06 - val_loss: 2.8697e-06 - val_MSE: 2.8697e-06\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 9s 45ms/step - loss: 4.2901e-06 - MSE: 4.2901e-06 - val_loss: 2.8451e-06 - val_MSE: 2.8451e-06\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 4.1933e-06 - MSE: 4.1933e-06 - val_loss: 2.8262e-06 - val_MSE: 2.8262e-06\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 4.2017e-06 - MSE: 4.2017e-06 - val_loss: 2.8769e-06 - val_MSE: 2.8769e-06\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 4.0348e-06 - MSE: 4.0348e-06 - val_loss: 2.8212e-06 - val_MSE: 2.8212e-06\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 4.0050e-06 - MSE: 4.0050e-06 - val_loss: 2.9261e-06 - val_MSE: 2.9261e-06\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 3.9897e-06 - MSE: 3.9897e-06 - val_loss: 3.0925e-06 - val_MSE: 3.0925e-06\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 12s 59ms/step - loss: 4.0758e-06 - MSE: 4.0758e-06 - val_loss: 2.7922e-06 - val_MSE: 2.7922e-06\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 3.9671e-06 - MSE: 3.9671e-06 - val_loss: 2.7745e-06 - val_MSE: 2.7745e-06\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 12s 59ms/step - loss: 3.8605e-06 - MSE: 3.8605e-06 - val_loss: 2.8095e-06 - val_MSE: 2.8095e-06\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 3.8484e-06 - MSE: 3.8484e-06 - val_loss: 2.8448e-06 - val_MSE: 2.8448e-06\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 3.8832e-06 - MSE: 3.8832e-06 - val_loss: 2.8404e-06 - val_MSE: 2.8404e-06\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 12s 59ms/step - loss: 3.8929e-06 - MSE: 3.8929e-06 - val_loss: 2.8747e-06 - val_MSE: 2.8747e-06\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 3.8789e-06 - MSE: 3.8789e-06 - val_loss: 2.7820e-06 - val_MSE: 2.7820e-06\n",
      "INFO:tensorflow:Assets written to: saved_model/MLI_30_log_s103_sigmoid_4layers\\assets\n",
      "Dataset MLI has been trained and saved.\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 19s 35ms/step - loss: 3.7177 - MSE: 3.7177 - val_loss: 0.2760 - val_MSE: 0.2760\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 16s 36ms/step - loss: 0.2188 - MSE: 0.2188 - val_loss: 0.1309 - val_MSE: 0.1309\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 16s 35ms/step - loss: 0.1210 - MSE: 0.1210 - val_loss: 0.0747 - val_MSE: 0.0747\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 14s 32ms/step - loss: 0.0723 - MSE: 0.0723 - val_loss: 0.0438 - val_MSE: 0.0438\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 18s 41ms/step - loss: 0.0414 - MSE: 0.0414 - val_loss: 0.0245 - val_MSE: 0.0245\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 17s 37ms/step - loss: 0.0229 - MSE: 0.0229 - val_loss: 0.0143 - val_MSE: 0.0143\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 12s 26ms/step - loss: 0.0136 - MSE: 0.0136 - val_loss: 0.0088 - val_MSE: 0.0088\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 14s 31ms/step - loss: 0.0084 - MSE: 0.0084 - val_loss: 0.0060 - val_MSE: 0.0060\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 15s 34ms/step - loss: 0.0057 - MSE: 0.0057 - val_loss: 0.0041 - val_MSE: 0.0041\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 12s 27ms/step - loss: 0.0041 - MSE: 0.0041 - val_loss: 0.0030 - val_MSE: 0.0030\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 15s 33ms/step - loss: 0.0030 - MSE: 0.0030 - val_loss: 0.0022 - val_MSE: 0.0022\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 15s 33ms/step - loss: 0.0021 - MSE: 0.0021 - val_loss: 0.0016 - val_MSE: 0.0016\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 19s 42ms/step - loss: 0.0016 - MSE: 0.0016 - val_loss: 0.0012 - val_MSE: 0.0012\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 15s 34ms/step - loss: 0.0012 - MSE: 0.0012 - val_loss: 8.5944e-04 - val_MSE: 8.5944e-04\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 21s 47ms/step - loss: 9.0294e-04 - MSE: 9.0294e-04 - val_loss: 6.4159e-04 - val_MSE: 6.4159e-04\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 14s 32ms/step - loss: 6.6152e-04 - MSE: 6.6152e-04 - val_loss: 4.6892e-04 - val_MSE: 4.6892e-04\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 15s 34ms/step - loss: 5.0311e-04 - MSE: 5.0311e-04 - val_loss: 3.4892e-04 - val_MSE: 3.4892e-04\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 19s 41ms/step - loss: 3.9381e-04 - MSE: 3.9381e-04 - val_loss: 2.6656e-04 - val_MSE: 2.6656e-04\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 20s 44ms/step - loss: 3.0537e-04 - MSE: 3.0537e-04 - val_loss: 2.0289e-04 - val_MSE: 2.0289e-04\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 16s 35ms/step - loss: 2.3074e-04 - MSE: 2.3074e-04 - val_loss: 1.5049e-04 - val_MSE: 1.5049e-04\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 22s 48ms/step - loss: 1.7425e-04 - MSE: 1.7425e-04 - val_loss: 1.1472e-04 - val_MSE: 1.1472e-04\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 14s 31ms/step - loss: 1.3613e-04 - MSE: 1.3613e-04 - val_loss: 8.6865e-05 - val_MSE: 8.6865e-05\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 26s 58ms/step - loss: 1.0715e-04 - MSE: 1.0715e-04 - val_loss: 6.4982e-05 - val_MSE: 6.4982e-05\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 24s 52ms/step - loss: 8.4691e-05 - MSE: 8.4691e-05 - val_loss: 4.9882e-05 - val_MSE: 4.9882e-05\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 30s 66ms/step - loss: 6.4696e-05 - MSE: 6.4696e-05 - val_loss: 3.9617e-05 - val_MSE: 3.9617e-05\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 21s 46ms/step - loss: 5.0484e-05 - MSE: 5.0484e-05 - val_loss: 3.0278e-05 - val_MSE: 3.0278e-05\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 15s 33ms/step - loss: 3.8670e-05 - MSE: 3.8670e-05 - val_loss: 2.4095e-05 - val_MSE: 2.4095e-05\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 26s 58ms/step - loss: 3.2924e-05 - MSE: 3.2924e-05 - val_loss: 1.9511e-05 - val_MSE: 1.9511e-05\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 17s 39ms/step - loss: 2.5676e-05 - MSE: 2.5676e-05 - val_loss: 1.5852e-05 - val_MSE: 1.5852e-05\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 2.1384e-05 - MSE: 2.1384e-05 - val_loss: 1.3368e-05 - val_MSE: 1.3368e-05\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 24s 53ms/step - loss: 1.7887e-05 - MSE: 1.7887e-05 - val_loss: 1.1532e-05 - val_MSE: 1.1532e-05\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 23s 50ms/step - loss: 1.5762e-05 - MSE: 1.5762e-05 - val_loss: 9.9965e-06 - val_MSE: 9.9965e-06\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 21s 47ms/step - loss: 1.3751e-05 - MSE: 1.3751e-05 - val_loss: 9.2304e-06 - val_MSE: 9.2304e-06\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 24s 53ms/step - loss: 1.2219e-05 - MSE: 1.2219e-05 - val_loss: 8.0950e-06 - val_MSE: 8.0950e-06\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 22s 50ms/step - loss: 1.1778e-05 - MSE: 1.1778e-05 - val_loss: 7.4386e-06 - val_MSE: 7.4386e-06\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 22s 48ms/step - loss: 1.0626e-05 - MSE: 1.0626e-05 - val_loss: 6.8064e-06 - val_MSE: 6.8064e-06\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 24s 52ms/step - loss: 9.5235e-06 - MSE: 9.5235e-06 - val_loss: 6.4111e-06 - val_MSE: 6.4111e-06\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 25s 56ms/step - loss: 9.3794e-06 - MSE: 9.3794e-06 - val_loss: 6.4672e-06 - val_MSE: 6.4672e-06\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 8.6414e-06 - MSE: 8.6414e-06 - val_loss: 5.8047e-06 - val_MSE: 5.8047e-06\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 24s 53ms/step - loss: 8.4160e-06 - MSE: 8.4160e-06 - val_loss: 5.6345e-06 - val_MSE: 5.6345e-06\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 23s 51ms/step - loss: 8.0927e-06 - MSE: 8.0927e-06 - val_loss: 5.4574e-06 - val_MSE: 5.4574e-06\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 21s 48ms/step - loss: 7.9461e-06 - MSE: 7.9461e-06 - val_loss: 5.4043e-06 - val_MSE: 5.4043e-06\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 22s 48ms/step - loss: 7.7759e-06 - MSE: 7.7759e-06 - val_loss: 5.2129e-06 - val_MSE: 5.2129e-06\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 23s 50ms/step - loss: 7.6064e-06 - MSE: 7.6064e-06 - val_loss: 5.1687e-06 - val_MSE: 5.1687e-06\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 23s 52ms/step - loss: 7.4940e-06 - MSE: 7.4940e-06 - val_loss: 5.0812e-06 - val_MSE: 5.0812e-06\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 22s 48ms/step - loss: 7.4951e-06 - MSE: 7.4951e-06 - val_loss: 5.0143e-06 - val_MSE: 5.0143e-06\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 24s 53ms/step - loss: 7.3778e-06 - MSE: 7.3778e-06 - val_loss: 5.0109e-06 - val_MSE: 5.0109e-06\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 22s 50ms/step - loss: 7.3276e-06 - MSE: 7.3276e-06 - val_loss: 4.9477e-06 - val_MSE: 4.9477e-06\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 13s 28ms/step - loss: 7.2823e-06 - MSE: 7.2823e-06 - val_loss: 4.9450e-06 - val_MSE: 4.9450e-06\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 7s 15ms/step - loss: 7.3128e-06 - MSE: 7.3128e-06 - val_loss: 4.8957e-06 - val_MSE: 4.8957e-06\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 14s 31ms/step - loss: 7.1973e-06 - MSE: 7.1973e-06 - val_loss: 4.8928e-06 - val_MSE: 4.8928e-06\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 33s 75ms/step - loss: 7.1810e-06 - MSE: 7.1810e-06 - val_loss: 4.9069e-06 - val_MSE: 4.9069e-06\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 50s 112ms/step - loss: 7.1721e-06 - MSE: 7.1721e-06 - val_loss: 4.8704e-06 - val_MSE: 4.8704e-06\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 13s 30ms/step - loss: 7.1482e-06 - MSE: 7.1482e-06 - val_loss: 4.8467e-06 - val_MSE: 4.8467e-06\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 42s 94ms/step - loss: 7.1612e-06 - MSE: 7.1612e-06 - val_loss: 4.8904e-06 - val_MSE: 4.8904e-06\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 26s 58ms/step - loss: 7.1630e-06 - MSE: 7.1630e-06 - val_loss: 4.8526e-06 - val_MSE: 4.8526e-06\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 7.1331e-06 - MSE: 7.1331e-06 - val_loss: 4.8215e-06 - val_MSE: 4.8215e-06\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 12s 27ms/step - loss: 7.1239e-06 - MSE: 7.1239e-06 - val_loss: 4.8469e-06 - val_MSE: 4.8469e-06\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 18s 41ms/step - loss: 7.1112e-06 - MSE: 7.1112e-06 - val_loss: 4.8359e-06 - val_MSE: 4.8359e-06\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 17s 39ms/step - loss: 7.1065e-06 - MSE: 7.1065e-06 - val_loss: 4.8963e-06 - val_MSE: 4.8963e-06\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 7.0932e-06 - MSE: 7.0932e-06 - val_loss: 4.8453e-06 - val_MSE: 4.8453e-06\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 12s 26ms/step - loss: 7.1073e-06 - MSE: 7.1073e-06 - val_loss: 4.8044e-06 - val_MSE: 4.8044e-06\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 56s 125ms/step - loss: 7.0923e-06 - MSE: 7.0923e-06 - val_loss: 4.8241e-06 - val_MSE: 4.8241e-06\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 47s 104ms/step - loss: 7.0974e-06 - MSE: 7.0974e-06 - val_loss: 4.8246e-06 - val_MSE: 4.8246e-06\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 36s 80ms/step - loss: 7.0778e-06 - MSE: 7.0778e-06 - val_loss: 4.8723e-06 - val_MSE: 4.8723e-06\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 12s 27ms/step - loss: 7.0739e-06 - MSE: 7.0739e-06 - val_loss: 4.8698e-06 - val_MSE: 4.8698e-06\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 14s 32ms/step - loss: 7.0686e-06 - MSE: 7.0686e-06 - val_loss: 4.8623e-06 - val_MSE: 4.8623e-06\n",
      "INFO:tensorflow:Assets written to: saved_model/NFE_30_log_s103_sigmoid_4layers\\assets\n",
      "Dataset NFE has been trained and saved.\n",
      "Epoch 1/100\n",
      "349/349 [==============================] - 15s 33ms/step - loss: 0.4958 - MSE: 0.4958 - val_loss: 0.0715 - val_MSE: 0.0715\n",
      "Epoch 2/100\n",
      "349/349 [==============================] - 8s 22ms/step - loss: 0.0752 - MSE: 0.0752 - val_loss: 0.0330 - val_MSE: 0.0330\n",
      "Epoch 3/100\n",
      "349/349 [==============================] - 10s 30ms/step - loss: 0.0347 - MSE: 0.0347 - val_loss: 0.0149 - val_MSE: 0.0149\n",
      "Epoch 4/100\n",
      "349/349 [==============================] - 8s 22ms/step - loss: 0.0177 - MSE: 0.0177 - val_loss: 0.0076 - val_MSE: 0.0076\n",
      "Epoch 5/100\n",
      "349/349 [==============================] - 10s 28ms/step - loss: 0.0104 - MSE: 0.0104 - val_loss: 0.0047 - val_MSE: 0.0047\n",
      "Epoch 6/100\n",
      "349/349 [==============================] - 11s 33ms/step - loss: 0.0070 - MSE: 0.0070 - val_loss: 0.0032 - val_MSE: 0.0032\n",
      "Epoch 7/100\n",
      "349/349 [==============================] - 24s 69ms/step - loss: 0.0049 - MSE: 0.0049 - val_loss: 0.0022 - val_MSE: 0.0022\n",
      "Epoch 8/100\n",
      "349/349 [==============================] - 28s 81ms/step - loss: 0.0035 - MSE: 0.0035 - val_loss: 0.0016 - val_MSE: 0.0016\n",
      "Epoch 9/100\n",
      "349/349 [==============================] - 12s 33ms/step - loss: 0.0026 - MSE: 0.0026 - val_loss: 0.0012 - val_MSE: 0.0012\n",
      "Epoch 10/100\n",
      "349/349 [==============================] - 15s 43ms/step - loss: 0.0019 - MSE: 0.0019 - val_loss: 8.7277e-04 - val_MSE: 8.7277e-04\n",
      "Epoch 11/100\n",
      "349/349 [==============================] - 11s 33ms/step - loss: 0.0014 - MSE: 0.0014 - val_loss: 6.4547e-04 - val_MSE: 6.4547e-04\n",
      "Epoch 12/100\n",
      "349/349 [==============================] - 11s 30ms/step - loss: 0.0011 - MSE: 0.0011 - val_loss: 4.7332e-04 - val_MSE: 4.7332e-04\n",
      "Epoch 13/100\n",
      "349/349 [==============================] - 8s 23ms/step - loss: 8.6797e-04 - MSE: 8.6797e-04 - val_loss: 3.5043e-04 - val_MSE: 3.5043e-04\n",
      "Epoch 14/100\n",
      "349/349 [==============================] - 14s 39ms/step - loss: 6.5942e-04 - MSE: 6.5942e-04 - val_loss: 2.6045e-04 - val_MSE: 2.6045e-04\n",
      "Epoch 15/100\n",
      "349/349 [==============================] - 8s 23ms/step - loss: 5.2732e-04 - MSE: 5.2732e-04 - val_loss: 1.9320e-04 - val_MSE: 1.9320e-04\n",
      "Epoch 16/100\n",
      "349/349 [==============================] - 11s 32ms/step - loss: 4.2263e-04 - MSE: 4.2263e-04 - val_loss: 1.4983e-04 - val_MSE: 1.4983e-04\n",
      "Epoch 17/100\n",
      "349/349 [==============================] - 11s 32ms/step - loss: 3.2541e-04 - MSE: 3.2541e-04 - val_loss: 1.1402e-04 - val_MSE: 1.1402e-04\n",
      "Epoch 18/100\n",
      "349/349 [==============================] - 10s 30ms/step - loss: 2.6818e-04 - MSE: 2.6818e-04 - val_loss: 8.5452e-05 - val_MSE: 8.5452e-05\n",
      "Epoch 19/100\n",
      "349/349 [==============================] - 11s 31ms/step - loss: 2.1010e-04 - MSE: 2.1010e-04 - val_loss: 6.6692e-05 - val_MSE: 6.6692e-05\n",
      "Epoch 20/100\n",
      "349/349 [==============================] - 8s 22ms/step - loss: 1.7485e-04 - MSE: 1.7485e-04 - val_loss: 5.2519e-05 - val_MSE: 5.2519e-05\n",
      "Epoch 21/100\n",
      "349/349 [==============================] - 10s 29ms/step - loss: 1.3588e-04 - MSE: 1.3588e-04 - val_loss: 4.1792e-05 - val_MSE: 4.1792e-05\n",
      "Epoch 22/100\n",
      "349/349 [==============================] - 14s 41ms/step - loss: 1.1335e-04 - MSE: 1.1335e-04 - val_loss: 3.5465e-05 - val_MSE: 3.5465e-05\n",
      "Epoch 23/100\n",
      "349/349 [==============================] - 21s 61ms/step - loss: 8.9775e-05 - MSE: 8.9775e-05 - val_loss: 2.7447e-05 - val_MSE: 2.7447e-05\n",
      "Epoch 24/100\n",
      "349/349 [==============================] - 14s 41ms/step - loss: 7.5421e-05 - MSE: 7.5421e-05 - val_loss: 2.2728e-05 - val_MSE: 2.2728e-05\n",
      "Epoch 25/100\n",
      "349/349 [==============================] - 25s 72ms/step - loss: 6.2393e-05 - MSE: 6.2393e-05 - val_loss: 1.8563e-05 - val_MSE: 1.8563e-05\n",
      "Epoch 26/100\n",
      "349/349 [==============================] - 8s 22ms/step - loss: 5.2473e-05 - MSE: 5.2473e-05 - val_loss: 1.5957e-05 - val_MSE: 1.5957e-05\n",
      "Epoch 27/100\n",
      "349/349 [==============================] - 25s 71ms/step - loss: 4.2634e-05 - MSE: 4.2634e-05 - val_loss: 1.2541e-05 - val_MSE: 1.2541e-05\n",
      "Epoch 28/100\n",
      "349/349 [==============================] - 15s 43ms/step - loss: 3.4661e-05 - MSE: 3.4661e-05 - val_loss: 1.0965e-05 - val_MSE: 1.0965e-05\n",
      "Epoch 29/100\n",
      "349/349 [==============================] - 16s 47ms/step - loss: 2.9006e-05 - MSE: 2.9006e-05 - val_loss: 8.6597e-06 - val_MSE: 8.6597e-06\n",
      "Epoch 30/100\n",
      "349/349 [==============================] - 18s 53ms/step - loss: 2.4283e-05 - MSE: 2.4283e-05 - val_loss: 7.2637e-06 - val_MSE: 7.2637e-06\n",
      "Epoch 31/100\n",
      "349/349 [==============================] - 8s 22ms/step - loss: 1.9888e-05 - MSE: 1.9888e-05 - val_loss: 6.1194e-06 - val_MSE: 6.1194e-06\n",
      "Epoch 32/100\n",
      "349/349 [==============================] - 14s 40ms/step - loss: 1.6620e-05 - MSE: 1.6620e-05 - val_loss: 5.2128e-06 - val_MSE: 5.2128e-06\n",
      "Epoch 33/100\n",
      "349/349 [==============================] - 8s 22ms/step - loss: 1.4101e-05 - MSE: 1.4101e-05 - val_loss: 4.2960e-06 - val_MSE: 4.2960e-06\n",
      "Epoch 34/100\n",
      "349/349 [==============================] - 16s 46ms/step - loss: 1.1926e-05 - MSE: 1.1926e-05 - val_loss: 4.8968e-06 - val_MSE: 4.8968e-06\n",
      "Epoch 35/100\n",
      "349/349 [==============================] - 8s 22ms/step - loss: 1.0087e-05 - MSE: 1.0087e-05 - val_loss: 3.1650e-06 - val_MSE: 3.1650e-06\n",
      "Epoch 36/100\n",
      "349/349 [==============================] - 10s 29ms/step - loss: 8.4461e-06 - MSE: 8.4461e-06 - val_loss: 2.7506e-06 - val_MSE: 2.7506e-06\n",
      "Epoch 37/100\n",
      "349/349 [==============================] - 45s 130ms/step - loss: 7.4826e-06 - MSE: 7.4826e-06 - val_loss: 2.6175e-06 - val_MSE: 2.6175e-06\n",
      "Epoch 38/100\n",
      "349/349 [==============================] - 33s 94ms/step - loss: 6.1779e-06 - MSE: 6.1779e-06 - val_loss: 2.1379e-06 - val_MSE: 2.1379e-06\n",
      "Epoch 39/100\n",
      "349/349 [==============================] - 32s 91ms/step - loss: 5.3793e-06 - MSE: 5.3793e-06 - val_loss: 1.9047e-06 - val_MSE: 1.9047e-06\n",
      "Epoch 40/100\n",
      "349/349 [==============================] - 49s 142ms/step - loss: 4.7391e-06 - MSE: 4.7391e-06 - val_loss: 2.1327e-06 - val_MSE: 2.1327e-06\n",
      "Epoch 41/100\n",
      "349/349 [==============================] - 46s 133ms/step - loss: 4.2497e-06 - MSE: 4.2497e-06 - val_loss: 1.7396e-06 - val_MSE: 1.7396e-06\n",
      "Epoch 42/100\n",
      "349/349 [==============================] - 53s 150ms/step - loss: 3.7838e-06 - MSE: 3.7838e-06 - val_loss: 1.6251e-06 - val_MSE: 1.6251e-06\n",
      "Epoch 43/100\n",
      "349/349 [==============================] - 28s 82ms/step - loss: 3.3022e-06 - MSE: 3.3022e-06 - val_loss: 1.5058e-06 - val_MSE: 1.5058e-06\n",
      "Epoch 44/100\n",
      "349/349 [==============================] - 29s 82ms/step - loss: 2.9997e-06 - MSE: 2.9997e-06 - val_loss: 1.4541e-06 - val_MSE: 1.4541e-06\n",
      "Epoch 45/100\n",
      "349/349 [==============================] - 18s 52ms/step - loss: 2.8028e-06 - MSE: 2.8028e-06 - val_loss: 1.2655e-06 - val_MSE: 1.2655e-06\n",
      "Epoch 46/100\n",
      "349/349 [==============================] - 10s 30ms/step - loss: 2.6098e-06 - MSE: 2.6098e-06 - val_loss: 2.7897e-06 - val_MSE: 2.7897e-06\n",
      "Epoch 47/100\n",
      "349/349 [==============================] - 8s 22ms/step - loss: 2.4815e-06 - MSE: 2.4815e-06 - val_loss: 1.8930e-06 - val_MSE: 1.8930e-06\n",
      "Epoch 48/100\n",
      "349/349 [==============================] - 10s 30ms/step - loss: 2.2753e-06 - MSE: 2.2753e-06 - val_loss: 1.7597e-06 - val_MSE: 1.7597e-06\n",
      "Epoch 49/100\n",
      "349/349 [==============================] - 11s 31ms/step - loss: 2.1936e-06 - MSE: 2.1936e-06 - val_loss: 6.3609e-06 - val_MSE: 6.3609e-06\n",
      "Epoch 50/100\n",
      "349/349 [==============================] - 40s 115ms/step - loss: 2.1365e-06 - MSE: 2.1365e-06 - val_loss: 1.2544e-06 - val_MSE: 1.2544e-06\n",
      "Epoch 51/100\n",
      "349/349 [==============================] - 46s 131ms/step - loss: 1.9448e-06 - MSE: 1.9448e-06 - val_loss: 1.6123e-06 - val_MSE: 1.6123e-06\n",
      "Epoch 52/100\n",
      "349/349 [==============================] - 48s 137ms/step - loss: 1.9120e-06 - MSE: 1.9120e-06 - val_loss: 1.5768e-06 - val_MSE: 1.5768e-06\n",
      "Epoch 53/100\n",
      "349/349 [==============================] - 12s 33ms/step - loss: 1.8605e-06 - MSE: 1.8605e-06 - val_loss: 2.0896e-06 - val_MSE: 2.0896e-06\n",
      "Epoch 54/100\n",
      "349/349 [==============================] - 24s 70ms/step - loss: 1.8495e-06 - MSE: 1.8495e-06 - val_loss: 3.2319e-06 - val_MSE: 3.2319e-06\n",
      "Epoch 55/100\n",
      "349/349 [==============================] - 45s 130ms/step - loss: 1.8641e-06 - MSE: 1.8641e-06 - val_loss: 1.4388e-06 - val_MSE: 1.4388e-06\n",
      "INFO:tensorflow:Assets written to: saved_model/NOC_30_log_s103_sigmoid_4layers\\assets\n",
      "Dataset NOC has been trained and saved.\n",
      "Epoch 1/100\n",
      "531/531 [==============================] - 18s 29ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "Epoch 2/100\n",
      "531/531 [==============================] - 18s 34ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "Epoch 3/100\n",
      "531/531 [==============================] - 59s 112ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "Epoch 4/100\n",
      "531/531 [==============================] - 36s 67ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "Epoch 5/100\n",
      "531/531 [==============================] - 14s 27ms/step - loss: nan - MSE: nan - val_loss: nan - val_MSE: nan\n",
      "INFO:tensorflow:Assets written to: saved_model/PBR_30_log_s103_sigmoid_4layers\\assets\n",
      "Dataset PBR has been trained and saved.\n",
      "Epoch 1/100\n",
      "562/562 [==============================] - 27s 41ms/step - loss: 0.3177 - MSE: 0.3177 - val_loss: 0.0505 - val_MSE: 0.0505\n",
      "Epoch 2/100\n",
      "562/562 [==============================] - 32s 57ms/step - loss: 0.0393 - MSE: 0.0393 - val_loss: 0.0149 - val_MSE: 0.0149\n",
      "Epoch 3/100\n",
      "562/562 [==============================] - 29s 52ms/step - loss: 0.0145 - MSE: 0.0145 - val_loss: 0.0059 - val_MSE: 0.0059\n",
      "Epoch 4/100\n",
      "562/562 [==============================] - 28s 50ms/step - loss: 0.0064 - MSE: 0.0064 - val_loss: 0.0027 - val_MSE: 0.0027\n",
      "Epoch 5/100\n",
      "562/562 [==============================] - 28s 50ms/step - loss: 0.0033 - MSE: 0.0033 - val_loss: 0.0014 - val_MSE: 0.0014\n",
      "Epoch 6/100\n",
      "562/562 [==============================] - 28s 50ms/step - loss: 0.0018 - MSE: 0.0018 - val_loss: 7.9871e-04 - val_MSE: 7.9871e-04\n",
      "Epoch 7/100\n",
      "562/562 [==============================] - 28s 50ms/step - loss: 0.0011 - MSE: 0.0011 - val_loss: 4.8792e-04 - val_MSE: 4.8792e-04\n",
      "Epoch 8/100\n",
      "562/562 [==============================] - 28s 46ms/step - loss: 6.9224e-04 - MSE: 6.9224e-04 - val_loss: 3.1464e-04 - val_MSE: 3.1464e-04\n",
      "Epoch 9/100\n",
      "562/562 [==============================] - 28s 49ms/step - loss: 4.5665e-04 - MSE: 4.5665e-04 - val_loss: 2.0536e-04 - val_MSE: 2.0536e-04\n",
      "Epoch 10/100\n",
      "562/562 [==============================] - 30s 54ms/step - loss: 3.0419e-04 - MSE: 3.0419e-04 - val_loss: 1.3961e-04 - val_MSE: 1.3961e-04\n",
      "Epoch 11/100\n",
      "562/562 [==============================] - 28s 50ms/step - loss: 2.0784e-04 - MSE: 2.0784e-04 - val_loss: 9.3797e-05 - val_MSE: 9.3797e-05\n",
      "Epoch 12/100\n",
      "562/562 [==============================] - 29s 52ms/step - loss: 1.4267e-04 - MSE: 1.4267e-04 - val_loss: 6.5173e-05 - val_MSE: 6.5173e-05\n",
      "Epoch 13/100\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 9.8850e-05 - MSE: 9.8850e-05 - val_loss: 4.4761e-05 - val_MSE: 4.4761e-05\n",
      "Epoch 14/100\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 6.9838e-05 - MSE: 6.9838e-05 - val_loss: 3.3424e-05 - val_MSE: 3.3424e-05\n",
      "Epoch 15/100\n",
      "562/562 [==============================] - 21s 38ms/step - loss: 5.0393e-05 - MSE: 5.0393e-05 - val_loss: 2.2657e-05 - val_MSE: 2.2657e-05\n",
      "Epoch 16/100\n",
      "562/562 [==============================] - 18s 31ms/step - loss: 3.5022e-05 - MSE: 3.5022e-05 - val_loss: 1.6409e-05 - val_MSE: 1.6409e-05\n",
      "Epoch 17/100\n",
      "562/562 [==============================] - 22s 40ms/step - loss: 2.7050e-05 - MSE: 2.7050e-05 - val_loss: 1.3610e-05 - val_MSE: 1.3610e-05\n",
      "Epoch 18/100\n",
      "562/562 [==============================] - 14s 25ms/step - loss: 1.9335e-05 - MSE: 1.9335e-05 - val_loss: 9.6090e-06 - val_MSE: 9.6090e-06\n",
      "Epoch 19/100\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 1.5761e-05 - MSE: 1.5761e-05 - val_loss: 7.1917e-06 - val_MSE: 7.1917e-06\n",
      "Epoch 20/100\n",
      "562/562 [==============================] - 5s 8ms/step - loss: 1.2054e-05 - MSE: 1.2054e-05 - val_loss: 6.0597e-06 - val_MSE: 6.0597e-06\n",
      "Epoch 21/100\n",
      "562/562 [==============================] - 5s 9ms/step - loss: 9.8593e-06 - MSE: 9.8593e-06 - val_loss: 4.8455e-06 - val_MSE: 4.8455e-06\n",
      "Epoch 22/100\n",
      "562/562 [==============================] - 16s 28ms/step - loss: 8.2066e-06 - MSE: 8.2066e-06 - val_loss: 4.3541e-06 - val_MSE: 4.3541e-06\n",
      "Epoch 23/100\n",
      "562/562 [==============================] - 25s 45ms/step - loss: 6.5287e-06 - MSE: 6.5287e-06 - val_loss: 4.1291e-06 - val_MSE: 4.1291e-06\n",
      "Epoch 24/100\n",
      "562/562 [==============================] - 40s 71ms/step - loss: 5.8338e-06 - MSE: 5.8338e-06 - val_loss: 3.2690e-06 - val_MSE: 3.2690e-06\n",
      "Epoch 25/100\n",
      "562/562 [==============================] - 30s 54ms/step - loss: 4.9880e-06 - MSE: 4.9880e-06 - val_loss: 3.1175e-06 - val_MSE: 3.1175e-06\n",
      "Epoch 26/100\n",
      "562/562 [==============================] - 34s 60ms/step - loss: 4.8893e-06 - MSE: 4.8893e-06 - val_loss: 3.1137e-06 - val_MSE: 3.1137e-06\n",
      "Epoch 27/100\n",
      "562/562 [==============================] - 36s 64ms/step - loss: 4.3146e-06 - MSE: 4.3146e-06 - val_loss: 2.9450e-06 - val_MSE: 2.9450e-06\n",
      "Epoch 28/100\n",
      "562/562 [==============================] - 28s 50ms/step - loss: 3.9216e-06 - MSE: 3.9216e-06 - val_loss: 2.8720e-06 - val_MSE: 2.8720e-06\n",
      "Epoch 29/100\n",
      "562/562 [==============================] - 28s 49ms/step - loss: 3.9465e-06 - MSE: 3.9465e-06 - val_loss: 3.0750e-06 - val_MSE: 3.0750e-06\n",
      "Epoch 30/100\n",
      "562/562 [==============================] - 27s 48ms/step - loss: 3.7194e-06 - MSE: 3.7194e-06 - val_loss: 2.5006e-06 - val_MSE: 2.5006e-06\n",
      "Epoch 31/100\n",
      "562/562 [==============================] - 28s 50ms/step - loss: 3.5380e-06 - MSE: 3.5380e-06 - val_loss: 2.8640e-06 - val_MSE: 2.8640e-06\n",
      "Epoch 32/100\n",
      "562/562 [==============================] - 54s 96ms/step - loss: 3.5593e-06 - MSE: 3.5593e-06 - val_loss: 2.5475e-06 - val_MSE: 2.5475e-06\n",
      "Epoch 33/100\n",
      "562/562 [==============================] - 29s 48ms/step - loss: 3.4431e-06 - MSE: 3.4431e-06 - val_loss: 2.4204e-06 - val_MSE: 2.4204e-06\n",
      "Epoch 34/100\n",
      "562/562 [==============================] - 44s 78ms/step - loss: 3.4312e-06 - MSE: 3.4312e-06 - val_loss: 2.5655e-06 - val_MSE: 2.5655e-06\n",
      "Epoch 35/100\n",
      "562/562 [==============================] - 31s 55ms/step - loss: 3.3540e-06 - MSE: 3.3540e-06 - val_loss: 2.5288e-06 - val_MSE: 2.5288e-06\n",
      "Epoch 36/100\n",
      "562/562 [==============================] - 25s 45ms/step - loss: 3.3758e-06 - MSE: 3.3758e-06 - val_loss: 2.8809e-06 - val_MSE: 2.8809e-06\n",
      "Epoch 37/100\n",
      "562/562 [==============================] - 28s 46ms/step - loss: 3.2976e-06 - MSE: 3.2976e-06 - val_loss: 2.7455e-06 - val_MSE: 2.7455e-06\n",
      "Epoch 38/100\n",
      "562/562 [==============================] - 28s 49ms/step - loss: 3.3037e-06 - MSE: 3.3037e-06 - val_loss: 2.9004e-06 - val_MSE: 2.9004e-06\n",
      "INFO:tensorflow:Assets written to: saved_model/STLD_30_log_s103_sigmoid_4layers\\assets\n",
      "Dataset STLD has been trained and saved.\n"
     ]
    }
   ],
   "source": [
    "STRIDE = (10, 3)    # stride for feature block\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "CALLBACKS = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "all_loss = list()\n",
    "for name, train_dt, valid_dt in load_dataset(TRAIN_DIR, VALID_DIR):\n",
    "    feature = alphanet_feature_Block(STRIDE)\n",
    "    model = build_AlphaNet(\n",
    "        feature, \n",
    "        input_shape=tuple(train_dt.element_spec[0].shape), \n",
    "        output_shape=tuple(train_dt.element_spec[1].shape)[0]\n",
    "    )\n",
    "    train_dt = train_dt.batch(BATCH_SIZE)\n",
    "    valid_dt = valid_dt.batch(BATCH_SIZE)\n",
    "    history = model.fit(\n",
    "        train_dt,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=valid_dt,\n",
    "        callbacks=CALLBACKS,\n",
    "        shuffle=True,\n",
    "        use_multiprocessing=True,\n",
    "        workers=10\n",
    "    )\n",
    "    all_loss.append({\n",
    "        \"stock\": name,\n",
    "        \"epoch\": len(history.history[\"loss\"]),\n",
    "        \"train_loss\": history.history[\"loss\"][-1],\n",
    "        \"valid_loss\": history.history[\"val_loss\"][-1]\n",
    "    })\n",
    "    model.save(f\"{SAVE_DIR}/{name}_30_log_s103_sigmoid_4layers\")\n",
    "    print(f\"Dataset {name} has been trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  stock  epoch  train_loss    valid_loss\n0   LMT     62    0.000001  7.941369e-07\n1   BAH     88    0.000001  9.241734e-07\n2   NOC     55    0.000002  1.438844e-06\n3   HII     95    0.000002  2.145176e-06\n4   MLI     94    0.000004  2.782025e-06\n5  STLD     38    0.000003  2.900367e-06\n6   NFE     67    0.000007  4.862328e-06\n7    AR      5         NaN           NaN\n8   FTI      5         NaN           NaN\n9   PBR      5         NaN           NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stock</th>\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LMT</td>\n      <td>62</td>\n      <td>0.000001</td>\n      <td>7.941369e-07</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BAH</td>\n      <td>88</td>\n      <td>0.000001</td>\n      <td>9.241734e-07</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NOC</td>\n      <td>55</td>\n      <td>0.000002</td>\n      <td>1.438844e-06</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HII</td>\n      <td>95</td>\n      <td>0.000002</td>\n      <td>2.145176e-06</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MLI</td>\n      <td>94</td>\n      <td>0.000004</td>\n      <td>2.782025e-06</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>STLD</td>\n      <td>38</td>\n      <td>0.000003</td>\n      <td>2.900367e-06</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NFE</td>\n      <td>67</td>\n      <td>0.000007</td>\n      <td>4.862328e-06</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>AR</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>FTI</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>PBR</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.DataFrame(all_loss).sort_values(\"valid_loss\").reset_index(drop=True)\n",
    "temp.to_csv(f\"{OUTPUT_DIR}/all_loss_30_log_s103_sigmoid_4layers.csv\")\n",
    "temp"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "31b9f103-0a81-4cc1-8e8c-226d40f546cf",
    "98797082-b05f-4949-97e3-02b52bb03d79",
    "cyfLh4DI-5it"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "99ea4bddf300909f6d049ff7715ecae0b1516d9d48b79ec655d91803e59750be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}