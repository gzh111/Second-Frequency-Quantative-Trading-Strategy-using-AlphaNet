{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8325518a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_ENABLE_ONEDNN_OPTS=0\n"
     ]
    }
   ],
   "source": [
    "%env TF_ENABLE_ONEDNN_OPTS 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f7ec64-69af-4cc9-884c-f61023d07132",
   "metadata": {
    "executionInfo": {
     "elapsed": 2863,
     "status": "ok",
     "timestamp": 1666274947208,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "75f7ec64-69af-4cc9-884c-f61023d07132",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c70190e4-1903-41ed-8897-040f6808a020",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274947208,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "c70190e4-1903-41ed-8897-040f6808a020",
    "outputId": "248fa78f-5e9d-4db2-d134-2762d69323b5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HRKbR8ZA9tIu",
   "metadata": {
    "id": "HRKbR8ZA9tIu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DSA5205 Project\n",
    "\n",
    "Ref:\n",
    "- [AlphaNet：因子挖掘神经网络 - 华泰证券](https://crm.htsc.com.cn/doc/2020/10750101/74856806-a2e3-41cb-be4c-695dc6cc1341.pdf)\n",
    "- [再探AlphaNet：结构和特征优化 - 华泰证券](https://crm.htsc.com.cn/doc/2020/10750101/74619658-f648-4001-a255-5b78174b073a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9f103-0a81-4cc1-8e8c-226d40f546cf",
   "metadata": {
    "id": "31b9f103-0a81-4cc1-8e8c-226d40f546cf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe3037f-3941-4da6-87ff-c5fffc8ac6de",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947208,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "8fe3037f-3941-4da6-87ff-c5fffc8ac6de",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#combination c^2_N\n",
    "def generate(N):\n",
    "    col = []\n",
    "    col_rev = []\n",
    "    for i in range(1,N):\n",
    "        for j in range(0,i):\n",
    "            col.append([i,j])\n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98797082-b05f-4949-97e3-02b52bb03d79",
   "metadata": {
    "id": "98797082-b05f-4949-97e3-02b52bb03d79",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51408d77-3d75-4d0c-b64e-850a12d322e1",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947208,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "51408d77-3d75-4d0c-b64e-850a12d322e1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_corr(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    H = Matrix.shape[1]\n",
    "    num=generate(H)\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*len(num)*2*stride\n",
    "        data1 = tf.gather(Matrix,axis=1,indices=num)[:,:,:,start_index:end_index]\n",
    "        mean1 = tf.reduce_mean(data1, axis=-1)\n",
    "        spread1 = data1 - tf.expand_dims(mean1, axis=-1)\n",
    "        #N*len(num)*2*1\n",
    "        std=(tf.reduce_sum(spread1*spread1,axis=-1, keepdims=True)/(spread1.shape[-1]-1))**0.5\n",
    "        #N*len(num)*1\n",
    "        cov=tf.reduce_sum(tf.reduce_prod(spread1, axis=-2), axis=-1, keepdims=True)/(data1.shape[-1]-1)\n",
    "        corr=cov/tf.reduce_prod(std,axis=-2)\n",
    "\n",
    "        l.append(corr)\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*len(num)*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f42aa7-a80f-44fe-9186-fbb2f9f21b1b",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947208,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "36f42aa7-a80f-44fe-9186-fbb2f9f21b1b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_cov(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    H = Matrix.shape[1]\n",
    "    num=generate(H)\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*len(num)*2*stride\n",
    "        data1 = tf.gather(Matrix,axis=1,indices=num)[:,:,:,start_index:end_index]\n",
    "        mean1 = tf.reduce_mean(data1, axis=-1)\n",
    "        spread1 = data1 - tf.expand_dims(mean1, axis=-1)\n",
    "        #N*len(num)*1\n",
    "        cov=tf.reduce_sum(tf.reduce_prod(spread1, axis=-2), axis=-1, keepdims=True)/(data1.shape[-1]-1)\n",
    "\n",
    "        l.append(cov)\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*len(num)*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qI2v4XeD9qiK",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947208,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "qI2v4XeD9qiK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_stdde(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*stride\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        mean1 = tf.reduce_mean(data1, axis=-1)\n",
    "        spread1 = data1 - tf.expand_dims(mean1, axis=-1)\n",
    "        #N*H*1\n",
    "        std=(tf.reduce_sum(spread1*spread1,axis=-1, keepdims=True)/(spread1.shape[-1]-1))**0.5\n",
    "\n",
    "        l.append(std)\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "LjC9Q6Wf9qiK",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947209,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "LjC9Q6Wf9qiK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_zscore(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*stride\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        mean1 = tf.reduce_mean(data1, axis=-1)\n",
    "        spread1 = data1 - tf.expand_dims(mean1, axis=-1)\n",
    "        #N*H*1\n",
    "        std=(tf.reduce_sum(spread1*spread1,axis=-1, keepdims=True)/(spread1.shape[-1]-1))**0.5\n",
    "\n",
    "        l.append(tf.expand_dims(mean1, axis=-1)/std)\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1xqDKFSK9qiL",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947209,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "1xqDKFSK9qiL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_return(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*1\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        return_ = (data1[:,:,-1]-data1[:,:,0])/(data1[:,:,0]+0.0001)-1\n",
    "        l.append(tf.expand_dims(return_, axis=-1))\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2CvdvxDV9qiL",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274947209,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "2CvdvxDV9qiL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_decaylinear(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*stride\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        range_ = end_index-start_index\n",
    "        weight = tf.range(1,range_+1,dtype=Matrix.dtype)\n",
    "        weight = weight/tf.reduce_sum(weight)\n",
    "        #N*H*1\n",
    "        wd = tf.reduce_sum(data1*weight, axis=-1, keepdims=True)\n",
    "        l.append(wd)\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "jpFyYUOZ9qiL",
   "metadata": {
    "executionInfo": {
     "elapsed": 1216,
     "status": "ok",
     "timestamp": 1666274948422,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "jpFyYUOZ9qiL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_min(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*stride\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        #N*H*1\n",
    "        l.append(tf.reduce_min(data1,axis=-1, keepdims=True))\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vLzWttfT9qiM",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666274948423,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "vLzWttfT9qiM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_max(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*stride\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        #N*H*1\n",
    "        l.append(tf.reduce_max(data1,axis=-1, keepdims=True))\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Yq9APQKB9qiM",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274948423,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "Yq9APQKB9qiM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ts_mean(Matrix,stride):\n",
    "#Matrix: tensorflow(none, 9, 30)\n",
    "    W = Matrix.shape[2]\n",
    "    if W % stride == 0:\n",
    "        Index_list = list(np.arange(0,W+stride,stride))\n",
    "    else:\n",
    "        mod = W % stride\n",
    "        Index_list = list(np.arange(0,W+stride-mod,stride)) + [W]\n",
    "    l = []\n",
    "    for i in range(len(Index_list)-1):\n",
    "        start_index = Index_list[i]\n",
    "        end_index = Index_list[i+1]\n",
    "        #N*H*stride\n",
    "        data1 = Matrix[:,:,start_index:end_index]\n",
    "        #N*H*1\n",
    "        l.append(tf.reduce_mean(data1,axis=-1, keepdims=True))\n",
    "\n",
    "    return tf.concat(l,axis=-1) #N*H*[W/H]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7lv5tBBV9qiM",
   "metadata": {
    "collapsed": false,
    "id": "7lv5tBBV9qiM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.1 check the output of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "HhoWAOWE9qiM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666274948424,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "HhoWAOWE9qiM",
    "outputId": "6da160ed-fa8c-4587-d707-ab10f73217b3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mean = [-10, 30]\n",
    "cov = [[10, 20], [20, 1000]]\n",
    "x, y = np.random.multivariate_normal(mean, cov, 5000).T\n",
    "plt.plot(x, y, 'x')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "JoMmFYsY9qiM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274948424,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "JoMmFYsY9qiM",
    "outputId": "b5d36d87-2b96-4bde-fe86-690691e4d64f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 30000), dtype=float64, numpy=\narray([[[-11.18240286, -17.74601551,  -6.54080062, ...,  -8.55160593,\n         -11.51659436, -12.47792104],\n        [ 15.91239754,  82.74482125,   4.70732421, ...,  64.45100921,\n          56.63076325,  31.75182717]],\n\n       [[ -8.14956777, -10.82795638,  -6.63719604, ..., -10.76424745,\n         -11.70877459,  -7.76526079],\n        [ 25.95617354,  26.00493679, -14.5020123 , ...,  49.37203763,\n          18.50112257,  28.97797769]]])>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just for test, so the dimension may be different with real dataset\n",
    "testdata=tf.transpose(tf.constant(np.random.multivariate_normal(mean, cov, [2,30000])), perm=[0, 2, 1])\n",
    "testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "t8H8J8NO9qiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274948424,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "t8H8J8NO9qiN",
    "outputId": "5d6386d6-18ed-486c-f977-991679db727f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 1, 3), dtype=float64, numpy=\narray([[[0.19906125, 0.20219223, 0.19397767]],\n\n       [[0.2001684 , 0.22042309, 0.21381659]]])>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20/(10*1000)**0.5=0.2\n",
    "ts_corr(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "uLUulnfU9qiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274948424,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "uLUulnfU9qiN",
    "outputId": "c1b41baa-7b62-4316-a424-a13316874fa0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 1, 3), dtype=float64, numpy=\narray([[[20.06829444, 20.35868711, 19.35579165]],\n\n       [[19.84256935, 21.98970477, 21.55615482]]])>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20\n",
    "ts_cov(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "VAZ3yh4e9qiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1029,
     "status": "ok",
     "timestamp": 1666274949450,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "VAZ3yh4e9qiN",
    "outputId": "9d4936a3-bb78-46d5-8866-e12a539b00d1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[ 3.16523411,  3.15699721,  3.17516074],\n        [31.85061989, 31.8941558 , 31.4263167 ]],\n\n       [[ 3.12423721,  3.15467105,  3.17621881],\n        [31.72914563, 31.62337587, 31.74091718]]])>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_std=10**0.5 about 3.162;y_std=1000**0.5\n",
    "ts_stdde(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ntGpHdeF9qiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274949450,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "ntGpHdeF9qiN",
    "outputId": "7f121e5b-ddc3-427d-8bdb-da50436238a4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[-3.17533774, -3.15570119, -3.14069164],\n        [ 0.9273378 ,  0.94700484,  0.94998259]],\n\n       [[-3.16951267, -3.18050082, -3.14441519],\n        [ 0.94910031,  0.93577855,  0.93045445]]])>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_mean/x_std=-10**0.5 ; y_mean/y_std=0.3*10**0.5\n",
    "ts_zscore(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "x0dSeqrb9qiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274949450,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "x0dSeqrb9qiN",
    "outputId": "96ce5bd7-f633-40fd-9d7b-11e14fa613f5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[-1.30825024, -1.12685326, -0.72879839],\n        [-1.46919113, -1.02974489, 10.40487921]],\n\n       [[-1.04668909, -1.13390735, -0.77702219],\n        [-1.26372655, -0.89248795,  0.2404006 ]]])>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_return(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9GLIPEiU9qiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274949450,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "9GLIPEiU9qiN",
    "outputId": "cf914e80-f84f-4241-849b-d87fe23fc25c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[-10.07600858,  -9.96362526,  -9.99118054],\n        [ 29.66341746,  30.02132221,  29.7922632 ]],\n\n       [[ -9.88056744,  -9.9939446 ,  -9.98645649],\n        [ 30.08603853,  29.57510023,  29.61294574]]])>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_decaylinear(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "zTrKR_7-9qiO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274949451,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "zTrKR_7-9qiO",
    "outputId": "cbe9b968-09dc-4293-a23f-6abc3f99efed",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[ -21.13574431,  -23.3451375 ,  -22.8980446 ],\n        [ -96.64006209,  -91.23486106,  -79.22608756]],\n\n       [[ -21.17863965,  -22.62715335,  -21.67435755],\n        [ -90.55962898,  -97.34829822, -100.9165887 ]]])>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_min(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "PtqAumv-9qiO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274949451,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "PtqAumv-9qiO",
    "outputId": "4f980e54-4456-4f2a-e49d-3dad4e3b75cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[  2.18811185,   2.75352056,   1.49296952],\n        [147.92230849, 156.06440295, 146.05235107]],\n\n       [[  2.4847805 ,   1.16270509,   0.79359983],\n        [159.81475624, 139.53203518, 150.31945649]]])>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_max(testdata,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "lF7GJrp39qiO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274949451,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "lF7GJrp39qiO",
    "outputId": "41485dde-38d9-4730-e5ec-c0fc2cff7aa7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float64, numpy=\narray([[[-10.05068734,  -9.96253983,  -9.97220078],\n        [ 29.53628393,  30.20391988,  29.85445361]],\n\n       [[ -9.90230943, -10.03343386,  -9.98735067],\n        [ 30.11414204,  29.5924769 ,  29.53347777]]])>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_mean(testdata,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gX7BY77z9qiO",
   "metadata": {
    "collapsed": false,
    "id": "gX7BY77z9qiO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From the results above, the dimensions of outputs and statistics properties are right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PbUAsSs89qiO",
   "metadata": {
    "collapsed": false,
    "id": "PbUAsSs89qiO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. block construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab53c7b3-b0a1-4404-ade6-1d3f51b3aac3",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274949451,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "ab53c7b3-b0a1-4404-ade6-1d3f51b3aac3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class alphanet_feature_Block(tf.keras.Model):\n",
    "    def __init__(self, stride):\n",
    "        super(alphanet_feature_Block, self).__init__(name='')\n",
    "        #stride: tensorflow/array(2,)\n",
    "        self.stride_feature, self.stride_pool = stride\n",
    "\n",
    "        #pooling layers\n",
    "        self.max_pool = tf.keras.layers.MaxPooling2D(pool_size=(1, 3),strides=(1, self.stride_pool), padding='valid')\n",
    "        self.mean_pool = tf.keras.layers.AveragePooling2D(pool_size=(1, 3),strides=(1, self.stride_pool), padding='valid')\n",
    "        #different in input and output\n",
    "        self.min_pool = tf.keras.layers.MaxPooling2D(pool_size=(1, 3),strides=(1, self.stride_pool), padding='valid')\n",
    "\n",
    "        #batchnormal for feature input\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn5 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn6 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn7 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        #batchnormal for pooling layer\n",
    "        self.bn1mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn4mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn5mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn6mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn7mean = tf.keras.layers.BatchNormalization()\n",
    "        self.bn1max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn4max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn5max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn6max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn7max = tf.keras.layers.BatchNormalization()\n",
    "        self.bn1min = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2min = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3min = tf.keras.layers.BatchNormalization()\n",
    "        self.bn4min = tf.keras.layers.BatchNormalization()\n",
    "        self.bn5min = tf.keras.layers.BatchNormalization()\n",
    "        self.bn6min = tf.keras.layers.BatchNormalization()\n",
    "        self.bn7min = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "\n",
    "\n",
    "    #output: N*H*[W/H]*1, the last is the number of channel\n",
    "    def ts_corr(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_corr(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_cov(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_cov(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_stdde(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_stdde(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_zscore(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_zscore(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_return(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_return(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_decaylinear(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_decaylinear(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    #below functions could be replaced with pooling layers\n",
    "    def ts_min(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_min(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_max(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_max(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "    def ts_mean(self, input_tensor, stride):\n",
    "        return tf.expand_dims(ts_mean(input_tensor, stride),axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        #input_tensor tensorflow:(N,9,30)\n",
    "        #feature layer\n",
    "        x1 = self.ts_corr(input_tensor,self.stride_feature)\n",
    "        x1 = self.bn1(x1, training=training)\n",
    "        x1f = tf.keras.layers.Flatten()(x1)\n",
    "\n",
    "        x2 = self.ts_cov(input_tensor,self.stride_feature)\n",
    "        x2 = self.bn2(x2, training=training)\n",
    "        x2f = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "        x3 = self.ts_stdde(input_tensor,self.stride_feature)\n",
    "        x3 = self.bn3(x3, training=training)\n",
    "        x3f = tf.keras.layers.Flatten()(x3)\n",
    "\n",
    "        x4 = self.ts_zscore(input_tensor,self.stride_feature)\n",
    "        x4 = self.bn4(x4, training=training)\n",
    "        x4f = tf.keras.layers.Flatten()(x4)\n",
    "\n",
    "        x5 = self.ts_return(input_tensor,self.stride_feature)\n",
    "        x5 = self.bn5(x5, training=training)\n",
    "        x5f = tf.keras.layers.Flatten()(x5)\n",
    "\n",
    "        x6 = self.ts_decaylinear(input_tensor,self.stride_feature)\n",
    "        x6 = self.bn6(x6, training=training)\n",
    "        x6f = tf.keras.layers.Flatten()(x6)\n",
    "\n",
    "        x7 = self.ts_mean(input_tensor,self.stride_feature)\n",
    "        x7 = self.bn7(x7, training=training)\n",
    "        x7f = tf.keras.layers.Flatten()(x7)\n",
    "\n",
    "        #feature output\n",
    "        feature_output = tf.concat([x1f, x2f, x3f, x4f, x5f, x6f, x7f],axis=-1)\n",
    "\n",
    "        #pooling layer\n",
    "        x1mean = self.mean_pool(x1)\n",
    "        x1mean = self.bn1mean(x1mean, training=training)\n",
    "        x1max = self.max_pool(x1)\n",
    "        x1max = self.bn1max(x1max, training=training)\n",
    "        x1min = -self.min_pool(-x1)\n",
    "        x1min = self.bn1min(x1min, training=training)\n",
    "        x1p = tf.concat([tf.keras.layers.Flatten()(x1mean),tf.keras.layers.Flatten()(x1max),tf.keras.layers.Flatten()(x1min)],axis=-1)\n",
    "\n",
    "        x2mean = self.mean_pool(x2)\n",
    "        x2mean = self.bn2mean(x2mean, training=training)\n",
    "        x2max = self.max_pool(x2)\n",
    "        x2max = self.bn2max(x2max, training=training)\n",
    "        x2min = -self.min_pool(-x2)\n",
    "        x2min = self.bn2min(x2min, training=training)\n",
    "        x2p = tf.concat([tf.keras.layers.Flatten()(x2mean),tf.keras.layers.Flatten()(x2max),tf.keras.layers.Flatten()(x2min)],axis=-1)\n",
    "\n",
    "        x3mean = self.mean_pool(x3)\n",
    "        x3mean = self.bn3mean(x3mean, training=training)\n",
    "        x3max = self.max_pool(x3)\n",
    "        x3max = self.bn3max(x3max, training=training)\n",
    "        x3min = -self.min_pool(-x3)\n",
    "        x3min = self.bn3min(x3min, training=training)\n",
    "        x3p = tf.concat([tf.keras.layers.Flatten()(x3mean),tf.keras.layers.Flatten()(x3max),tf.keras.layers.Flatten()(x3min)],axis=-1)\n",
    "\n",
    "        x4mean = self.mean_pool(x4)\n",
    "        x4mean = self.bn4mean(x4mean, training=training)\n",
    "        x4max = self.max_pool(x4)\n",
    "        x4max = self.bn4max(x4max, training=training)\n",
    "        x4min = -self.min_pool(-x4)\n",
    "        x4min = self.bn4min(x4min, training=training)\n",
    "        x4p = tf.concat([tf.keras.layers.Flatten()(x4mean),tf.keras.layers.Flatten()(x4max),tf.keras.layers.Flatten()(x4min)],axis=-1)\n",
    "\n",
    "        x5mean = self.mean_pool(x5)\n",
    "        x5mean = self.bn5mean(x5mean, training=training)\n",
    "        x5max = self.max_pool(x5)\n",
    "        x5max = self.bn5max(x5max, training=training)\n",
    "        x5min = -self.min_pool(-x5)\n",
    "        x5min = self.bn5min(x5min, training=training)\n",
    "        x5p = tf.concat([tf.keras.layers.Flatten()(x5mean),tf.keras.layers.Flatten()(x5max),tf.keras.layers.Flatten()(x5min)],axis=-1)\n",
    "\n",
    "        x6mean = self.mean_pool(x6)\n",
    "        x6mean = self.bn6mean(x6mean, training=training)\n",
    "        x6max = self.max_pool(x6)\n",
    "        x6max = self.bn6max(x6max, training=training)\n",
    "        x6min = -self.min_pool(-x6)\n",
    "        x6min = self.bn6min(x6min, training=training)\n",
    "        x6p = tf.concat([tf.keras.layers.Flatten()(x6mean),tf.keras.layers.Flatten()(x6max),tf.keras.layers.Flatten()(x6min)],axis=-1)\n",
    "\n",
    "        x7mean = self.mean_pool(x7)\n",
    "        x7mean = self.bn7mean(x7mean, training=training)\n",
    "        x7max = self.max_pool(x7)\n",
    "        x7max = self.bn7max(x7max, training=training)\n",
    "        x7min = -self.min_pool(-x7)\n",
    "        x7min = self.bn7min(x7min, training=training)\n",
    "        x7p = tf.concat([tf.keras.layers.Flatten()(x7mean),tf.keras.layers.Flatten()(x7max),tf.keras.layers.Flatten()(x7min)],axis=-1)\n",
    "\n",
    "        #pooling output\n",
    "        pooling_output = tf.concat([x1p,x2p,x3p,x4p,x5p,x6p,x7p],axis=-1)\n",
    "\n",
    "        #output\n",
    "        output = tf.concat([feature_output,pooling_output],axis=-1)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "G6q2NjOZ-tMl",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274949451,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "G6q2NjOZ-tMl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_AlphaNet(feature_block, input_shape=(9, 30), output_shape=10, name=\"AlphaNet\", dropout_seed=None, version=1):\n",
    "    r\"\"\"\n",
    "    Build AlphaNet model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_block: alphanet_feature_Block\n",
    "        Feature extraction model.\n",
    "    input_shape: int or tuple. optional, default=(9, 30)\n",
    "        Input data shape. (NOT include batch size)\n",
    "    output_shape: int, optional, default=10\n",
    "        Output data shape, i.e. the length of predicted days.\n",
    "    name: str, optional, default=\"AlphaNet\"\n",
    "        Model name.\n",
    "    dropout_seed: None or int, optional, default=None\n",
    "        Random seed in \"Dropout\" layer.\n",
    "    version: {1, 2, 3}, optional, default=1\n",
    "        Version of AlphaNet. Details can be found in papers. Not supported for 2/3.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: tf.keras.Model\n",
    "        AlphaNet model.\n",
    "    \"\"\"\n",
    "    # input\n",
    "    inputs = keras.layers.Input(shape=input_shape)\n",
    "    # feature layers\n",
    "    x = feature_block(inputs)\n",
    "    # the rest\n",
    "    if version == 1:   # AlphaNet-v1\n",
    "        x = keras.layers.Dropout(rate=0.5, seed=dropout_seed, name=\"Dropout1\")(x)\n",
    "        #x = keras.layers.Dense(30, activation=\"relu\", kernel_initializer=init, name=\"Dense\")(x)\n",
    "        #sigmoid\n",
    "        x = keras.layers.Dense(512, activation=\"sigmoid\", kernel_initializer=keras.initializers.TruncatedNormal(mean=0., stddev=1.), name=\"Dense1\")(x)\n",
    "        x = keras.layers.Dense(256, activation=\"sigmoid\", kernel_initializer=keras.initializers.TruncatedNormal(mean=0., stddev=1.), name=\"Dense2\")(x)\n",
    "        x = keras.layers.Dense(128, activation=\"sigmoid\", kernel_initializer=keras.initializers.TruncatedNormal(mean=0., stddev=1.), name=\"Dense3\")(x)\n",
    "        x = keras.layers.Dense(64, activation=\"sigmoid\", kernel_initializer=keras.initializers.TruncatedNormal(mean=0., stddev=1.), name=\"Dense4\")(x)\n",
    "        x = keras.layers.Dense(32, activation=\"sigmoid\", kernel_initializer=keras.initializers.TruncatedNormal(mean=0., stddev=1.), name=\"Dense5\")(x)\n",
    "    # elif version == 2:  # AlphaNet-v2\n",
    "    #     x = keras.layers.LSTM(30, activation=\"relu\", name=\"LSTM\")(x)\n",
    "    #     x = keras.layers.BatchNormalization(name=\"BN\")(x)\n",
    "    # elif version == 3:  # AlphaNet-v3\n",
    "    #     x = keras.layers.GRU(30, activation=\"relu\", name=\"GRU\")(x)\n",
    "    #     x = keras.layers.BatchNormalization(name=\"BN\")(x)\n",
    "    outputs = keras.layers.Dense(output_shape, activation=\"linear\", kernel_initializer=keras.initializers.TruncatedNormal(mean=0., stddev=1.), name=\"Output\")(x)\n",
    "\n",
    "    # create model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=name) \n",
    "\n",
    "    # complie setting\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    # if version == 1:\n",
    "    #     optimizer = keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "    # else:\n",
    "    #     optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    loss = keras.losses.MeanSquaredError()\n",
    "    metrics = [keras.metrics.MeanSquaredError(name=\"MSE\")]\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    # return\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cyfLh4DI-5it",
   "metadata": {
    "id": "cyfLh4DI-5it",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aoZ-Hz8g9qiP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274949451,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "aoZ-Hz8g9qiP",
    "outputId": "03faa590-37e4-446b-be63-f8ae14f77f75",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([100, 9, 30])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = np.arange(1,10)\n",
    "cov = np.eye(9)*[2,3,6,7,2,9,3,5,3]\n",
    "testdata2 = tf.transpose(tf.constant(np.random.multivariate_normal(mean, cov, [100,30])), perm=[0, 2, 1])\n",
    "testdata2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1abu7ZcF9qiP",
   "metadata": {
    "executionInfo": {
     "elapsed": 4013,
     "status": "ok",
     "timestamp": 1666274953461,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "1abu7ZcF9qiP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "testb_feature = alphanet_feature_Block([10,3])\n",
    "testb = build_AlphaNet(testb_feature, dropout_seed=123, version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3WS8Pv9s9qiP",
   "metadata": {
    "executionInfo": {
     "elapsed": 1358,
     "status": "ok",
     "timestamp": 1666274954817,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "3WS8Pv9s9qiP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "modelb = testb(testdata2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "PyAXAuSu9qiP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1666274954818,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "PyAXAuSu9qiP",
    "outputId": "e5ef364c-5d18-4b21-f42b-6b930af187f3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(100, 10), dtype=float32, numpy=\narray([[ 4.0447397 ,  3.3324912 ,  2.5471873 , -1.559852  , -2.9185395 ,\n         3.6389112 ,  0.46532598,  7.0248694 ,  3.8541083 , -2.1867986 ],\n       [ 3.8741598 ,  3.2112913 ,  2.83692   , -1.5079187 , -3.0008867 ,\n         4.2955804 , -0.15021192,  6.9417186 ,  4.2584085 , -2.4458117 ],\n       [ 3.512124  ,  4.234714  ,  0.99257356, -1.1853831 , -3.2140348 ,\n         4.8284855 ,  0.7883116 ,  5.2397604 ,  3.4935327 , -2.219681  ],\n       [ 4.400022  ,  4.152371  ,  2.06964   , -0.3009731 , -1.8431295 ,\n         3.1497207 ,  0.32771868,  6.445024  ,  3.3590097 , -0.5889912 ],\n       [ 3.128948  ,  4.664479  ,  1.2586424 , -1.9594759 , -4.0213413 ,\n         4.836536  ,  1.139286  ,  5.242202  ,  3.4617429 , -2.549184  ],\n       [ 3.6756055 ,  4.954755  ,  0.46935534, -1.3256266 , -2.9980316 ,\n         4.612325  ,  1.1823455 ,  4.9171753 ,  2.732041  , -3.2126577 ],\n       [ 4.3134184 ,  4.4841304 ,  1.0121956 , -1.532807  , -1.8967206 ,\n         4.4175215 ,  1.0621027 ,  5.1246233 ,  3.012574  , -2.673615  ],\n       [ 4.01357   ,  3.688134  ,  1.5945263 , -1.3229892 , -3.261699  ,\n         4.0888176 ,  0.18389793,  6.2270384 ,  3.657367  , -2.180595  ],\n       [ 3.1428473 ,  6.1108136 ,  2.9935095 , -2.237197  , -3.1592908 ,\n         1.9885056 ,  0.13747305,  6.5534487 ,  3.9463174 , -2.1977496 ],\n       [ 3.5723753 ,  3.7906635 ,  3.668255  , -0.8237183 , -2.1073973 ,\n         3.2986753 , -0.17667927,  6.9360895 ,  5.280274  , -1.325344  ],\n       [ 3.483988  ,  4.3281603 ,  1.1615593 , -2.0232263 , -2.5044892 ,\n         3.2607331 ,  0.78661317,  5.377925  ,  3.3234982 , -2.749847  ],\n       [ 3.3423667 ,  5.164693  ,  1.1170807 , -1.1787269 , -3.5674603 ,\n         4.619541  ,  1.010062  ,  5.243163  ,  3.138453  , -2.8259552 ],\n       [ 3.7222507 ,  4.4473276 ,  2.1996672 , -1.4389169 , -3.7133718 ,\n         4.235347  , -0.05390958,  6.46798   ,  3.649557  , -3.6198711 ],\n       [ 2.8616602 ,  3.8153636 ,  1.3999287 , -1.6715509 , -3.74215   ,\n         4.6893673 ,  1.0109487 ,  5.02525   ,  2.9992404 , -2.4649718 ],\n       [ 3.291667  ,  4.1626654 ,  1.8400223 , -2.1837015 , -3.9562445 ,\n         4.4015775 ,  0.9594172 ,  5.8229156 ,  3.79521   , -2.5703225 ],\n       [ 3.8786283 ,  4.4970202 ,  1.6956952 , -1.3513521 , -3.533805  ,\n         4.226397  ,  1.0726731 ,  5.5992613 ,  3.8691394 , -2.6502252 ],\n       [ 4.079286  ,  3.1569355 ,  1.8562355 , -1.0107654 , -2.1849031 ,\n         3.9035695 ,  0.32481402,  5.7990303 ,  3.2080717 , -2.7076502 ],\n       [ 4.871507  ,  4.699853  ,  1.710998  , -0.6036015 , -3.0644834 ,\n         4.269399  ,  0.49855196,  5.99873   ,  3.4762936 , -2.4993577 ],\n       [ 3.0890434 ,  4.041781  ,  1.159594  , -1.5599713 , -3.3227158 ,\n         4.4364505 ,  1.0319883 ,  4.994183  ,  2.8899896 , -3.3565383 ],\n       [ 3.956451  ,  4.1115627 ,  1.8677331 , -1.580296  , -3.711353  ,\n         4.168014  ,  0.33659613,  6.0896187 ,  3.4018455 , -2.692349  ],\n       [ 3.8763952 ,  3.7561197 ,  1.7638224 , -1.487235  , -2.6159782 ,\n         4.8814764 ,  0.40663537,  5.860724  ,  3.8931699 , -2.6107802 ],\n       [ 3.5384777 ,  4.266957  ,  1.4929339 , -0.924586  , -3.236449  ,\n         4.7091017 ,  0.77710056,  5.1172833 ,  2.5808387 , -3.2364204 ],\n       [ 3.043009  ,  4.4050694 ,  0.13587907, -1.6586491 , -2.673215  ,\n         4.379181  ,  1.6185337 ,  4.491464  ,  2.6747348 , -3.1916065 ],\n       [ 4.697377  ,  4.4623857 ,  2.418124  , -0.9250858 , -3.1853042 ,\n         3.576922  ,  0.56708515,  6.062403  ,  4.08122   , -2.090979  ],\n       [ 4.3013916 ,  5.5787296 ,  2.1979768 ,  0.0785293 , -2.4419665 ,\n         3.0653827 , -0.20517264,  6.3928943 ,  3.1491363 , -1.9047117 ],\n       [ 3.6571584 ,  5.6617923 ,  0.905693  , -0.81184113, -2.9773166 ,\n         4.234452  ,  1.1394825 ,  5.075049  ,  3.1351855 , -2.519522  ],\n       [ 4.5500083 ,  3.7405605 ,  2.5504324 , -1.3560581 , -3.5730867 ,\n         3.039932  , -1.0119929 ,  7.2393537 ,  4.251212  , -1.6118159 ],\n       [ 4.774954  ,  4.715755  ,  1.7916616 , -1.8997339 , -4.2012177 ,\n         4.3258038 , -0.01151583,  6.5070853 ,  3.7436898 , -2.502274  ],\n       [ 4.488596  ,  4.055545  ,  1.1927177 , -1.0510267 , -1.7725214 ,\n         4.3228345 ,  0.79312855,  5.229204  ,  3.1398437 , -2.655699  ],\n       [ 4.1184077 ,  4.745577  ,  0.8618179 , -0.887218  , -2.690756  ,\n         4.239665  ,  0.8672757 ,  5.494326  ,  3.634051  , -2.3952308 ],\n       [ 4.1746216 ,  3.2851267 ,  3.5043378 , -1.4769229 , -3.4025922 ,\n         4.2426095 , -0.5273536 ,  7.514077  ,  4.6641355 , -2.360284  ],\n       [ 3.7277858 ,  4.674189  ,  1.5081179 , -0.66054124, -3.0322795 ,\n         4.650375  ,  0.78725976,  5.217488  ,  2.5083    , -4.07604   ],\n       [ 3.1657338 ,  4.2671914 ,  2.2045403 , -0.5540915 , -2.7562158 ,\n         3.853984  , -0.00925082,  6.4591846 ,  3.129224  , -2.7278595 ],\n       [ 4.1338162 ,  4.401744  ,  3.5892663 , -2.7759929 , -4.169119  ,\n         2.5121663 , -1.0403993 ,  7.4512715 ,  5.213025  , -3.9113464 ],\n       [ 3.6239028 ,  4.325505  ,  1.477336  , -1.2064922 , -2.8304315 ,\n         4.8466377 ,  0.9904542 ,  5.5050845 ,  3.8087745 , -2.1351976 ],\n       [ 4.582106  ,  4.752075  ,  1.1722009 , -1.012914  , -2.9372747 ,\n         4.752442  ,  0.6828361 ,  5.1784987 ,  3.0662804 , -2.5441284 ],\n       [ 4.248013  ,  4.576482  ,  2.097411  , -2.1820993 , -3.1786437 ,\n         4.1645827 ,  0.11196139,  6.650185  ,  4.5069566 , -2.2729788 ],\n       [ 3.883804  ,  3.7998266 ,  0.9993253 , -1.3812757 , -2.7199798 ,\n         4.4834747 ,  1.0379579 ,  5.078328  ,  2.775423  , -3.0143273 ],\n       [ 3.67728   ,  3.654982  ,  1.420808  , -0.63928986, -2.2867212 ,\n         4.611975  ,  0.9506188 ,  5.1288095 ,  2.7232058 , -3.906768  ],\n       [ 2.5263445 ,  3.7850106 ,  2.5744214 , -0.3643352 , -2.2689762 ,\n         5.3082767 ,  0.37905654,  5.19527   ,  3.3847744 , -3.8505273 ],\n       [ 3.715761  ,  2.8176565 ,  1.9493895 , -2.9740798 , -2.557764  ,\n         4.323168  ,  0.425518  ,  6.3439755 ,  3.5178764 , -2.7688413 ],\n       [ 4.6079965 ,  2.7869694 ,  3.1061742 , -1.3686035 , -2.4444723 ,\n         3.864776  , -0.40640262,  7.0498157 ,  3.580221  , -3.055345  ],\n       [ 4.0108094 ,  4.522387  ,  1.3029879 , -0.8254631 , -3.5156193 ,\n         4.834315  ,  0.85682714,  5.0517507 ,  2.715283  , -3.4235992 ],\n       [ 3.55236   ,  5.756173  ,  0.8612737 , -1.0059693 , -3.2902071 ,\n         5.066244  ,  1.4656433 ,  4.596285  ,  3.730128  , -2.2851756 ],\n       [ 4.3341746 ,  4.8661594 ,  1.2830166 , -1.2043313 , -2.9914227 ,\n         3.5100524 ,  0.68009186,  5.847914  ,  3.4890604 , -2.2881618 ],\n       [ 3.5416605 ,  4.165712  ,  1.1515739 , -1.3171002 , -2.9219685 ,\n         4.263259  ,  0.69979006,  5.383572  ,  2.606299  , -3.2356327 ],\n       [ 4.107863  ,  5.5295677 ,  2.069644  , -1.8179914 , -2.5520809 ,\n         3.7066226 ,  1.2013664 ,  5.614911  ,  3.7696404 , -4.1105633 ],\n       [ 3.4314048 ,  4.3534703 ,  1.4375193 , -0.9671956 , -3.2159183 ,\n         4.875033  ,  1.1023064 ,  5.416863  ,  3.8810594 , -2.7265794 ],\n       [ 3.913307  ,  4.0948906 ,  1.5776743 , -1.0120442 , -2.8960626 ,\n         3.807485  ,  0.32479873,  5.7446313 ,  3.5682478 , -2.6155403 ],\n       [ 3.6920805 ,  4.2278595 ,  2.4776437 , -1.305913  , -3.1689618 ,\n         4.088145  , -0.60873544,  6.950335  ,  4.345727  , -2.6326163 ],\n       [ 3.7599275 ,  3.1650476 ,  2.5768607 , -1.319558  , -3.2522109 ,\n         4.143826  , -0.0968814 ,  6.860495  ,  3.8978634 , -2.8127465 ],\n       [ 4.0439663 ,  5.6071405 ,  2.0850737 , -0.68214524, -2.353082  ,\n         3.786064  ,  1.2748227 ,  5.392213  ,  4.1498923 , -1.9505829 ],\n       [ 4.0174403 ,  4.8810897 ,  1.2101932 , -0.8318124 , -3.7843156 ,\n         4.5781655 ,  0.48118976,  5.18405   ,  2.5999596 , -2.960601  ],\n       [ 4.000536  ,  5.0141473 ,  1.8408314 , -0.6913517 , -4.036068  ,\n         5.2497582 ,  0.14403173,  5.876665  ,  3.1126885 , -3.816307  ],\n       [ 4.000666  ,  4.0855465 ,  2.4296966 , -1.0340365 , -2.1355822 ,\n         2.8345726 ,  0.503683  ,  5.687834  ,  4.2557635 , -1.8793566 ],\n       [ 4.4757686 ,  4.844524  ,  3.382306  , -0.463187  , -3.1558151 ,\n         3.4555652 ,  0.02537573,  6.2808943 ,  3.5136902 , -1.9175159 ],\n       [ 4.6616054 ,  3.6977959 ,  2.1451852 , -1.3394165 , -3.0820794 ,\n         4.40595   , -0.1693945 ,  6.429926  ,  3.5583117 , -2.9195998 ],\n       [ 4.1071625 ,  5.51376   ,  0.63963616, -0.94167835, -1.156544  ,\n         3.6847386 ,  1.2022918 ,  5.4097056 ,  4.3522887 , -0.09164834],\n       [ 3.6862998 ,  4.5388656 ,  2.126717  , -1.1618928 , -3.9302924 ,\n         4.3611073 ,  0.5227167 ,  5.8505764 ,  3.849287  , -2.4968681 ],\n       [ 3.275891  ,  4.3295774 ,  1.4913527 , -1.4843768 , -3.1444445 ,\n         3.8516338 ,  0.8546773 ,  5.8769264 ,  4.133471  , -1.7776493 ],\n       [ 3.8715208 ,  3.9262118 ,  1.5688685 , -1.4891579 , -3.5669754 ,\n         3.9624555 ,  0.56499314,  5.9548492 ,  3.6665883 , -2.153639  ],\n       [ 4.775701  ,  4.691871  ,  1.6092154 , -1.4528191 , -3.5540013 ,\n         3.950971  , -0.15875155,  6.3364725 ,  3.4575775 , -2.6298707 ],\n       [ 3.423205  ,  4.242621  ,  1.3914473 , -0.83152676, -3.0178254 ,\n         4.8208265 ,  0.6246957 ,  5.222065  ,  2.2947876 , -3.6676579 ],\n       [ 4.7815194 ,  5.1347766 ,  2.4332724 , -0.8774146 , -3.2217891 ,\n         3.4332151 ,  0.21382804,  6.563966  ,  4.423135  , -2.011051  ],\n       [ 3.3530903 ,  4.801877  ,  0.94000083, -1.2774241 , -2.7404237 ,\n         4.186043  ,  1.6852088 ,  4.8934565 ,  3.589848  , -2.5221136 ],\n       [ 4.5302753 ,  4.7003965 ,  1.4048098 , -1.6063356 , -3.919827  ,\n         4.197872  ,  0.40488365,  5.8554006 ,  3.3080313 , -2.3988066 ],\n       [ 4.1829786 ,  5.01126   ,  2.092182  , -0.6153868 , -2.5288024 ,\n         4.284117  ,  0.69447786,  5.31312   ,  3.3771715 , -3.103753  ],\n       [ 4.0691333 ,  4.8601103 ,  1.2818024 , -1.4356025 , -3.25483   ,\n         4.03908   ,  0.57039976,  5.9628916 ,  3.8736284 , -2.7134006 ],\n       [ 4.2036633 ,  4.907406  ,  1.5224913 , -0.95062035, -2.780344  ,\n         5.2439747 ,  0.08419979,  5.789327  ,  3.3661256 , -2.8760297 ],\n       [ 3.3636148 ,  4.251638  ,  2.0845637 , -1.6497275 , -2.7723396 ,\n         5.1273527 ,  0.7099851 ,  5.732317  ,  3.5880125 , -3.3836467 ],\n       [ 4.470087  ,  4.6171894 ,  1.7166892 , -1.1916782 , -3.2927    ,\n         4.5433292 ,  0.58982587,  5.9046264 ,  3.6654997 , -2.5585346 ],\n       [ 3.6489727 ,  3.93231   ,  2.55034   , -1.1202393 , -3.6076353 ,\n         4.1567717 , -0.08757254,  6.643497  ,  4.116047  , -2.2039511 ],\n       [ 4.109413  ,  5.0145917 ,  1.2668924 , -0.43265185, -2.833534  ,\n         5.1014724 ,  0.6877367 ,  5.5128045 ,  3.08373   , -3.5967042 ],\n       [ 4.192111  ,  3.662669  ,  1.5578176 , -1.5249851 , -3.0812962 ,\n         4.720578  ,  0.3356086 ,  5.910651  ,  3.206359  , -3.0508468 ],\n       [ 4.1397433 ,  4.3524413 ,  1.1169405 , -1.3693162 , -3.3290968 ,\n         4.7892947 ,  0.5090709 ,  5.5905533 ,  2.957462  , -3.1703527 ],\n       [ 3.3153853 ,  5.0612416 ,  1.3895649 , -0.91373956, -2.6657407 ,\n         4.3784    ,  0.7811623 ,  5.257824  ,  3.2384493 , -3.291433  ],\n       [ 3.888358  ,  5.229105  ,  0.8947246 , -0.867363  , -2.5999515 ,\n         5.169663  ,  0.58318037,  4.6781297 ,  2.8996859 , -3.0144358 ],\n       [ 3.7377348 ,  4.8056984 ,  1.915948  , -1.1614884 , -3.285522  ,\n         4.4574924 ,  0.2046871 ,  5.80078   ,  2.5903535 , -4.5216765 ],\n       [ 4.0653725 ,  4.2037554 ,  0.6743497 , -1.35869   , -2.7680461 ,\n         4.739087  ,  1.0357307 ,  5.0020514 ,  2.7146807 , -3.0001364 ],\n       [ 4.2350206 ,  4.415901  ,  0.8897317 , -0.65094745, -2.322033  ,\n         4.3916597 ,  1.1226995 ,  4.6444716 ,  2.5865397 , -3.1403701 ],\n       [ 3.6271765 ,  3.802812  ,  1.7285705 , -1.0699687 , -2.5444725 ,\n         4.0045485 ,  0.3858595 ,  5.3166747 ,  3.2138498 , -2.4822955 ],\n       [ 3.7900403 ,  4.1982436 ,  2.4108715 , -1.0326433 , -3.275989  ,\n         4.4067984 , -0.09092771,  6.20265   ,  3.8192873 , -2.5196025 ],\n       [ 3.9583035 ,  3.9375963 ,  2.2472522 , -1.4235406 , -2.7051802 ,\n         3.5556223 , -0.03885488,  6.5887694 ,  3.7849102 , -3.4167027 ],\n       [ 3.3681302 ,  4.2027893 ,  2.686061  , -1.9178559 , -4.097547  ,\n         3.6688616 , -0.46816343,  7.045186  ,  4.593194  , -2.4519317 ],\n       [ 3.5994167 ,  4.5494995 ,  0.7789494 , -1.1907012 , -3.3377054 ,\n         4.986942  ,  1.1786481 ,  4.9289403 ,  2.5888424 , -3.3391623 ],\n       [ 4.8977733 ,  3.8528917 ,  3.1485078 , -1.0037706 , -3.3956726 ,\n         3.741123  , -0.08493723,  6.835437  ,  4.0937653 , -2.8508794 ],\n       [ 4.3139386 ,  3.6378462 ,  1.5714415 , -0.99270356, -1.0541307 ,\n         4.1565514 , -0.21831457,  6.2775    ,  3.5392742 , -1.7994251 ],\n       [ 4.109075  ,  4.0025253 ,  2.103309  , -1.5505506 , -3.4050994 ,\n         3.5997758 , -0.5670019 ,  6.9294486 ,  3.89603   , -2.5702014 ],\n       [ 3.3606505 ,  3.4525232 ,  1.3815475 , -1.4338377 , -2.898681  ,\n         4.359664  ,  0.53489095,  5.518283  ,  3.1394367 , -2.6495473 ],\n       [ 4.4123874 ,  4.871165  ,  1.9408047 , -1.7432766 , -3.4809232 ,\n         4.8846397 ,  0.12130586,  6.246761  ,  4.114184  , -1.4542838 ],\n       [ 3.2816682 ,  4.6111364 ,  1.5632877 , -1.5819929 , -2.395323  ,\n         3.7026632 ,  1.7712685 ,  4.699393  ,  3.2171512 , -3.6529036 ],\n       [ 2.6764367 ,  3.4671578 ,  2.8700218 , -2.466184  , -3.1700032 ,\n         4.2417755 ,  0.45990148,  5.9907045 ,  4.0342917 , -2.1669765 ],\n       [ 3.8829145 ,  3.8504515 ,  1.0277877 , -1.0597454 , -1.8947673 ,\n         4.480398  ,  0.9496172 ,  4.911325  ,  3.1095426 , -2.6905346 ],\n       [ 4.259699  ,  4.5151377 ,  1.3208296 , -1.6702977 , -3.048377  ,\n         2.981734  ,  0.20525323,  6.3006024 ,  3.7551312 , -1.9787142 ],\n       [ 3.4286067 ,  3.6415658 ,  1.4096115 , -1.4172262 , -3.1816015 ,\n         4.612262  ,  0.7669044 ,  5.43977   ,  3.2833872 , -2.6890996 ],\n       [ 4.4562488 ,  4.6302934 ,  1.4870592 , -0.8390906 , -3.2321637 ,\n         4.675434  ,  0.3900913 ,  6.072311  ,  3.5135808 , -2.5104873 ],\n       [ 4.273132  ,  4.016428  ,  1.5963508 , -0.9436236 , -2.6844404 ,\n         4.1290917 ,  0.22660884,  6.202409  ,  3.573373  , -2.359373  ],\n       [ 3.661351  ,  4.430346  ,  2.3433878 , -0.5876856 , -3.0336227 ,\n         4.9308295 ,  0.11252983,  6.076273  ,  3.5932808 , -3.0755363 ],\n       [ 3.123881  ,  3.8864408 ,  1.333381  , -1.3775965 , -2.7736626 ,\n         4.364804  ,  1.1028261 ,  5.0569563 ,  3.0335774 , -3.4987626 ],\n       [ 4.275021  ,  4.4867268 ,  1.7415068 , -1.2013346 , -3.020522  ,\n         4.3345823 , -0.15891255,  6.3069696 ,  3.49903   , -2.6854975 ]],\n      dtype=float32)>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6pwvwqWl9qiP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666274954818,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "6pwvwqWl9qiP",
    "outputId": "586b9725-4108-446c-e550-44545eae8a6d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[<keras.engine.input_layer.InputLayer at 0x23ce1996700>,\n <__main__.alphanet_feature_Block at 0x23ce1996910>,\n <keras.layers.core.Dropout at 0x23c82105a90>,\n <keras.layers.core.Dense at 0x23c822a7d90>,\n <keras.layers.core.Dense at 0x23c894cbf70>,\n <keras.layers.core.Dense at 0x23c894bc550>,\n <keras.layers.core.Dense at 0x23c894f9fa0>,\n <keras.layers.core.Dense at 0x23c822dcd00>,\n <keras.layers.core.Dense at 0x23c8acad280>]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testb.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "s376ihCV9qiP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666274954818,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "s376ihCV9qiP",
    "outputId": "d53bf44f-04be-4f31-e233-e8f3798a5b17",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# 7+7*3+3=31\n",
    "print(len(testb_feature.layers))\n",
    "# input + feature + dropout + dense * 2\n",
    "print(len(testb.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "PujjexyA9qiP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666274954818,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "PujjexyA9qiP",
    "outputId": "64163e95-1e8b-48fd-eb4e-29efc3b61ae3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AlphaNet\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 9, 30)]           0         \n",
      "_________________________________________________________________\n",
      "alphanet_feature__block (alp (None, 702)               112       \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 702)               0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 512)               359936    \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "Dense3 (Dense)               (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "Dense4 (Dense)               (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "Dense5 (Dense)               (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 534,938\n",
      "Trainable params: 534,882\n",
      "Non-trainable params: 56\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "testb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40Kdtn069qiP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666274954818,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "40Kdtn069qiP",
    "outputId": "41744c04-2656-4700-ca0c-bb7d075704e4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Variable 'alphanet_feature__block/batch_normalization/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_1/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_1/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_1/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_1/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_2/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_2/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_2/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_2/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_3/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_3/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_3/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_3/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_4/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_4/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_4/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_4/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_5/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_5/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_5/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_5/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_6/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_6/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_6/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_6/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_7/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_7/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_7/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_7/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_8/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_8/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_8/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_8/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_9/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_9/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_9/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_9/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_10/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_10/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_10/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_10/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_11/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_11/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_11/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_11/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_12/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_12/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_12/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_12/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_13/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_13/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_13/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_13/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_14/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_14/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_14/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_14/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_15/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_15/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_15/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_15/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_16/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_16/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_16/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_16/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_17/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_17/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_17/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_17/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_18/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_18/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_18/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_18/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_19/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_19/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_19/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_19/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_20/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_20/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_20/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_20/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_21/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_21/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_21/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_21/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_22/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_22/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_22/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_22/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_23/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_23/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_23/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_23/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_24/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_24/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_24/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_24/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_25/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_25/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_25/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_25/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_26/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_26/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_26/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_26/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_27/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_27/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_27/moving_mean:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n <tf.Variable 'alphanet_feature__block/batch_normalization_27/moving_variance:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>,\n <tf.Variable 'Dense1/kernel:0' shape=(702, 512) dtype=float32, numpy=\n array([[-0.03637793, -1.6647227 , -1.7416564 , ..., -0.4034153 ,\n          0.07842974, -1.439343  ],\n        [-0.15560521,  0.2915717 , -0.36048332, ..., -0.7854359 ,\n         -0.79654557,  0.41204685],\n        [-0.71040326, -0.47615623, -0.7410497 , ...,  0.71128714,\n          0.40994284,  0.33603284],\n        ...,\n        [-0.66458917,  1.6661855 , -0.84182113, ..., -1.114838  ,\n          0.6879816 ,  1.4942335 ],\n        [ 0.32074913, -0.11259004,  0.7698426 , ...,  0.73483545,\n         -1.3719126 , -0.725982  ],\n        [-0.69387513,  0.0488148 , -0.22628032, ...,  0.03945242,\n          0.717237  ,  0.12716362]], dtype=float32)>,\n <tf.Variable 'Dense1/bias:0' shape=(512,) dtype=float32, numpy=\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.], dtype=float32)>,\n <tf.Variable 'Dense2/kernel:0' shape=(512, 256) dtype=float32, numpy=\n array([[-1.0039177 ,  0.5457302 , -0.48755002, ..., -0.57885194,\n         -0.46422407,  0.06939782],\n        [-0.58298045,  0.12919828, -1.2816497 , ..., -0.5535967 ,\n         -0.25331575, -0.03627823],\n        [-0.4247929 , -0.5433134 , -0.8059991 , ..., -1.6443591 ,\n         -0.40221173, -0.17863332],\n        ...,\n        [-0.536061  ,  0.16723162, -1.1258087 , ..., -0.12579985,\n         -1.4087939 , -1.9190663 ],\n        [ 0.3692017 ,  1.3493788 ,  1.2061036 , ...,  0.00925607,\n          1.4873812 ,  0.2785195 ],\n        [-1.1960796 ,  0.49819183,  1.3874264 , ...,  0.4658919 ,\n         -0.6699342 , -0.47292456]], dtype=float32)>,\n <tf.Variable 'Dense2/bias:0' shape=(256,) dtype=float32, numpy=\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.], dtype=float32)>,\n <tf.Variable 'Dense3/kernel:0' shape=(256, 128) dtype=float32, numpy=\n array([[ 0.9932575 ,  0.5633959 ,  0.52584535, ..., -0.5728405 ,\n         -0.7123292 , -0.90196604],\n        [-0.10207921,  0.29992127,  1.5203898 , ...,  0.5652761 ,\n          0.86094433, -0.18215981],\n        [ 1.125581  , -0.11777473, -1.8024142 , ...,  0.6834168 ,\n          0.03028778,  0.122443  ],\n        ...,\n        [-0.5655748 ,  0.29425076,  0.10371364, ..., -1.2567779 ,\n          1.3282444 ,  0.27211076],\n        [ 0.33371428, -0.7602737 ,  1.1680174 , ...,  0.8074897 ,\n         -0.8135777 ,  1.0694354 ],\n        [-0.5551773 , -0.2204653 , -0.27980018, ..., -0.90370756,\n         -0.9439334 ,  0.40419045]], dtype=float32)>,\n <tf.Variable 'Dense3/bias:0' shape=(128,) dtype=float32, numpy=\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n <tf.Variable 'Dense4/kernel:0' shape=(128, 64) dtype=float32, numpy=\n array([[-1.0477935 ,  0.29631767,  1.3019253 , ..., -0.28516433,\n          0.17695077, -0.6108744 ],\n        [-0.28618273, -0.7132569 ,  0.0727119 , ...,  0.10141283,\n         -0.18389776,  1.5341817 ],\n        [ 0.23218441, -0.00845608,  0.8782524 , ...,  0.5937684 ,\n         -0.3471438 ,  0.5595813 ],\n        ...,\n        [ 1.5350748 , -0.66457206, -0.29109433, ..., -0.82232267,\n          0.19439563, -0.3079565 ],\n        [-0.42513573,  1.8637565 , -0.8360832 , ..., -0.53690803,\n         -0.60801274,  0.31453866],\n        [-0.7977591 , -0.7430318 ,  0.96167004, ..., -0.28661272,\n         -0.05040472, -1.2571939 ]], dtype=float32)>,\n <tf.Variable 'Dense4/bias:0' shape=(64,) dtype=float32, numpy=\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n <tf.Variable 'Dense5/kernel:0' shape=(64, 32) dtype=float32, numpy=\n array([[-0.6365469 , -0.7428032 , -1.0918874 , ..., -0.4308323 ,\n          0.84030604,  0.9871693 ],\n        [-0.25812268,  1.2772523 , -0.4950337 , ...,  0.50887764,\n          1.2780706 ,  0.6814245 ],\n        [ 0.6216633 ,  0.49113598,  0.8295262 , ...,  0.47933805,\n          1.4376473 , -0.15294713],\n        ...,\n        [ 0.24770351,  1.4110307 , -0.07949525, ...,  1.7790205 ,\n         -1.5023385 ,  0.31040385],\n        [-0.9659841 , -1.3263774 ,  0.28783444, ...,  1.1305925 ,\n         -0.752554  ,  0.08128349],\n        [-0.92216945,  1.1788071 ,  0.16266324, ..., -0.6794087 ,\n         -0.15845826, -1.5071739 ]], dtype=float32)>,\n <tf.Variable 'Dense5/bias:0' shape=(32,) dtype=float32, numpy=\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       dtype=float32)>,\n <tf.Variable 'Output/kernel:0' shape=(32, 10) dtype=float32, numpy=\n array([[ 1.58069670e+00, -6.18712842e-01, -2.02855244e-01,\n          7.55792260e-01,  4.63961631e-01,  3.18895400e-01,\n         -5.98333955e-01,  4.61478144e-01, -7.50838399e-01,\n         -4.05456126e-01],\n        [-1.04189634e+00, -7.39130795e-01,  1.24473798e+00,\n         -5.06529331e-01, -8.16082895e-01,  4.22409892e-01,\n          1.47658920e+00, -5.04658878e-01,  1.00749373e+00,\n          3.17645222e-02],\n        [ 4.49891269e-01, -1.56463969e+00,  1.53101909e+00,\n         -5.54514885e-01, -1.06615472e+00, -7.06188798e-01,\n         -1.58938110e+00,  1.43106782e+00,  3.87293547e-01,\n          1.60837293e+00],\n        [ 2.88600504e-01, -2.72052675e-01,  8.80755484e-01,\n         -4.34387743e-01,  1.03454733e+00, -2.34708846e-01,\n         -6.44471467e-01,  9.91642356e-01,  1.63094386e-01,\n         -7.53623903e-01],\n        [ 1.11055565e+00, -8.99158895e-01, -3.44753087e-01,\n         -1.72979212e+00,  5.80228865e-01,  6.82769239e-01,\n          3.01687833e-04,  5.28582513e-01,  1.39450729e+00,\n         -1.42091012e+00],\n        [-1.02676725e+00, -1.20435572e+00,  3.35880280e-01,\n         -7.98418999e-01,  2.42473528e-01, -1.26178527e+00,\n         -1.02479768e+00,  1.68807447e-01, -6.76195174e-02,\n         -1.58097482e+00],\n        [ 2.44282454e-01,  1.76803875e+00, -1.51075184e+00,\n          4.11718667e-01,  3.37600932e-02,  1.19801080e+00,\n         -3.13594013e-01, -3.36093426e-01,  1.35206687e+00,\n         -3.96428049e-01],\n        [-7.01445118e-02,  6.45833910e-02, -2.74289072e-01,\n          1.99838722e+00, -1.34312868e-01, -3.52779359e-01,\n         -4.46910381e-01, -7.14108467e-01, -8.54991615e-01,\n          2.50315815e-01],\n        [-2.10551023e-01, -1.69415855e+00, -2.60812044e-01,\n          8.05657983e-01,  3.71268064e-01,  2.80569464e-01,\n          1.85241401e-01, -1.26836741e+00, -3.70878100e-01,\n          4.08243001e-01],\n        [-7.70695731e-02, -3.15935105e-01, -1.47753924e-01,\n          9.93258119e-01,  1.42437744e+00, -1.20218837e+00,\n          3.72923732e-01,  1.19140625e+00, -9.53721464e-01,\n          1.03249300e+00],\n        [-4.71239984e-01, -6.79568470e-01, -3.98396514e-02,\n          2.89708912e-01, -1.65082800e+00, -2.45563343e-01,\n          1.07532382e+00, -1.29926950e-01, -9.50540364e-01,\n         -1.84152305e+00],\n        [ 3.93518060e-02, -3.10061723e-01, -1.37596929e+00,\n         -6.46200538e-01,  8.49201858e-01, -7.29978800e-01,\n          1.00014424e+00, -4.57430154e-01,  3.69545698e-01,\n          1.55668533e+00],\n        [-1.24594879e+00,  5.24118960e-01, -7.83556402e-01,\n         -3.78872633e-01, -8.69516730e-01,  1.34923553e+00,\n         -4.03932750e-01,  4.07930911e-01, -7.09672749e-01,\n          1.15633346e-01],\n        [ 5.09261847e-01,  8.81597161e-01,  3.93658549e-01,\n          1.47519529e+00,  1.42237377e+00, -1.23731112e+00,\n         -8.13456059e-01,  1.54935622e+00, -1.62665749e+00,\n         -1.15667015e-01],\n        [ 1.31470963e-01,  3.26064318e-01, -5.53399146e-01,\n          8.75847399e-01, -2.36305043e-01,  7.94304669e-01,\n          3.30398977e-01, -7.08580613e-01, -2.35451490e-01,\n          4.57118005e-01],\n        [-5.18163800e-01,  3.92390609e-01,  7.46826112e-01,\n          9.49566901e-01, -1.02177560e+00,  7.34367430e-01,\n          9.18261111e-01,  4.16190863e-01,  1.30873239e+00,\n          8.27763796e-01],\n        [ 1.11403143e+00,  1.66113150e+00, -1.11273944e+00,\n          2.51083344e-01,  7.65945613e-01,  4.27334785e-01,\n         -2.88928658e-01,  1.72512603e+00, -9.85859632e-01,\n          2.25403428e-01],\n        [-1.02694643e+00,  1.84564805e+00, -6.27013206e-01,\n          5.37904799e-01,  4.62386131e-01, -1.12016046e+00,\n          1.32117093e+00, -1.75229624e-01,  2.74057060e-01,\n          1.06221747e+00],\n        [-1.16514957e+00,  1.11202765e+00,  2.92753696e-01,\n         -1.51156709e-01, -7.26570427e-01,  1.60171375e-01,\n          1.23010553e-01,  1.47589073e-01, -2.33449023e-02,\n          6.86596274e-01],\n        [ 1.63801336e+00, -6.28654063e-01, -1.52003133e+00,\n         -6.87909484e-01, -5.31748116e-01, -3.20944071e-01,\n          1.01138532e+00,  6.75756335e-01, -1.54995412e-01,\n          5.44583142e-01],\n        [ 9.46517944e-01, -4.35975224e-01,  1.91907597e+00,\n          7.96026886e-01,  1.03457916e+00, -1.28280425e+00,\n          2.90214092e-01, -2.63609052e-01,  6.08296216e-01,\n         -6.44579113e-01],\n        [ 4.65939254e-01,  1.33778918e+00, -1.43209130e-01,\n         -1.00966799e+00,  6.23407662e-02,  1.82098910e-01,\n         -2.45098546e-01,  3.59641701e-01,  8.20638895e-01,\n         -1.32572293e-01],\n        [ 3.57294828e-01,  1.97092545e+00, -1.17844664e-01,\n          7.38850117e-01, -1.31921327e+00,  1.25789702e+00,\n         -1.95407495e-02, -4.14225340e-01, -3.04650158e-01,\n         -4.81488526e-01],\n        [-1.01453650e+00,  2.08756849e-01, -2.10733041e-01,\n         -1.13650405e+00, -6.97167292e-02,  1.96699977e+00,\n         -2.87272274e-01, -7.91013479e-01, -3.53591204e-01,\n         -3.81070077e-02],\n        [ 3.20770532e-01,  5.44849098e-01,  6.46216691e-01,\n         -7.37970710e-01, -1.50696993e+00,  1.24635887e+00,\n         -2.23476902e-01,  2.34372392e-01,  1.95723569e+00,\n         -6.36964917e-01],\n        [ 4.83393520e-01,  5.89545131e-01,  6.61377728e-01,\n          1.19603582e-01, -1.32502198e+00,  2.57973149e-02,\n         -7.31228888e-01,  1.22359788e+00,  1.48787713e+00,\n          8.90707433e-01],\n        [-9.82192755e-01,  1.47871494e+00, -2.03613639e-01,\n         -1.11251736e+00, -6.62448168e-01, -1.60652554e+00,\n         -9.82682109e-01,  6.81547940e-01,  1.86885071e+00,\n         -2.79671788e-01],\n        [ 1.78225171e-02,  5.73664129e-01,  1.60833871e+00,\n          4.45432216e-01,  8.10189128e-01, -6.92851603e-01,\n          4.74668071e-02,  2.62750357e-01,  1.72266364e-01,\n          1.83964297e-01],\n        [ 1.26593792e+00,  2.74499983e-01,  2.92654723e-01,\n         -1.47628486e+00, -2.91049387e-02, -2.00522855e-01,\n          7.95155615e-02,  2.82960892e-01,  1.42733395e+00,\n         -1.91961944e+00],\n        [ 8.17824721e-01, -4.63499367e-01,  3.34712684e-01,\n         -7.53129900e-01,  6.86906636e-01,  8.68162394e-01,\n          3.34533662e-01,  1.91443539e+00, -6.33936882e-01,\n         -5.06941369e-03],\n        [ 6.53884232e-01,  1.62618327e+00, -3.29618663e-01,\n         -5.25967479e-01,  4.24037784e-01, -1.83178282e+00,\n         -4.80439067e-01,  4.83749717e-01,  6.57887876e-01,\n          8.86320770e-01],\n        [-1.37127316e+00, -1.04489338e+00,  1.05124366e+00,\n          8.23695183e-01,  1.47615182e+00,  8.92950356e-01,\n         -6.11209590e-03, -1.61270767e-01,  5.34887433e-01,\n          4.76406775e-02]], dtype=float32)>,\n <tf.Variable 'Output/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testb.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BFb1peMG9qiP",
   "metadata": {
    "id": "BFb1peMG9qiP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Real Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "Jnoq1AOlEwsk",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1666274954818,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "Jnoq1AOlEwsk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TRAIN_DIR = \"train_120\"\n",
    "VALID_DIR = \"valid_120\"\n",
    "SAVE_DIR = \"saved_model\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "def load_hdf5(file):\n",
    "    r\"\"\" \n",
    "    Load HDF5 data file and preprocess.\n",
    "    \"\"\"\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        X, y = f[\"X\"][...], f[\"y\"][...]\n",
    "    # preprocess: log(1+x)\n",
    "    X[5, :] = np.log(X[5, :] + 1)\n",
    "    X[6, :] = np.log(X[6, :] + 1)\n",
    "    y = np.log(y + 1)\n",
    "    return X, y\n",
    "\n",
    "def load_dataset(train_dir, valid_dir):\n",
    "    r\"\"\"\n",
    "    Load preprocessed datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_dir: str\n",
    "        Fold contains training datasets.\n",
    "    valid_dir: str\n",
    "        Fold contains test datasets.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    value: generator\n",
    "        A generator for tuples like (name, train_data, valid_data)\n",
    "    \"\"\"\n",
    "    \n",
    "    for file in os.listdir(train_dir):\n",
    "        name = file.split(\"_\")[0]\n",
    "        train_dt = tf.data.Dataset.from_tensor_slices(\n",
    "            load_hdf5(f\"{train_dir}/{name}_train.hdf5\")\n",
    "        )\n",
    "        valid_dt = tf.data.Dataset.from_tensor_slices(\n",
    "            load_hdf5(f\"{valid_dir}/{name}_valid.hdf5\")\n",
    "        )\n",
    "        yield (name, train_dt, valid_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "SKa09rxZJMSc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2087239,
     "status": "ok",
     "timestamp": 1666277042055,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "SKa09rxZJMSc",
    "outputId": "f38f6c16-4b03-4bfb-971d-1ef15ca056da",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 120)\n",
      "(1,)\n",
      "Epoch 1/100\n",
      "473/473 [==============================] - 7s 10ms/step - loss: 0.2655 - MSE: 0.2655 - val_loss: 0.0463 - val_MSE: 0.0463\n",
      "Epoch 2/100\n",
      "473/473 [==============================] - 4s 9ms/step - loss: 0.0295 - MSE: 0.0295 - val_loss: 0.0133 - val_MSE: 0.0133\n",
      "Epoch 3/100\n",
      "473/473 [==============================] - 4s 8ms/step - loss: 0.0117 - MSE: 0.0117 - val_loss: 0.0053 - val_MSE: 0.0053\n",
      "Epoch 4/100\n",
      "473/473 [==============================] - 4s 9ms/step - loss: 0.0057 - MSE: 0.0057 - val_loss: 0.0024 - val_MSE: 0.0024\n",
      "Epoch 5/100\n",
      "473/473 [==============================] - 4s 8ms/step - loss: 0.0031 - MSE: 0.0031 - val_loss: 0.0012 - val_MSE: 0.0012\n",
      "Epoch 6/100\n",
      "473/473 [==============================] - 4s 9ms/step - loss: 0.0018 - MSE: 0.0018 - val_loss: 7.2017e-04 - val_MSE: 7.2017e-04\n",
      "Epoch 7/100\n",
      "473/473 [==============================] - 4s 9ms/step - loss: 0.0012 - MSE: 0.0012 - val_loss: 4.3185e-04 - val_MSE: 4.3185e-04\n",
      "Epoch 8/100\n",
      "473/473 [==============================] - 4s 8ms/step - loss: 7.9126e-04 - MSE: 7.9126e-04 - val_loss: 2.9190e-04 - val_MSE: 2.9190e-04\n",
      "Epoch 9/100\n",
      "473/473 [==============================] - 4s 8ms/step - loss: 5.4872e-04 - MSE: 5.4872e-04 - val_loss: 2.1088e-04 - val_MSE: 2.1088e-04\n",
      "Epoch 10/100\n",
      "473/473 [==============================] - 4s 9ms/step - loss: 4.1145e-04 - MSE: 4.1145e-04 - val_loss: 1.5246e-04 - val_MSE: 1.5246e-04\n",
      "Epoch 11/100\n",
      "473/473 [==============================] - 11s 23ms/step - loss: 2.9658e-04 - MSE: 2.9658e-04 - val_loss: 1.2120e-04 - val_MSE: 1.2120e-04\n",
      "Epoch 12/100\n",
      "473/473 [==============================] - 17s 36ms/step - loss: 2.2324e-04 - MSE: 2.2324e-04 - val_loss: 8.6585e-05 - val_MSE: 8.6585e-05\n",
      "Epoch 13/100\n",
      "473/473 [==============================] - 30s 64ms/step - loss: 1.6676e-04 - MSE: 1.6676e-04 - val_loss: 6.7966e-05 - val_MSE: 6.7966e-05\n",
      "Epoch 14/100\n",
      "473/473 [==============================] - 26s 55ms/step - loss: 1.2806e-04 - MSE: 1.2806e-04 - val_loss: 5.5892e-05 - val_MSE: 5.5892e-05\n",
      "Epoch 15/100\n",
      "473/473 [==============================] - 27s 57ms/step - loss: 9.9582e-05 - MSE: 9.9582e-05 - val_loss: 4.1469e-05 - val_MSE: 4.1469e-05\n",
      "Epoch 16/100\n",
      "473/473 [==============================] - 21s 45ms/step - loss: 7.4675e-05 - MSE: 7.4675e-05 - val_loss: 2.8588e-05 - val_MSE: 2.8588e-05\n",
      "Epoch 17/100\n",
      "473/473 [==============================] - 38s 82ms/step - loss: 6.1640e-05 - MSE: 6.1640e-05 - val_loss: 2.2549e-05 - val_MSE: 2.2549e-05\n",
      "Epoch 18/100\n",
      "473/473 [==============================] - 26s 55ms/step - loss: 4.5568e-05 - MSE: 4.5568e-05 - val_loss: 1.7571e-05 - val_MSE: 1.7571e-05\n",
      "Epoch 19/100\n",
      "473/473 [==============================] - 22s 47ms/step - loss: 3.6657e-05 - MSE: 3.6657e-05 - val_loss: 1.5431e-05 - val_MSE: 1.5431e-05\n",
      "Epoch 20/100\n",
      "473/473 [==============================] - 24s 51ms/step - loss: 2.8040e-05 - MSE: 2.8040e-05 - val_loss: 1.2146e-05 - val_MSE: 1.2146e-05\n",
      "Epoch 21/100\n",
      "473/473 [==============================] - 31s 66ms/step - loss: 2.2901e-05 - MSE: 2.2901e-05 - val_loss: 9.7957e-06 - val_MSE: 9.7957e-06\n",
      "Epoch 22/100\n",
      "473/473 [==============================] - 23s 50ms/step - loss: 1.8712e-05 - MSE: 1.8712e-05 - val_loss: 9.7361e-06 - val_MSE: 9.7361e-06\n",
      "Epoch 23/100\n",
      "473/473 [==============================] - 33s 70ms/step - loss: 1.6672e-05 - MSE: 1.6672e-05 - val_loss: 7.9950e-06 - val_MSE: 7.9950e-06\n",
      "Epoch 24/100\n",
      "473/473 [==============================] - 23s 48ms/step - loss: 1.3385e-05 - MSE: 1.3385e-05 - val_loss: 8.4872e-06 - val_MSE: 8.4872e-06\n",
      "Epoch 25/100\n",
      "473/473 [==============================] - 26s 55ms/step - loss: 1.2212e-05 - MSE: 1.2212e-05 - val_loss: 6.3197e-06 - val_MSE: 6.3197e-06\n",
      "Epoch 26/100\n",
      "473/473 [==============================] - 34s 72ms/step - loss: 1.0038e-05 - MSE: 1.0038e-05 - val_loss: 5.7858e-06 - val_MSE: 5.7858e-06\n",
      "Epoch 27/100\n",
      "473/473 [==============================] - 40s 84ms/step - loss: 9.1178e-06 - MSE: 9.1178e-06 - val_loss: 5.9074e-06 - val_MSE: 5.9074e-06\n",
      "Epoch 28/100\n",
      "473/473 [==============================] - 40s 84ms/step - loss: 8.4274e-06 - MSE: 8.4274e-06 - val_loss: 5.4100e-06 - val_MSE: 5.4100e-06\n",
      "Epoch 29/100\n",
      "473/473 [==============================] - 43s 91ms/step - loss: 7.6593e-06 - MSE: 7.6593e-06 - val_loss: 5.7135e-06 - val_MSE: 5.7135e-06\n",
      "Epoch 30/100\n",
      "473/473 [==============================] - 35s 75ms/step - loss: 7.2793e-06 - MSE: 7.2793e-06 - val_loss: 5.0167e-06 - val_MSE: 5.0167e-06\n",
      "Epoch 31/100\n",
      "473/473 [==============================] - 35s 67ms/step - loss: 6.8252e-06 - MSE: 6.8252e-06 - val_loss: 5.0241e-06 - val_MSE: 5.0241e-06\n",
      "Epoch 32/100\n",
      "473/473 [==============================] - 26s 56ms/step - loss: 6.7150e-06 - MSE: 6.7150e-06 - val_loss: 4.9189e-06 - val_MSE: 4.9189e-06\n",
      "Epoch 33/100\n",
      "473/473 [==============================] - 27s 57ms/step - loss: 6.4085e-06 - MSE: 6.4085e-06 - val_loss: 4.8057e-06 - val_MSE: 4.8057e-06\n",
      "Epoch 34/100\n",
      "473/473 [==============================] - 19s 41ms/step - loss: 6.2412e-06 - MSE: 6.2412e-06 - val_loss: 5.1251e-06 - val_MSE: 5.1251e-06\n",
      "Epoch 35/100\n",
      "473/473 [==============================] - 23s 49ms/step - loss: 6.1411e-06 - MSE: 6.1411e-06 - val_loss: 4.8001e-06 - val_MSE: 4.8001e-06\n",
      "Epoch 36/100\n",
      "473/473 [==============================] - 19s 41ms/step - loss: 6.0503e-06 - MSE: 6.0503e-06 - val_loss: 4.7932e-06 - val_MSE: 4.7932e-06\n",
      "Epoch 37/100\n",
      "473/473 [==============================] - 20s 43ms/step - loss: 5.9768e-06 - MSE: 5.9768e-06 - val_loss: 4.6985e-06 - val_MSE: 4.6985e-06\n",
      "Epoch 38/100\n",
      "473/473 [==============================] - 23s 48ms/step - loss: 5.9107e-06 - MSE: 5.9107e-06 - val_loss: 4.6584e-06 - val_MSE: 4.6584e-06\n",
      "Epoch 39/100\n",
      "473/473 [==============================] - 23s 49ms/step - loss: 5.8584e-06 - MSE: 5.8584e-06 - val_loss: 4.6646e-06 - val_MSE: 4.6646e-06\n",
      "Epoch 40/100\n",
      "473/473 [==============================] - 19s 39ms/step - loss: 5.8176e-06 - MSE: 5.8176e-06 - val_loss: 4.6464e-06 - val_MSE: 4.6464e-06\n",
      "Epoch 41/100\n",
      "473/473 [==============================] - 19s 39ms/step - loss: 5.8060e-06 - MSE: 5.8060e-06 - val_loss: 4.6390e-06 - val_MSE: 4.6390e-06\n",
      "Epoch 42/100\n",
      "473/473 [==============================] - 23s 50ms/step - loss: 5.7900e-06 - MSE: 5.7900e-06 - val_loss: 4.6362e-06 - val_MSE: 4.6362e-06\n",
      "Epoch 43/100\n",
      "473/473 [==============================] - 19s 41ms/step - loss: 5.7324e-06 - MSE: 5.7324e-06 - val_loss: 4.6263e-06 - val_MSE: 4.6263e-06\n",
      "Epoch 44/100\n",
      "473/473 [==============================] - 19s 39ms/step - loss: 5.7413e-06 - MSE: 5.7413e-06 - val_loss: 4.6288e-06 - val_MSE: 4.6288e-06\n",
      "Epoch 45/100\n",
      "473/473 [==============================] - 22s 47ms/step - loss: 5.7528e-06 - MSE: 5.7528e-06 - val_loss: 4.6151e-06 - val_MSE: 4.6151e-06\n",
      "Epoch 46/100\n",
      "473/473 [==============================] - 29s 61ms/step - loss: 5.7049e-06 - MSE: 5.7049e-06 - val_loss: 4.6110e-06 - val_MSE: 4.6110e-06\n",
      "Epoch 47/100\n",
      "473/473 [==============================] - 32s 67ms/step - loss: 5.7209e-06 - MSE: 5.7209e-06 - val_loss: 4.6127e-06 - val_MSE: 4.6127e-06\n",
      "Epoch 48/100\n",
      "473/473 [==============================] - 19s 41ms/step - loss: 5.7089e-06 - MSE: 5.7089e-06 - val_loss: 4.6186e-06 - val_MSE: 4.6186e-06\n",
      "Epoch 49/100\n",
      "473/473 [==============================] - 18s 39ms/step - loss: 5.6873e-06 - MSE: 5.6873e-06 - val_loss: 4.6254e-06 - val_MSE: 4.6254e-06\n",
      "Epoch 50/100\n",
      "473/473 [==============================] - 23s 49ms/step - loss: 5.6812e-06 - MSE: 5.6812e-06 - val_loss: 4.6056e-06 - val_MSE: 4.6056e-06\n",
      "Epoch 51/100\n",
      "473/473 [==============================] - 32s 68ms/step - loss: 5.6801e-06 - MSE: 5.6801e-06 - val_loss: 4.6064e-06 - val_MSE: 4.6064e-06\n",
      "Epoch 52/100\n",
      "473/473 [==============================] - 26s 55ms/step - loss: 5.6871e-06 - MSE: 5.6871e-06 - val_loss: 4.6100e-06 - val_MSE: 4.6100e-06\n",
      "Epoch 53/100\n",
      "473/473 [==============================] - 32s 69ms/step - loss: 5.6958e-06 - MSE: 5.6958e-06 - val_loss: 4.6079e-06 - val_MSE: 4.6079e-06\n",
      "Epoch 54/100\n",
      "473/473 [==============================] - 23s 48ms/step - loss: 5.6863e-06 - MSE: 5.6863e-06 - val_loss: 4.6222e-06 - val_MSE: 4.6222e-06\n",
      "Epoch 55/100\n",
      "473/473 [==============================] - 23s 50ms/step - loss: 5.6573e-06 - MSE: 5.6573e-06 - val_loss: 4.6154e-06 - val_MSE: 4.6154e-06\n",
      "INFO:tensorflow:Assets written to: saved_model/AR_120_log_s302_sigmoid_4layers\\assets\n",
      "Dataset AR has been trained and saved.\n",
      "(7, 120)\n",
      "(1,)\n",
      "Epoch 1/100\n",
      "366/366 [==============================] - 19s 44ms/step - loss: 0.3974 - MSE: 0.3974 - val_loss: 0.0576 - val_MSE: 0.0576\n",
      "Epoch 2/100\n",
      "366/366 [==============================] - 16s 44ms/step - loss: 0.0524 - MSE: 0.0524 - val_loss: 0.0191 - val_MSE: 0.0191\n",
      "Epoch 3/100\n",
      "366/366 [==============================] - 12s 34ms/step - loss: 0.0203 - MSE: 0.0203 - val_loss: 0.0099 - val_MSE: 0.0099\n",
      "Epoch 4/100\n",
      "366/366 [==============================] - 12s 34ms/step - loss: 0.0113 - MSE: 0.0113 - val_loss: 0.0058 - val_MSE: 0.0058\n",
      "Epoch 5/100\n",
      "366/366 [==============================] - 16s 43ms/step - loss: 0.0066 - MSE: 0.0066 - val_loss: 0.0035 - val_MSE: 0.0035\n",
      "Epoch 6/100\n",
      "366/366 [==============================] - 13s 34ms/step - loss: 0.0042 - MSE: 0.0042 - val_loss: 0.0022 - val_MSE: 0.0022\n",
      "Epoch 7/100\n",
      "366/366 [==============================] - 16s 43ms/step - loss: 0.0028 - MSE: 0.0028 - val_loss: 0.0015 - val_MSE: 0.0015\n",
      "Epoch 8/100\n",
      "366/366 [==============================] - 16s 43ms/step - loss: 0.0019 - MSE: 0.0019 - val_loss: 0.0010 - val_MSE: 0.0010\n",
      "Epoch 9/100\n",
      "366/366 [==============================] - 16s 44ms/step - loss: 0.0014 - MSE: 0.0014 - val_loss: 7.0426e-04 - val_MSE: 7.0426e-04\n",
      "Epoch 10/100\n",
      "366/366 [==============================] - 16s 43ms/step - loss: 9.5847e-04 - MSE: 9.5847e-04 - val_loss: 4.9212e-04 - val_MSE: 4.9212e-04\n",
      "Epoch 11/100\n",
      "366/366 [==============================] - 13s 35ms/step - loss: 7.1957e-04 - MSE: 7.1957e-04 - val_loss: 3.4738e-04 - val_MSE: 3.4738e-04\n",
      "Epoch 12/100\n",
      "366/366 [==============================] - 16s 43ms/step - loss: 5.4900e-04 - MSE: 5.4900e-04 - val_loss: 2.4350e-04 - val_MSE: 2.4350e-04\n",
      "Epoch 13/100\n",
      "366/366 [==============================] - 16s 42ms/step - loss: 3.9528e-04 - MSE: 3.9528e-04 - val_loss: 1.7702e-04 - val_MSE: 1.7702e-04\n",
      "Epoch 14/100\n",
      "366/366 [==============================] - 12s 34ms/step - loss: 3.0101e-04 - MSE: 3.0101e-04 - val_loss: 1.3096e-04 - val_MSE: 1.3096e-04\n",
      "Epoch 15/100\n",
      "366/366 [==============================] - 12s 32ms/step - loss: 2.4146e-04 - MSE: 2.4146e-04 - val_loss: 9.5106e-05 - val_MSE: 9.5106e-05\n",
      "Epoch 16/100\n",
      "366/366 [==============================] - 8s 23ms/step - loss: 1.8653e-04 - MSE: 1.8653e-04 - val_loss: 7.3932e-05 - val_MSE: 7.3932e-05\n",
      "Epoch 17/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 1.3831e-04 - MSE: 1.3831e-04 - val_loss: 5.9625e-05 - val_MSE: 5.9625e-05\n",
      "Epoch 18/100\n",
      "366/366 [==============================] - 12s 32ms/step - loss: 1.0613e-04 - MSE: 1.0613e-04 - val_loss: 4.7268e-05 - val_MSE: 4.7268e-05\n",
      "Epoch 19/100\n",
      "366/366 [==============================] - 22s 59ms/step - loss: 8.9168e-05 - MSE: 8.9168e-05 - val_loss: 3.6632e-05 - val_MSE: 3.6632e-05\n",
      "Epoch 20/100\n",
      "366/366 [==============================] - 21s 59ms/step - loss: 6.9427e-05 - MSE: 6.9427e-05 - val_loss: 2.8587e-05 - val_MSE: 2.8587e-05\n",
      "Epoch 21/100\n",
      "366/366 [==============================] - 12s 34ms/step - loss: 5.5135e-05 - MSE: 5.5135e-05 - val_loss: 2.2354e-05 - val_MSE: 2.2354e-05\n",
      "Epoch 22/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 4.4321e-05 - MSE: 4.4321e-05 - val_loss: 1.9824e-05 - val_MSE: 1.9824e-05\n",
      "Epoch 23/100\n",
      "366/366 [==============================] - 15s 40ms/step - loss: 3.8820e-05 - MSE: 3.8820e-05 - val_loss: 1.4393e-05 - val_MSE: 1.4393e-05\n",
      "Epoch 24/100\n",
      "366/366 [==============================] - 16s 44ms/step - loss: 2.9641e-05 - MSE: 2.9641e-05 - val_loss: 1.2401e-05 - val_MSE: 1.2401e-05\n",
      "Epoch 25/100\n",
      "366/366 [==============================] - 15s 42ms/step - loss: 2.2660e-05 - MSE: 2.2660e-05 - val_loss: 9.7975e-06 - val_MSE: 9.7975e-06\n",
      "Epoch 26/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 1.9228e-05 - MSE: 1.9228e-05 - val_loss: 7.9410e-06 - val_MSE: 7.9410e-06\n",
      "Epoch 27/100\n",
      "366/366 [==============================] - 15s 42ms/step - loss: 1.5114e-05 - MSE: 1.5114e-05 - val_loss: 6.7663e-06 - val_MSE: 6.7663e-06\n",
      "Epoch 28/100\n",
      "366/366 [==============================] - 12s 33ms/step - loss: 1.3080e-05 - MSE: 1.3080e-05 - val_loss: 7.8755e-06 - val_MSE: 7.8755e-06\n",
      "Epoch 29/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 1.1122e-05 - MSE: 1.1122e-05 - val_loss: 4.8828e-06 - val_MSE: 4.8828e-06\n",
      "Epoch 30/100\n",
      "366/366 [==============================] - 14s 38ms/step - loss: 9.7482e-06 - MSE: 9.7482e-06 - val_loss: 3.9855e-06 - val_MSE: 3.9855e-06\n",
      "Epoch 31/100\n",
      "366/366 [==============================] - 11s 29ms/step - loss: 7.6376e-06 - MSE: 7.6376e-06 - val_loss: 3.3656e-06 - val_MSE: 3.3656e-06\n",
      "Epoch 32/100\n",
      "366/366 [==============================] - 14s 40ms/step - loss: 6.1704e-06 - MSE: 6.1704e-06 - val_loss: 3.0318e-06 - val_MSE: 3.0318e-06\n",
      "Epoch 33/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 5.4943e-06 - MSE: 5.4943e-06 - val_loss: 2.5019e-06 - val_MSE: 2.5019e-06\n",
      "Epoch 34/100\n",
      "366/366 [==============================] - 15s 42ms/step - loss: 5.1085e-06 - MSE: 5.1085e-06 - val_loss: 2.3821e-06 - val_MSE: 2.3821e-06\n",
      "Epoch 35/100\n",
      "366/366 [==============================] - 15s 41ms/step - loss: 3.8497e-06 - MSE: 3.8497e-06 - val_loss: 1.9703e-06 - val_MSE: 1.9703e-06\n",
      "Epoch 36/100\n",
      "366/366 [==============================] - 30s 82ms/step - loss: 3.4660e-06 - MSE: 3.4660e-06 - val_loss: 1.7350e-06 - val_MSE: 1.7350e-06\n",
      "Epoch 37/100\n",
      "366/366 [==============================] - 27s 74ms/step - loss: 3.2487e-06 - MSE: 3.2487e-06 - val_loss: 1.6390e-06 - val_MSE: 1.6390e-06\n",
      "Epoch 38/100\n",
      "366/366 [==============================] - 24s 66ms/step - loss: 2.9557e-06 - MSE: 2.9557e-06 - val_loss: 1.6706e-06 - val_MSE: 1.6706e-06\n",
      "Epoch 39/100\n",
      "366/366 [==============================] - 26s 71ms/step - loss: 2.6845e-06 - MSE: 2.6845e-06 - val_loss: 1.7045e-06 - val_MSE: 1.7045e-06\n",
      "Epoch 40/100\n",
      "366/366 [==============================] - 26s 72ms/step - loss: 2.3467e-06 - MSE: 2.3467e-06 - val_loss: 1.2272e-06 - val_MSE: 1.2272e-06\n",
      "Epoch 41/100\n",
      "366/366 [==============================] - 31s 86ms/step - loss: 2.2501e-06 - MSE: 2.2501e-06 - val_loss: 1.4034e-06 - val_MSE: 1.4034e-06\n",
      "Epoch 42/100\n",
      "366/366 [==============================] - 28s 77ms/step - loss: 2.0463e-06 - MSE: 2.0463e-06 - val_loss: 1.2030e-06 - val_MSE: 1.2030e-06\n",
      "Epoch 43/100\n",
      "366/366 [==============================] - 28s 78ms/step - loss: 1.8987e-06 - MSE: 1.8987e-06 - val_loss: 1.0853e-06 - val_MSE: 1.0853e-06\n",
      "Epoch 44/100\n",
      "366/366 [==============================] - 28s 77ms/step - loss: 1.8927e-06 - MSE: 1.8927e-06 - val_loss: 1.2485e-06 - val_MSE: 1.2485e-06\n",
      "Epoch 45/100\n",
      "366/366 [==============================] - 28s 76ms/step - loss: 1.8521e-06 - MSE: 1.8521e-06 - val_loss: 9.8853e-07 - val_MSE: 9.8853e-07\n",
      "Epoch 46/100\n",
      "366/366 [==============================] - 28s 77ms/step - loss: 1.6979e-06 - MSE: 1.6979e-06 - val_loss: 1.2513e-06 - val_MSE: 1.2513e-06\n",
      "Epoch 47/100\n",
      "366/366 [==============================] - 30s 81ms/step - loss: 1.6328e-06 - MSE: 1.6328e-06 - val_loss: 1.1495e-06 - val_MSE: 1.1495e-06\n",
      "Epoch 48/100\n",
      "366/366 [==============================] - 29s 78ms/step - loss: 1.6558e-06 - MSE: 1.6558e-06 - val_loss: 1.2917e-06 - val_MSE: 1.2917e-06\n",
      "Epoch 49/100\n",
      "366/366 [==============================] - 29s 78ms/step - loss: 1.5524e-06 - MSE: 1.5524e-06 - val_loss: 1.0803e-06 - val_MSE: 1.0803e-06\n",
      "Epoch 50/100\n",
      "366/366 [==============================] - 28s 77ms/step - loss: 1.5342e-06 - MSE: 1.5342e-06 - val_loss: 2.1492e-06 - val_MSE: 2.1492e-06\n",
      "INFO:tensorflow:Assets written to: saved_model/BAH_120_log_s302_sigmoid_4layers\\assets\n",
      "Dataset BAH has been trained and saved.\n",
      "(7, 120)\n",
      "(1,)\n",
      "Epoch 1/100\n",
      "460/460 [==============================] - 43s 86ms/step - loss: 3.4515 - MSE: 3.4515 - val_loss: 0.1593 - val_MSE: 0.1593\n",
      "Epoch 2/100\n",
      "460/460 [==============================] - 36s 78ms/step - loss: 0.1669 - MSE: 0.1669 - val_loss: 0.0710 - val_MSE: 0.0710\n",
      "Epoch 3/100\n",
      "460/460 [==============================] - 36s 78ms/step - loss: 0.0739 - MSE: 0.0739 - val_loss: 0.0256 - val_MSE: 0.0256\n",
      "Epoch 4/100\n",
      "460/460 [==============================] - 36s 78ms/step - loss: 0.0381 - MSE: 0.0381 - val_loss: 0.0158 - val_MSE: 0.0158\n",
      "Epoch 5/100\n",
      "460/460 [==============================] - 37s 80ms/step - loss: 0.0235 - MSE: 0.0235 - val_loss: 0.0092 - val_MSE: 0.0092\n",
      "Epoch 6/100\n",
      "460/460 [==============================] - 26s 56ms/step - loss: 0.0147 - MSE: 0.0147 - val_loss: 0.0051 - val_MSE: 0.0051\n",
      "Epoch 7/100\n",
      "460/460 [==============================] - 16s 35ms/step - loss: 0.0091 - MSE: 0.0091 - val_loss: 0.0028 - val_MSE: 0.0028\n",
      "Epoch 8/100\n",
      "460/460 [==============================] - 16s 36ms/step - loss: 0.0055 - MSE: 0.0055 - val_loss: 0.0016 - val_MSE: 0.0016\n",
      "Epoch 9/100\n",
      "460/460 [==============================] - 19s 35ms/step - loss: 0.0036 - MSE: 0.0036 - val_loss: 0.0010 - val_MSE: 0.0010\n",
      "Epoch 10/100\n",
      "460/460 [==============================] - 16s 35ms/step - loss: 0.0024 - MSE: 0.0024 - val_loss: 6.7241e-04 - val_MSE: 6.7241e-04\n",
      "Epoch 11/100\n",
      "460/460 [==============================] - 19s 42ms/step - loss: 0.0017 - MSE: 0.0017 - val_loss: 4.6397e-04 - val_MSE: 4.6397e-04\n",
      "Epoch 12/100\n",
      "460/460 [==============================] - 28s 61ms/step - loss: 0.0012 - MSE: 0.0012 - val_loss: 3.2841e-04 - val_MSE: 3.2841e-04\n",
      "Epoch 13/100\n",
      "460/460 [==============================] - 36s 78ms/step - loss: 8.8302e-04 - MSE: 8.8302e-04 - val_loss: 2.3437e-04 - val_MSE: 2.3437e-04\n",
      "Epoch 14/100\n",
      "460/460 [==============================] - 16s 35ms/step - loss: 6.3937e-04 - MSE: 6.3937e-04 - val_loss: 1.7623e-04 - val_MSE: 1.7623e-04\n",
      "Epoch 15/100\n",
      "460/460 [==============================] - 16s 35ms/step - loss: 4.7656e-04 - MSE: 4.7656e-04 - val_loss: 1.2729e-04 - val_MSE: 1.2729e-04\n",
      "Epoch 16/100\n",
      "460/460 [==============================] - 19s 42ms/step - loss: 3.6325e-04 - MSE: 3.6325e-04 - val_loss: 9.7476e-05 - val_MSE: 9.7476e-05\n",
      "Epoch 17/100\n",
      "460/460 [==============================] - 16s 35ms/step - loss: 2.6305e-04 - MSE: 2.6305e-04 - val_loss: 7.0510e-05 - val_MSE: 7.0510e-05\n",
      "Epoch 18/100\n",
      "460/460 [==============================] - 16s 35ms/step - loss: 2.0416e-04 - MSE: 2.0416e-04 - val_loss: 5.2891e-05 - val_MSE: 5.2891e-05\n",
      "Epoch 19/100\n",
      "460/460 [==============================] - 19s 42ms/step - loss: 1.5573e-04 - MSE: 1.5573e-04 - val_loss: 4.1261e-05 - val_MSE: 4.1261e-05\n",
      "Epoch 20/100\n",
      "460/460 [==============================] - 16s 36ms/step - loss: 1.1728e-04 - MSE: 1.1728e-04 - val_loss: 3.0724e-05 - val_MSE: 3.0724e-05\n",
      "Epoch 21/100\n",
      "460/460 [==============================] - 16s 35ms/step - loss: 9.5769e-05 - MSE: 9.5769e-05 - val_loss: 2.4054e-05 - val_MSE: 2.4054e-05\n",
      "Epoch 22/100\n",
      "460/460 [==============================] - 19s 42ms/step - loss: 6.8727e-05 - MSE: 6.8727e-05 - val_loss: 1.9581e-05 - val_MSE: 1.9581e-05\n",
      "Epoch 23/100\n",
      "460/460 [==============================] - 14s 31ms/step - loss: 5.8886e-05 - MSE: 5.8886e-05 - val_loss: 1.5984e-05 - val_MSE: 1.5984e-05\n",
      "Epoch 24/100\n",
      "460/460 [==============================] - 10s 22ms/step - loss: 4.4635e-05 - MSE: 4.4635e-05 - val_loss: 1.3345e-05 - val_MSE: 1.3345e-05\n",
      "Epoch 25/100\n",
      "460/460 [==============================] - 16s 34ms/step - loss: 3.5123e-05 - MSE: 3.5123e-05 - val_loss: 1.0991e-05 - val_MSE: 1.0991e-05\n",
      "Epoch 26/100\n",
      "460/460 [==============================] - 18s 40ms/step - loss: 3.0311e-05 - MSE: 3.0311e-05 - val_loss: 9.5448e-06 - val_MSE: 9.5448e-06\n",
      "Epoch 27/100\n",
      "460/460 [==============================] - 16s 34ms/step - loss: 2.3785e-05 - MSE: 2.3785e-05 - val_loss: 8.5628e-06 - val_MSE: 8.5628e-06\n",
      "Epoch 28/100\n",
      "460/460 [==============================] - 16s 34ms/step - loss: 2.0275e-05 - MSE: 2.0275e-05 - val_loss: 7.3692e-06 - val_MSE: 7.3692e-06\n",
      "Epoch 29/100\n",
      "460/460 [==============================] - 14s 30ms/step - loss: 1.6644e-05 - MSE: 1.6644e-05 - val_loss: 7.6459e-06 - val_MSE: 7.6459e-06\n",
      "Epoch 30/100\n",
      "460/460 [==============================] - 10s 23ms/step - loss: 1.4401e-05 - MSE: 1.4401e-05 - val_loss: 6.3610e-06 - val_MSE: 6.3610e-06\n",
      "Epoch 31/100\n",
      "460/460 [==============================] - 15s 33ms/step - loss: 1.3385e-05 - MSE: 1.3385e-05 - val_loss: 6.2389e-06 - val_MSE: 6.2389e-06\n",
      "Epoch 32/100\n",
      "460/460 [==============================] - 15s 33ms/step - loss: 1.1603e-05 - MSE: 1.1603e-05 - val_loss: 6.4273e-06 - val_MSE: 6.4273e-06\n",
      "Epoch 33/100\n",
      "460/460 [==============================] - 16s 34ms/step - loss: 1.0612e-05 - MSE: 1.0612e-05 - val_loss: 6.1605e-06 - val_MSE: 6.1605e-06\n",
      "Epoch 34/100\n",
      "460/460 [==============================] - 19s 41ms/step - loss: 9.6550e-06 - MSE: 9.6550e-06 - val_loss: 5.6284e-06 - val_MSE: 5.6284e-06\n",
      "Epoch 35/100\n",
      "460/460 [==============================] - 16s 34ms/step - loss: 9.1932e-06 - MSE: 9.1932e-06 - val_loss: 6.2737e-06 - val_MSE: 6.2737e-06\n",
      "Epoch 36/100\n",
      "460/460 [==============================] - 15s 33ms/step - loss: 8.5192e-06 - MSE: 8.5192e-06 - val_loss: 6.2440e-06 - val_MSE: 6.2440e-06\n",
      "Epoch 37/100\n",
      "460/460 [==============================] - 19s 40ms/step - loss: 8.4666e-06 - MSE: 8.4666e-06 - val_loss: 5.7208e-06 - val_MSE: 5.7208e-06\n",
      "Epoch 38/100\n",
      "460/460 [==============================] - 15s 32ms/step - loss: 7.7056e-06 - MSE: 7.7056e-06 - val_loss: 5.7576e-06 - val_MSE: 5.7576e-06\n",
      "Epoch 39/100\n",
      "460/460 [==============================] - 14s 32ms/step - loss: 7.5023e-06 - MSE: 7.5023e-06 - val_loss: 5.1158e-06 - val_MSE: 5.1158e-06\n",
      "Epoch 40/100\n",
      "460/460 [==============================] - 19s 42ms/step - loss: 7.4293e-06 - MSE: 7.4293e-06 - val_loss: 7.0669e-06 - val_MSE: 7.0669e-06\n",
      "Epoch 41/100\n",
      "460/460 [==============================] - 16s 35ms/step - loss: 7.2843e-06 - MSE: 7.2843e-06 - val_loss: 5.4415e-06 - val_MSE: 5.4415e-06\n",
      "Epoch 42/100\n",
      "460/460 [==============================] - 28s 61ms/step - loss: 7.1025e-06 - MSE: 7.1025e-06 - val_loss: 5.4650e-06 - val_MSE: 5.4650e-06\n",
      "Epoch 43/100\n",
      "460/460 [==============================] - 39s 85ms/step - loss: 7.0204e-06 - MSE: 7.0204e-06 - val_loss: 6.2197e-06 - val_MSE: 6.2197e-06\n",
      "Epoch 44/100\n",
      "460/460 [==============================] - 32s 70ms/step - loss: 6.9458e-06 - MSE: 6.9458e-06 - val_loss: 5.7186e-06 - val_MSE: 5.7186e-06\n",
      "INFO:tensorflow:Assets written to: saved_model/FTI_120_log_s302_sigmoid_4layers\\assets\n",
      "Dataset FTI has been trained and saved.\n",
      "(7, 120)\n",
      "(1,)\n",
      "Epoch 1/100\n",
      "228/228 [==============================] - 11s 36ms/step - loss: 0.7469 - MSE: 0.7469 - val_loss: 0.2263 - val_MSE: 0.2263\n",
      "Epoch 2/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 0.2039 - MSE: 0.2039 - val_loss: 0.0884 - val_MSE: 0.0884\n",
      "Epoch 3/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 0.0988 - MSE: 0.0988 - val_loss: 0.0456 - val_MSE: 0.0456\n",
      "Epoch 4/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 0.0530 - MSE: 0.0530 - val_loss: 0.0246 - val_MSE: 0.0246\n",
      "Epoch 5/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 0.0299 - MSE: 0.0299 - val_loss: 0.0138 - val_MSE: 0.0138\n",
      "Epoch 6/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 0.0188 - MSE: 0.0188 - val_loss: 0.0083 - val_MSE: 0.0083\n",
      "Epoch 7/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 0.0121 - MSE: 0.0121 - val_loss: 0.0054 - val_MSE: 0.0054\n",
      "Epoch 8/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 0.0087 - MSE: 0.0087 - val_loss: 0.0037 - val_MSE: 0.0037\n",
      "Epoch 9/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 0.0061 - MSE: 0.0061 - val_loss: 0.0028 - val_MSE: 0.0028\n",
      "Epoch 10/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 0.0047 - MSE: 0.0047 - val_loss: 0.0021 - val_MSE: 0.0021\n",
      "Epoch 11/100\n",
      "228/228 [==============================] - 11s 49ms/step - loss: 0.0037 - MSE: 0.0037 - val_loss: 0.0016 - val_MSE: 0.0016\n",
      "Epoch 12/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 0.0030 - MSE: 0.0030 - val_loss: 0.0012 - val_MSE: 0.0012\n",
      "Epoch 13/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 0.0024 - MSE: 0.0024 - val_loss: 9.8181e-04 - val_MSE: 9.8181e-04\n",
      "Epoch 14/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 0.0020 - MSE: 0.0020 - val_loss: 7.9478e-04 - val_MSE: 7.9478e-04\n",
      "Epoch 15/100\n",
      "228/228 [==============================] - 20s 87ms/step - loss: 0.0016 - MSE: 0.0016 - val_loss: 6.4091e-04 - val_MSE: 6.4091e-04\n",
      "Epoch 16/100\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0014 - MSE: 0.0014 - val_loss: 5.1874e-04 - val_MSE: 5.1874e-04\n",
      "Epoch 17/100\n",
      "228/228 [==============================] - 19s 85ms/step - loss: 0.0011 - MSE: 0.0011 - val_loss: 4.2354e-04 - val_MSE: 4.2354e-04\n",
      "Epoch 18/100\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 9.8997e-04 - MSE: 9.8997e-04 - val_loss: 3.5201e-04 - val_MSE: 3.5201e-04\n",
      "Epoch 19/100\n",
      "228/228 [==============================] - 20s 86ms/step - loss: 8.3767e-04 - MSE: 8.3767e-04 - val_loss: 2.9299e-04 - val_MSE: 2.9299e-04\n",
      "Epoch 20/100\n",
      "228/228 [==============================] - 19s 86ms/step - loss: 7.1008e-04 - MSE: 7.1008e-04 - val_loss: 2.4374e-04 - val_MSE: 2.4374e-04\n",
      "Epoch 21/100\n",
      "228/228 [==============================] - 20s 86ms/step - loss: 5.8510e-04 - MSE: 5.8510e-04 - val_loss: 2.0817e-04 - val_MSE: 2.0817e-04\n",
      "Epoch 22/100\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 5.1673e-04 - MSE: 5.1673e-04 - val_loss: 1.7748e-04 - val_MSE: 1.7748e-04\n",
      "Epoch 23/100\n",
      "228/228 [==============================] - 20s 86ms/step - loss: 4.5075e-04 - MSE: 4.5075e-04 - val_loss: 1.5028e-04 - val_MSE: 1.5028e-04\n",
      "Epoch 24/100\n",
      "228/228 [==============================] - 20s 86ms/step - loss: 3.7678e-04 - MSE: 3.7678e-04 - val_loss: 1.3001e-04 - val_MSE: 1.3001e-04\n",
      "Epoch 25/100\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 3.4780e-04 - MSE: 3.4780e-04 - val_loss: 1.1203e-04 - val_MSE: 1.1203e-04\n",
      "Epoch 26/100\n",
      "228/228 [==============================] - 20s 87ms/step - loss: 2.9299e-04 - MSE: 2.9299e-04 - val_loss: 9.5082e-05 - val_MSE: 9.5082e-05\n",
      "Epoch 27/100\n",
      "228/228 [==============================] - 20s 86ms/step - loss: 2.6161e-04 - MSE: 2.6161e-04 - val_loss: 8.1870e-05 - val_MSE: 8.1870e-05\n",
      "Epoch 28/100\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 2.3202e-04 - MSE: 2.3202e-04 - val_loss: 7.2215e-05 - val_MSE: 7.2215e-05\n",
      "Epoch 29/100\n",
      "228/228 [==============================] - 18s 80ms/step - loss: 1.8894e-04 - MSE: 1.8894e-04 - val_loss: 6.1898e-05 - val_MSE: 6.1898e-05\n",
      "Epoch 30/100\n",
      "228/228 [==============================] - 18s 80ms/step - loss: 1.6748e-04 - MSE: 1.6748e-04 - val_loss: 5.3304e-05 - val_MSE: 5.3304e-05\n",
      "Epoch 31/100\n",
      "228/228 [==============================] - 19s 82ms/step - loss: 1.5356e-04 - MSE: 1.5356e-04 - val_loss: 4.7262e-05 - val_MSE: 4.7262e-05\n",
      "Epoch 32/100\n",
      "228/228 [==============================] - 16s 71ms/step - loss: 1.3259e-04 - MSE: 1.3259e-04 - val_loss: 4.0954e-05 - val_MSE: 4.0954e-05\n",
      "Epoch 33/100\n",
      "228/228 [==============================] - 19s 84ms/step - loss: 1.1201e-04 - MSE: 1.1201e-04 - val_loss: 3.6253e-05 - val_MSE: 3.6253e-05\n",
      "Epoch 34/100\n",
      "228/228 [==============================] - 19s 83ms/step - loss: 9.7555e-05 - MSE: 9.7555e-05 - val_loss: 3.1715e-05 - val_MSE: 3.1715e-05\n",
      "Epoch 35/100\n",
      "228/228 [==============================] - 15s 68ms/step - loss: 8.8150e-05 - MSE: 8.8150e-05 - val_loss: 2.7699e-05 - val_MSE: 2.7699e-05\n",
      "Epoch 36/100\n",
      "228/228 [==============================] - 18s 81ms/step - loss: 7.9540e-05 - MSE: 7.9540e-05 - val_loss: 2.4681e-05 - val_MSE: 2.4681e-05\n",
      "Epoch 37/100\n",
      "228/228 [==============================] - 11s 49ms/step - loss: 6.8852e-05 - MSE: 6.8852e-05 - val_loss: 2.1717e-05 - val_MSE: 2.1717e-05\n",
      "Epoch 38/100\n",
      "228/228 [==============================] - 8s 34ms/step - loss: 5.8271e-05 - MSE: 5.8271e-05 - val_loss: 1.9140e-05 - val_MSE: 1.9140e-05\n",
      "Epoch 39/100\n",
      "228/228 [==============================] - 7s 33ms/step - loss: 5.4976e-05 - MSE: 5.4976e-05 - val_loss: 1.6811e-05 - val_MSE: 1.6811e-05\n",
      "Epoch 40/100\n",
      "228/228 [==============================] - 10s 45ms/step - loss: 4.7186e-05 - MSE: 4.7186e-05 - val_loss: 1.5200e-05 - val_MSE: 1.5200e-05\n",
      "Epoch 41/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 4.1861e-05 - MSE: 4.1861e-05 - val_loss: 1.3847e-05 - val_MSE: 1.3847e-05\n",
      "Epoch 42/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 3.7459e-05 - MSE: 3.7459e-05 - val_loss: 1.1639e-05 - val_MSE: 1.1639e-05\n",
      "Epoch 43/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 3.0648e-05 - MSE: 3.0648e-05 - val_loss: 1.0721e-05 - val_MSE: 1.0721e-05\n",
      "Epoch 44/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 2.8980e-05 - MSE: 2.8980e-05 - val_loss: 9.5665e-06 - val_MSE: 9.5665e-06\n",
      "Epoch 45/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 2.5456e-05 - MSE: 2.5456e-05 - val_loss: 8.4782e-06 - val_MSE: 8.4782e-06\n",
      "Epoch 46/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 2.4006e-05 - MSE: 2.4006e-05 - val_loss: 7.4719e-06 - val_MSE: 7.4719e-06\n",
      "Epoch 47/100\n",
      "228/228 [==============================] - 11s 49ms/step - loss: 2.0413e-05 - MSE: 2.0413e-05 - val_loss: 6.7780e-06 - val_MSE: 6.7780e-06\n",
      "Epoch 48/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 1.8432e-05 - MSE: 1.8432e-05 - val_loss: 6.3713e-06 - val_MSE: 6.3713e-06\n",
      "Epoch 49/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 1.6308e-05 - MSE: 1.6308e-05 - val_loss: 5.7995e-06 - val_MSE: 5.7995e-06\n",
      "Epoch 50/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 1.5221e-05 - MSE: 1.5221e-05 - val_loss: 5.3238e-06 - val_MSE: 5.3238e-06\n",
      "Epoch 51/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 1.3434e-05 - MSE: 1.3434e-05 - val_loss: 5.2815e-06 - val_MSE: 5.2815e-06\n",
      "Epoch 52/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 1.2417e-05 - MSE: 1.2417e-05 - val_loss: 4.4812e-06 - val_MSE: 4.4812e-06\n",
      "Epoch 53/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 1.1033e-05 - MSE: 1.1033e-05 - val_loss: 4.1000e-06 - val_MSE: 4.1000e-06\n",
      "Epoch 54/100\n",
      "228/228 [==============================] - 11s 35ms/step - loss: 1.0596e-05 - MSE: 1.0596e-05 - val_loss: 4.0890e-06 - val_MSE: 4.0890e-06\n",
      "Epoch 55/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 9.4643e-06 - MSE: 9.4643e-06 - val_loss: 3.7138e-06 - val_MSE: 3.7138e-06\n",
      "Epoch 56/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 8.2615e-06 - MSE: 8.2615e-06 - val_loss: 3.3777e-06 - val_MSE: 3.3777e-06\n",
      "Epoch 57/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 7.6471e-06 - MSE: 7.6471e-06 - val_loss: 3.2128e-06 - val_MSE: 3.2128e-06\n",
      "Epoch 58/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 6.7431e-06 - MSE: 6.7431e-06 - val_loss: 3.0853e-06 - val_MSE: 3.0853e-06\n",
      "Epoch 59/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 6.7370e-06 - MSE: 6.7370e-06 - val_loss: 3.0170e-06 - val_MSE: 3.0170e-06\n",
      "Epoch 60/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 5.8788e-06 - MSE: 5.8788e-06 - val_loss: 2.9865e-06 - val_MSE: 2.9865e-06\n",
      "Epoch 61/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 5.2651e-06 - MSE: 5.2651e-06 - val_loss: 3.3116e-06 - val_MSE: 3.3116e-06\n",
      "Epoch 62/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 5.0577e-06 - MSE: 5.0577e-06 - val_loss: 2.6189e-06 - val_MSE: 2.6189e-06\n",
      "Epoch 63/100\n",
      "228/228 [==============================] - 11s 49ms/step - loss: 4.7949e-06 - MSE: 4.7949e-06 - val_loss: 2.6255e-06 - val_MSE: 2.6255e-06\n",
      "Epoch 64/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 4.7102e-06 - MSE: 4.7102e-06 - val_loss: 2.4359e-06 - val_MSE: 2.4359e-06\n",
      "Epoch 65/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 4.2476e-06 - MSE: 4.2476e-06 - val_loss: 2.3974e-06 - val_MSE: 2.3974e-06\n",
      "Epoch 66/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 3.8981e-06 - MSE: 3.8981e-06 - val_loss: 2.3547e-06 - val_MSE: 2.3547e-06\n",
      "Epoch 67/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 3.5411e-06 - MSE: 3.5411e-06 - val_loss: 2.4340e-06 - val_MSE: 2.4340e-06\n",
      "Epoch 68/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 3.5106e-06 - MSE: 3.5106e-06 - val_loss: 2.3127e-06 - val_MSE: 2.3127e-06\n",
      "Epoch 69/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 3.3788e-06 - MSE: 3.3788e-06 - val_loss: 2.2067e-06 - val_MSE: 2.2067e-06\n",
      "Epoch 70/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 3.3168e-06 - MSE: 3.3168e-06 - val_loss: 2.2446e-06 - val_MSE: 2.2446e-06\n",
      "Epoch 71/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 3.1363e-06 - MSE: 3.1363e-06 - val_loss: 2.2056e-06 - val_MSE: 2.2056e-06\n",
      "Epoch 72/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 3.0885e-06 - MSE: 3.0885e-06 - val_loss: 2.1035e-06 - val_MSE: 2.1035e-06\n",
      "Epoch 73/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 2.9370e-06 - MSE: 2.9370e-06 - val_loss: 2.0705e-06 - val_MSE: 2.0705e-06\n",
      "Epoch 74/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 2.8240e-06 - MSE: 2.8240e-06 - val_loss: 2.1030e-06 - val_MSE: 2.1030e-06\n",
      "Epoch 75/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 2.6739e-06 - MSE: 2.6739e-06 - val_loss: 2.2326e-06 - val_MSE: 2.2326e-06\n",
      "Epoch 76/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 2.7176e-06 - MSE: 2.7176e-06 - val_loss: 2.0887e-06 - val_MSE: 2.0887e-06\n",
      "Epoch 77/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 2.8103e-06 - MSE: 2.8103e-06 - val_loss: 2.0582e-06 - val_MSE: 2.0582e-06\n",
      "Epoch 78/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 2.4831e-06 - MSE: 2.4831e-06 - val_loss: 2.0233e-06 - val_MSE: 2.0233e-06\n",
      "Epoch 79/100\n",
      "228/228 [==============================] - 11s 48ms/step - loss: 2.4655e-06 - MSE: 2.4655e-06 - val_loss: 2.0731e-06 - val_MSE: 2.0731e-06\n",
      "Epoch 80/100\n",
      "228/228 [==============================] - 8s 36ms/step - loss: 2.4560e-06 - MSE: 2.4560e-06 - val_loss: 2.0679e-06 - val_MSE: 2.0679e-06\n",
      "Epoch 81/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 2.6157e-06 - MSE: 2.6157e-06 - val_loss: 1.9783e-06 - val_MSE: 1.9783e-06\n",
      "Epoch 82/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 2.3181e-06 - MSE: 2.3181e-06 - val_loss: 2.0882e-06 - val_MSE: 2.0882e-06\n",
      "Epoch 83/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 2.3457e-06 - MSE: 2.3457e-06 - val_loss: 2.0993e-06 - val_MSE: 2.0993e-06\n",
      "Epoch 84/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 2.3024e-06 - MSE: 2.3024e-06 - val_loss: 1.9942e-06 - val_MSE: 1.9942e-06\n",
      "Epoch 85/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 2.2761e-06 - MSE: 2.2761e-06 - val_loss: 2.2143e-06 - val_MSE: 2.2143e-06\n",
      "Epoch 86/100\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 2.2498e-06 - MSE: 2.2498e-06 - val_loss: 2.0651e-06 - val_MSE: 2.0651e-06\n",
      "INFO:tensorflow:Assets written to: saved_model/HII_120_log_s302_sigmoid_4layers\\assets\n",
      "Dataset HII has been trained and saved.\n",
      "(7, 120)\n",
      "(1,)\n",
      "Epoch 1/100\n",
      "412/412 [==============================] - 14s 25ms/step - loss: 0.2843 - MSE: 0.2843 - val_loss: 0.0524 - val_MSE: 0.0524\n",
      "Epoch 2/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 0.0559 - MSE: 0.0559 - val_loss: 0.0186 - val_MSE: 0.0186\n",
      "Epoch 3/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 0.0209 - MSE: 0.0209 - val_loss: 0.0090 - val_MSE: 0.0090\n",
      "Epoch 4/100\n",
      "412/412 [==============================] - 16s 39ms/step - loss: 0.0097 - MSE: 0.0097 - val_loss: 0.0042 - val_MSE: 0.0042\n",
      "Epoch 5/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 0.0052 - MSE: 0.0052 - val_loss: 0.0020 - val_MSE: 0.0020\n",
      "Epoch 6/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 0.0029 - MSE: 0.0029 - val_loss: 0.0011 - val_MSE: 0.0011\n",
      "Epoch 7/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 0.0019 - MSE: 0.0019 - val_loss: 6.9287e-04 - val_MSE: 6.9287e-04\n",
      "Epoch 8/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 0.0013 - MSE: 0.0013 - val_loss: 4.4926e-04 - val_MSE: 4.4926e-04\n",
      "Epoch 9/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 8.8899e-04 - MSE: 8.8899e-04 - val_loss: 3.0584e-04 - val_MSE: 3.0584e-04\n",
      "Epoch 10/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 6.3535e-04 - MSE: 6.3535e-04 - val_loss: 2.0839e-04 - val_MSE: 2.0839e-04\n",
      "Epoch 11/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 4.7944e-04 - MSE: 4.7944e-04 - val_loss: 1.5333e-04 - val_MSE: 1.5333e-04\n",
      "Epoch 12/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 3.6158e-04 - MSE: 3.6158e-04 - val_loss: 1.0898e-04 - val_MSE: 1.0898e-04\n",
      "Epoch 13/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 2.6698e-04 - MSE: 2.6698e-04 - val_loss: 8.0401e-05 - val_MSE: 8.0401e-05\n",
      "Epoch 14/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 2.0947e-04 - MSE: 2.0947e-04 - val_loss: 6.1775e-05 - val_MSE: 6.1775e-05\n",
      "Epoch 15/100\n",
      "412/412 [==============================] - 16s 39ms/step - loss: 1.5994e-04 - MSE: 1.5994e-04 - val_loss: 4.7847e-05 - val_MSE: 4.7847e-05\n",
      "Epoch 16/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.2816e-04 - MSE: 1.2816e-04 - val_loss: 3.8352e-05 - val_MSE: 3.8352e-05\n",
      "Epoch 17/100\n",
      "412/412 [==============================] - 15s 37ms/step - loss: 9.9781e-05 - MSE: 9.9781e-05 - val_loss: 3.1151e-05 - val_MSE: 3.1151e-05\n",
      "Epoch 18/100\n",
      "412/412 [==============================] - 15s 35ms/step - loss: 7.7363e-05 - MSE: 7.7363e-05 - val_loss: 2.5838e-05 - val_MSE: 2.5838e-05\n",
      "Epoch 19/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 6.2047e-05 - MSE: 6.2047e-05 - val_loss: 2.1470e-05 - val_MSE: 2.1470e-05\n",
      "Epoch 20/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 4.7509e-05 - MSE: 4.7509e-05 - val_loss: 1.7932e-05 - val_MSE: 1.7932e-05\n",
      "Epoch 21/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 3.8938e-05 - MSE: 3.8938e-05 - val_loss: 1.5826e-05 - val_MSE: 1.5826e-05\n",
      "Epoch 22/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 3.2550e-05 - MSE: 3.2550e-05 - val_loss: 1.3311e-05 - val_MSE: 1.3311e-05\n",
      "Epoch 23/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 2.5581e-05 - MSE: 2.5581e-05 - val_loss: 1.1196e-05 - val_MSE: 1.1196e-05\n",
      "Epoch 24/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 2.0670e-05 - MSE: 2.0670e-05 - val_loss: 9.6454e-06 - val_MSE: 9.6454e-06\n",
      "Epoch 25/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.7677e-05 - MSE: 1.7677e-05 - val_loss: 8.7305e-06 - val_MSE: 8.7305e-06\n",
      "Epoch 26/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 1.3891e-05 - MSE: 1.3891e-05 - val_loss: 7.8536e-06 - val_MSE: 7.8536e-06\n",
      "Epoch 27/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 1.1592e-05 - MSE: 1.1592e-05 - val_loss: 6.7858e-06 - val_MSE: 6.7858e-06\n",
      "Epoch 28/100\n",
      "412/412 [==============================] - 16s 39ms/step - loss: 9.9126e-06 - MSE: 9.9126e-06 - val_loss: 5.9490e-06 - val_MSE: 5.9490e-06\n",
      "Epoch 29/100\n",
      "412/412 [==============================] - 16s 39ms/step - loss: 8.4245e-06 - MSE: 8.4245e-06 - val_loss: 5.8084e-06 - val_MSE: 5.8084e-06\n",
      "Epoch 30/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 6.7372e-06 - MSE: 6.7372e-06 - val_loss: 4.8078e-06 - val_MSE: 4.8078e-06\n",
      "Epoch 31/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 5.9436e-06 - MSE: 5.9436e-06 - val_loss: 3.9284e-06 - val_MSE: 3.9284e-06\n",
      "Epoch 32/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 5.0147e-06 - MSE: 5.0147e-06 - val_loss: 4.2427e-06 - val_MSE: 4.2427e-06\n",
      "Epoch 33/100\n",
      "412/412 [==============================] - 16s 39ms/step - loss: 4.3074e-06 - MSE: 4.3074e-06 - val_loss: 2.9899e-06 - val_MSE: 2.9899e-06\n",
      "Epoch 34/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 3.8332e-06 - MSE: 3.8332e-06 - val_loss: 2.5005e-06 - val_MSE: 2.5005e-06\n",
      "Epoch 35/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 3.2214e-06 - MSE: 3.2214e-06 - val_loss: 2.1544e-06 - val_MSE: 2.1544e-06\n",
      "Epoch 36/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 2.9062e-06 - MSE: 2.9062e-06 - val_loss: 1.8146e-06 - val_MSE: 1.8146e-06\n",
      "Epoch 37/100\n",
      "412/412 [==============================] - 16s 31ms/step - loss: 2.6320e-06 - MSE: 2.6320e-06 - val_loss: 1.9775e-06 - val_MSE: 1.9775e-06\n",
      "Epoch 38/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 2.2989e-06 - MSE: 2.2989e-06 - val_loss: 1.9487e-06 - val_MSE: 1.9487e-06\n",
      "Epoch 39/100\n",
      "412/412 [==============================] - 16s 39ms/step - loss: 2.1106e-06 - MSE: 2.1106e-06 - val_loss: 1.3742e-06 - val_MSE: 1.3742e-06\n",
      "Epoch 40/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.9340e-06 - MSE: 1.9340e-06 - val_loss: 1.3299e-06 - val_MSE: 1.3299e-06\n",
      "Epoch 41/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.7770e-06 - MSE: 1.7770e-06 - val_loss: 1.1343e-06 - val_MSE: 1.1343e-06\n",
      "Epoch 42/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.6467e-06 - MSE: 1.6467e-06 - val_loss: 9.2161e-07 - val_MSE: 9.2161e-07\n",
      "Epoch 43/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 1.5796e-06 - MSE: 1.5796e-06 - val_loss: 8.3876e-07 - val_MSE: 8.3876e-07\n",
      "Epoch 44/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.4869e-06 - MSE: 1.4869e-06 - val_loss: 8.2151e-07 - val_MSE: 8.2151e-07\n",
      "Epoch 45/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.4085e-06 - MSE: 1.4085e-06 - val_loss: 7.6870e-07 - val_MSE: 7.6870e-07\n",
      "Epoch 46/100\n",
      "412/412 [==============================] - 16s 39ms/step - loss: 1.3889e-06 - MSE: 1.3889e-06 - val_loss: 7.8077e-07 - val_MSE: 7.8077e-07\n",
      "Epoch 47/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.3063e-06 - MSE: 1.3063e-06 - val_loss: 7.2827e-07 - val_MSE: 7.2827e-07\n",
      "Epoch 48/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 1.2989e-06 - MSE: 1.2989e-06 - val_loss: 7.1413e-07 - val_MSE: 7.1413e-07\n",
      "Epoch 49/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 1.2634e-06 - MSE: 1.2634e-06 - val_loss: 7.2328e-07 - val_MSE: 7.2328e-07\n",
      "Epoch 50/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.2655e-06 - MSE: 1.2655e-06 - val_loss: 7.0445e-07 - val_MSE: 7.0445e-07\n",
      "Epoch 51/100\n",
      "412/412 [==============================] - 16s 39ms/step - loss: 1.2554e-06 - MSE: 1.2554e-06 - val_loss: 6.9862e-07 - val_MSE: 6.9862e-07\n",
      "Epoch 52/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.2292e-06 - MSE: 1.2292e-06 - val_loss: 7.0246e-07 - val_MSE: 7.0246e-07\n",
      "Epoch 53/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.2191e-06 - MSE: 1.2191e-06 - val_loss: 6.9604e-07 - val_MSE: 6.9604e-07\n",
      "Epoch 54/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 1.2021e-06 - MSE: 1.2021e-06 - val_loss: 7.1517e-07 - val_MSE: 7.1517e-07\n",
      "Epoch 55/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.2043e-06 - MSE: 1.2043e-06 - val_loss: 7.0127e-07 - val_MSE: 7.0127e-07\n",
      "Epoch 56/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.1902e-06 - MSE: 1.1902e-06 - val_loss: 6.8678e-07 - val_MSE: 6.8678e-07\n",
      "Epoch 57/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.1942e-06 - MSE: 1.1942e-06 - val_loss: 6.8527e-07 - val_MSE: 6.8527e-07\n",
      "Epoch 58/100\n",
      "412/412 [==============================] - 16s 39ms/step - loss: 1.1769e-06 - MSE: 1.1769e-06 - val_loss: 6.8532e-07 - val_MSE: 6.8532e-07\n",
      "Epoch 59/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.1894e-06 - MSE: 1.1894e-06 - val_loss: 6.9236e-07 - val_MSE: 6.9236e-07\n",
      "Epoch 60/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.1884e-06 - MSE: 1.1884e-06 - val_loss: 6.8853e-07 - val_MSE: 6.8853e-07\n",
      "Epoch 61/100\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 1.1777e-06 - MSE: 1.1777e-06 - val_loss: 6.8255e-07 - val_MSE: 6.8255e-07\n",
      "Epoch 62/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 1.1785e-06 - MSE: 1.1785e-06 - val_loss: 6.8662e-07 - val_MSE: 6.8662e-07\n",
      "Epoch 63/100\n",
      "412/412 [==============================] - 16s 39ms/step - loss: 1.1703e-06 - MSE: 1.1703e-06 - val_loss: 6.8244e-07 - val_MSE: 6.8244e-07\n",
      "Epoch 64/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.1736e-06 - MSE: 1.1736e-06 - val_loss: 6.8296e-07 - val_MSE: 6.8296e-07\n",
      "Epoch 65/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.1723e-06 - MSE: 1.1723e-06 - val_loss: 6.9061e-07 - val_MSE: 6.9061e-07\n",
      "Epoch 66/100\n",
      "412/412 [==============================] - 12s 30ms/step - loss: 1.1690e-06 - MSE: 1.1690e-06 - val_loss: 6.8131e-07 - val_MSE: 6.8131e-07\n",
      "Epoch 67/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.1719e-06 - MSE: 1.1719e-06 - val_loss: 6.8511e-07 - val_MSE: 6.8511e-07\n",
      "Epoch 68/100\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 1.1669e-06 - MSE: 1.1669e-06 - val_loss: 7.0277e-07 - val_MSE: 7.0277e-07\n",
      "Epoch 69/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.1795e-06 - MSE: 1.1795e-06 - val_loss: 6.9476e-07 - val_MSE: 6.9476e-07\n",
      "Epoch 70/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.1574e-06 - MSE: 1.1574e-06 - val_loss: 6.8780e-07 - val_MSE: 6.8780e-07\n",
      "Epoch 71/100\n",
      "412/412 [==============================] - 16s 38ms/step - loss: 1.1666e-06 - MSE: 1.1666e-06 - val_loss: 6.8296e-07 - val_MSE: 6.8296e-07\n",
      "INFO:tensorflow:Assets written to: saved_model/LMT_120_log_s302_sigmoid_4layers\\assets\n",
      "Dataset LMT has been trained and saved.\n",
      "(7, 120)\n",
      "(1,)\n",
      "Epoch 1/100\n",
      "199/199 [==============================] - 11s 42ms/step - loss: 0.7913 - MSE: 0.7913 - val_loss: 0.1806 - val_MSE: 0.1806\n",
      "Epoch 2/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 0.1406 - MSE: 0.1406 - val_loss: 0.0464 - val_MSE: 0.0464\n",
      "Epoch 3/100\n",
      "199/199 [==============================] - 5s 24ms/step - loss: 0.0544 - MSE: 0.0544 - val_loss: 0.0191 - val_MSE: 0.0191\n",
      "Epoch 4/100\n",
      "199/199 [==============================] - 5s 24ms/step - loss: 0.0289 - MSE: 0.0289 - val_loss: 0.0106 - val_MSE: 0.0106\n",
      "Epoch 5/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 0.0182 - MSE: 0.0182 - val_loss: 0.0070 - val_MSE: 0.0070\n",
      "Epoch 6/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 0.0126 - MSE: 0.0126 - val_loss: 0.0052 - val_MSE: 0.0052\n",
      "Epoch 7/100\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.0097 - MSE: 0.0097 - val_loss: 0.0039 - val_MSE: 0.0039\n",
      "Epoch 8/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 0.0075 - MSE: 0.0075 - val_loss: 0.0031 - val_MSE: 0.0031\n",
      "Epoch 9/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 0.0058 - MSE: 0.0058 - val_loss: 0.0024 - val_MSE: 0.0024\n",
      "Epoch 10/100\n",
      "199/199 [==============================] - 5s 24ms/step - loss: 0.0045 - MSE: 0.0045 - val_loss: 0.0020 - val_MSE: 0.0020\n",
      "Epoch 11/100\n",
      "199/199 [==============================] - 5s 25ms/step - loss: 0.0037 - MSE: 0.0037 - val_loss: 0.0016 - val_MSE: 0.0016\n",
      "Epoch 12/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 0.0031 - MSE: 0.0031 - val_loss: 0.0013 - val_MSE: 0.0013\n",
      "Epoch 13/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 0.0026 - MSE: 0.0026 - val_loss: 0.0011 - val_MSE: 0.0011\n",
      "Epoch 14/100\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.0022 - MSE: 0.0022 - val_loss: 8.9519e-04 - val_MSE: 8.9519e-04\n",
      "Epoch 15/100\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.0018 - MSE: 0.0018 - val_loss: 7.6267e-04 - val_MSE: 7.6267e-04\n",
      "Epoch 16/100\n",
      "199/199 [==============================] - 6s 32ms/step - loss: 0.0016 - MSE: 0.0016 - val_loss: 6.4850e-04 - val_MSE: 6.4850e-04\n",
      "Epoch 17/100\n",
      "199/199 [==============================] - 7s 34ms/step - loss: 0.0014 - MSE: 0.0014 - val_loss: 5.5479e-04 - val_MSE: 5.5479e-04\n",
      "Epoch 18/100\n",
      "199/199 [==============================] - 4s 21ms/step - loss: 0.0012 - MSE: 0.0012 - val_loss: 4.7668e-04 - val_MSE: 4.7668e-04\n",
      "Epoch 19/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 0.0010 - MSE: 0.0010 - val_loss: 4.1351e-04 - val_MSE: 4.1351e-04\n",
      "Epoch 20/100\n",
      "199/199 [==============================] - 7s 37ms/step - loss: 9.1587e-04 - MSE: 9.1587e-04 - val_loss: 3.5630e-04 - val_MSE: 3.5630e-04\n",
      "Epoch 21/100\n",
      "199/199 [==============================] - 7s 37ms/step - loss: 8.5038e-04 - MSE: 8.5038e-04 - val_loss: 3.0879e-04 - val_MSE: 3.0879e-04\n",
      "Epoch 22/100\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 7.5342e-04 - MSE: 7.5342e-04 - val_loss: 2.6924e-04 - val_MSE: 2.6924e-04\n",
      "Epoch 23/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 6.5141e-04 - MSE: 6.5141e-04 - val_loss: 2.3643e-04 - val_MSE: 2.3643e-04\n",
      "Epoch 24/100\n",
      "199/199 [==============================] - 8s 41ms/step - loss: 5.5771e-04 - MSE: 5.5771e-04 - val_loss: 2.0863e-04 - val_MSE: 2.0863e-04\n",
      "Epoch 25/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 5.2121e-04 - MSE: 5.2121e-04 - val_loss: 1.8519e-04 - val_MSE: 1.8519e-04\n",
      "Epoch 26/100\n",
      "199/199 [==============================] - 11s 54ms/step - loss: 4.4489e-04 - MSE: 4.4489e-04 - val_loss: 1.6488e-04 - val_MSE: 1.6488e-04\n",
      "Epoch 27/100\n",
      "199/199 [==============================] - 8s 38ms/step - loss: 4.1777e-04 - MSE: 4.1777e-04 - val_loss: 1.4677e-04 - val_MSE: 1.4677e-04\n",
      "Epoch 28/100\n",
      "199/199 [==============================] - 5s 25ms/step - loss: 3.6203e-04 - MSE: 3.6203e-04 - val_loss: 1.3129e-04 - val_MSE: 1.3129e-04\n",
      "Epoch 29/100\n",
      "199/199 [==============================] - 10s 52ms/step - loss: 3.2203e-04 - MSE: 3.2203e-04 - val_loss: 1.1742e-04 - val_MSE: 1.1742e-04\n",
      "Epoch 30/100\n",
      "199/199 [==============================] - 10s 48ms/step - loss: 2.9349e-04 - MSE: 2.9349e-04 - val_loss: 1.0550e-04 - val_MSE: 1.0550e-04\n",
      "Epoch 31/100\n",
      "199/199 [==============================] - 8s 41ms/step - loss: 2.6325e-04 - MSE: 2.6325e-04 - val_loss: 9.5763e-05 - val_MSE: 9.5763e-05\n",
      "Epoch 32/100\n",
      "199/199 [==============================] - 8s 38ms/step - loss: 2.3498e-04 - MSE: 2.3498e-04 - val_loss: 8.5546e-05 - val_MSE: 8.5546e-05\n",
      "Epoch 33/100\n",
      "199/199 [==============================] - 8s 38ms/step - loss: 2.1148e-04 - MSE: 2.1148e-04 - val_loss: 7.6630e-05 - val_MSE: 7.6630e-05\n",
      "Epoch 34/100\n",
      "199/199 [==============================] - 7s 38ms/step - loss: 1.9809e-04 - MSE: 1.9809e-04 - val_loss: 6.8280e-05 - val_MSE: 6.8280e-05\n",
      "Epoch 35/100\n",
      "199/199 [==============================] - 8s 38ms/step - loss: 1.6708e-04 - MSE: 1.6708e-04 - val_loss: 6.1636e-05 - val_MSE: 6.1636e-05\n",
      "Epoch 36/100\n",
      "199/199 [==============================] - 7s 37ms/step - loss: 1.5158e-04 - MSE: 1.5158e-04 - val_loss: 5.4098e-05 - val_MSE: 5.4098e-05\n",
      "Epoch 37/100\n",
      "199/199 [==============================] - 7s 37ms/step - loss: 1.3657e-04 - MSE: 1.3657e-04 - val_loss: 4.8490e-05 - val_MSE: 4.8490e-05\n",
      "Epoch 38/100\n",
      "199/199 [==============================] - 5s 24ms/step - loss: 1.1880e-04 - MSE: 1.1880e-04 - val_loss: 4.3653e-05 - val_MSE: 4.3653e-05\n",
      "Epoch 39/100\n",
      "199/199 [==============================] - 7s 37ms/step - loss: 1.0663e-04 - MSE: 1.0663e-04 - val_loss: 3.8302e-05 - val_MSE: 3.8302e-05\n",
      "Epoch 40/100\n",
      "199/199 [==============================] - 7s 37ms/step - loss: 9.2541e-05 - MSE: 9.2541e-05 - val_loss: 3.4283e-05 - val_MSE: 3.4283e-05\n",
      "Epoch 41/100\n",
      "199/199 [==============================] - 7s 37ms/step - loss: 8.9332e-05 - MSE: 8.9332e-05 - val_loss: 3.0461e-05 - val_MSE: 3.0461e-05\n",
      "Epoch 42/100\n",
      "199/199 [==============================] - 11s 57ms/step - loss: 7.5822e-05 - MSE: 7.5822e-05 - val_loss: 2.7320e-05 - val_MSE: 2.7320e-05\n",
      "Epoch 43/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 6.9258e-05 - MSE: 6.9258e-05 - val_loss: 2.4245e-05 - val_MSE: 2.4245e-05\n",
      "Epoch 44/100\n",
      "199/199 [==============================] - 7s 37ms/step - loss: 5.9439e-05 - MSE: 5.9439e-05 - val_loss: 2.1810e-05 - val_MSE: 2.1810e-05\n",
      "Epoch 45/100\n",
      "199/199 [==============================] - 7s 37ms/step - loss: 5.7079e-05 - MSE: 5.7079e-05 - val_loss: 1.9556e-05 - val_MSE: 1.9556e-05\n",
      "Epoch 46/100\n",
      "199/199 [==============================] - 11s 55ms/step - loss: 4.8178e-05 - MSE: 4.8178e-05 - val_loss: 1.7187e-05 - val_MSE: 1.7187e-05\n",
      "Epoch 47/100\n",
      "199/199 [==============================] - 11s 54ms/step - loss: 4.4777e-05 - MSE: 4.4777e-05 - val_loss: 1.6362e-05 - val_MSE: 1.6362e-05\n",
      "Epoch 48/100\n",
      "199/199 [==============================] - 11s 54ms/step - loss: 3.9099e-05 - MSE: 3.9099e-05 - val_loss: 1.4192e-05 - val_MSE: 1.4192e-05\n",
      "Epoch 49/100\n",
      "199/199 [==============================] - 11s 55ms/step - loss: 3.7733e-05 - MSE: 3.7733e-05 - val_loss: 1.2843e-05 - val_MSE: 1.2843e-05\n",
      "Epoch 50/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 3.1424e-05 - MSE: 3.1424e-05 - val_loss: 1.1521e-05 - val_MSE: 1.1521e-05\n",
      "Epoch 51/100\n",
      "199/199 [==============================] - 8s 41ms/step - loss: 2.8650e-05 - MSE: 2.8650e-05 - val_loss: 1.0580e-05 - val_MSE: 1.0580e-05\n",
      "Epoch 52/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 2.5931e-05 - MSE: 2.5931e-05 - val_loss: 9.2747e-06 - val_MSE: 9.2747e-06\n",
      "Epoch 53/100\n",
      "199/199 [==============================] - 8s 38ms/step - loss: 2.2489e-05 - MSE: 2.2489e-05 - val_loss: 8.6243e-06 - val_MSE: 8.6243e-06\n",
      "Epoch 54/100\n",
      "199/199 [==============================] - 8s 42ms/step - loss: 2.1143e-05 - MSE: 2.1143e-05 - val_loss: 8.0323e-06 - val_MSE: 8.0323e-06\n",
      "Epoch 55/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 1.9927e-05 - MSE: 1.9927e-05 - val_loss: 7.1968e-06 - val_MSE: 7.1968e-06\n",
      "Epoch 56/100\n",
      "199/199 [==============================] - 8s 38ms/step - loss: 1.7661e-05 - MSE: 1.7661e-05 - val_loss: 6.7926e-06 - val_MSE: 6.7926e-06\n",
      "Epoch 57/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 1.6955e-05 - MSE: 1.6955e-05 - val_loss: 6.2843e-06 - val_MSE: 6.2843e-06\n",
      "Epoch 58/100\n",
      "199/199 [==============================] - 8s 41ms/step - loss: 1.4648e-05 - MSE: 1.4648e-05 - val_loss: 5.8236e-06 - val_MSE: 5.8236e-06\n",
      "Epoch 59/100\n",
      "199/199 [==============================] - 11s 54ms/step - loss: 1.3606e-05 - MSE: 1.3606e-05 - val_loss: 5.6114e-06 - val_MSE: 5.6114e-06\n",
      "Epoch 60/100\n",
      "199/199 [==============================] - 11s 54ms/step - loss: 1.2616e-05 - MSE: 1.2616e-05 - val_loss: 6.5778e-06 - val_MSE: 6.5778e-06\n",
      "Epoch 61/100\n",
      "199/199 [==============================] - 14s 70ms/step - loss: 1.1731e-05 - MSE: 1.1731e-05 - val_loss: 5.0929e-06 - val_MSE: 5.0929e-06\n",
      "Epoch 62/100\n",
      "199/199 [==============================] - 11s 55ms/step - loss: 1.0428e-05 - MSE: 1.0428e-05 - val_loss: 4.7386e-06 - val_MSE: 4.7386e-06\n",
      "Epoch 63/100\n",
      "199/199 [==============================] - 11s 54ms/step - loss: 1.0232e-05 - MSE: 1.0232e-05 - val_loss: 4.6371e-06 - val_MSE: 4.6371e-06\n",
      "Epoch 64/100\n",
      "199/199 [==============================] - 11s 54ms/step - loss: 9.4003e-06 - MSE: 9.4003e-06 - val_loss: 4.3272e-06 - val_MSE: 4.3272e-06\n",
      "Epoch 65/100\n",
      "199/199 [==============================] - 11s 55ms/step - loss: 8.9910e-06 - MSE: 8.9910e-06 - val_loss: 4.4284e-06 - val_MSE: 4.4284e-06\n",
      "Epoch 66/100\n",
      "199/199 [==============================] - 11s 53ms/step - loss: 8.0287e-06 - MSE: 8.0287e-06 - val_loss: 4.0478e-06 - val_MSE: 4.0478e-06\n",
      "Epoch 67/100\n",
      "199/199 [==============================] - 11s 54ms/step - loss: 7.2346e-06 - MSE: 7.2346e-06 - val_loss: 3.9911e-06 - val_MSE: 3.9911e-06\n",
      "Epoch 68/100\n",
      "199/199 [==============================] - 11s 56ms/step - loss: 7.2180e-06 - MSE: 7.2180e-06 - val_loss: 4.0636e-06 - val_MSE: 4.0636e-06\n",
      "Epoch 69/100\n",
      "199/199 [==============================] - 13s 55ms/step - loss: 6.6282e-06 - MSE: 6.6282e-06 - val_loss: 3.7681e-06 - val_MSE: 3.7681e-06\n",
      "Epoch 70/100\n",
      "199/199 [==============================] - 14s 71ms/step - loss: 6.4457e-06 - MSE: 6.4457e-06 - val_loss: 4.0097e-06 - val_MSE: 4.0097e-06\n",
      "Epoch 71/100\n",
      "199/199 [==============================] - 11s 54ms/step - loss: 6.5993e-06 - MSE: 6.5993e-06 - val_loss: 3.6204e-06 - val_MSE: 3.6204e-06\n",
      "Epoch 72/100\n",
      "199/199 [==============================] - 11s 53ms/step - loss: 5.9879e-06 - MSE: 5.9879e-06 - val_loss: 3.4656e-06 - val_MSE: 3.4656e-06\n",
      "Epoch 73/100\n",
      "199/199 [==============================] - 11s 53ms/step - loss: 5.8238e-06 - MSE: 5.8238e-06 - val_loss: 3.8011e-06 - val_MSE: 3.8011e-06\n",
      "Epoch 74/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 5.5980e-06 - MSE: 5.5980e-06 - val_loss: 3.5047e-06 - val_MSE: 3.5047e-06\n",
      "Epoch 75/100\n",
      "199/199 [==============================] - 5s 24ms/step - loss: 5.2959e-06 - MSE: 5.2959e-06 - val_loss: 3.3157e-06 - val_MSE: 3.3157e-06\n",
      "Epoch 76/100\n",
      "199/199 [==============================] - 5s 25ms/step - loss: 4.9612e-06 - MSE: 4.9612e-06 - val_loss: 3.5992e-06 - val_MSE: 3.5992e-06\n",
      "Epoch 77/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 5.0330e-06 - MSE: 5.0330e-06 - val_loss: 3.3245e-06 - val_MSE: 3.3245e-06\n",
      "Epoch 78/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 4.8988e-06 - MSE: 4.8988e-06 - val_loss: 3.8830e-06 - val_MSE: 3.8830e-06\n",
      "Epoch 79/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 4.7642e-06 - MSE: 4.7642e-06 - val_loss: 3.2890e-06 - val_MSE: 3.2890e-06\n",
      "Epoch 80/100\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 4.8420e-06 - MSE: 4.8420e-06 - val_loss: 3.3436e-06 - val_MSE: 3.3436e-06\n",
      "Epoch 81/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 4.4464e-06 - MSE: 4.4464e-06 - val_loss: 3.2535e-06 - val_MSE: 3.2535e-06\n",
      "Epoch 82/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 4.3199e-06 - MSE: 4.3199e-06 - val_loss: 3.1819e-06 - val_MSE: 3.1819e-06\n",
      "Epoch 83/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 4.3310e-06 - MSE: 4.3310e-06 - val_loss: 3.1942e-06 - val_MSE: 3.1942e-06\n",
      "Epoch 84/100\n",
      "199/199 [==============================] - 5s 24ms/step - loss: 4.2483e-06 - MSE: 4.2483e-06 - val_loss: 3.0704e-06 - val_MSE: 3.0704e-06\n",
      "Epoch 85/100\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 4.2714e-06 - MSE: 4.2714e-06 - val_loss: 2.9413e-06 - val_MSE: 2.9413e-06\n",
      "Epoch 86/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 4.1755e-06 - MSE: 4.1755e-06 - val_loss: 3.0251e-06 - val_MSE: 3.0251e-06\n",
      "Epoch 87/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 4.1290e-06 - MSE: 4.1290e-06 - val_loss: 2.8891e-06 - val_MSE: 2.8891e-06\n",
      "Epoch 88/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 4.0739e-06 - MSE: 4.0739e-06 - val_loss: 2.8948e-06 - val_MSE: 2.8948e-06\n",
      "Epoch 89/100\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 3.9784e-06 - MSE: 3.9784e-06 - val_loss: 2.8720e-06 - val_MSE: 2.8720e-06\n",
      "Epoch 90/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 3.9607e-06 - MSE: 3.9607e-06 - val_loss: 2.9167e-06 - val_MSE: 2.9167e-06\n",
      "Epoch 91/100\n",
      "199/199 [==============================] - 5s 24ms/step - loss: 3.9418e-06 - MSE: 3.9418e-06 - val_loss: 2.8366e-06 - val_MSE: 2.8366e-06\n",
      "Epoch 92/100\n",
      "199/199 [==============================] - 5s 24ms/step - loss: 3.8811e-06 - MSE: 3.8811e-06 - val_loss: 2.8568e-06 - val_MSE: 2.8568e-06\n",
      "Epoch 93/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 3.8977e-06 - MSE: 3.8977e-06 - val_loss: 2.8187e-06 - val_MSE: 2.8187e-06\n",
      "Epoch 94/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 3.8749e-06 - MSE: 3.8749e-06 - val_loss: 2.8131e-06 - val_MSE: 2.8131e-06\n",
      "Epoch 95/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 3.8675e-06 - MSE: 3.8675e-06 - val_loss: 2.8538e-06 - val_MSE: 2.8538e-06\n",
      "Epoch 96/100\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 3.8224e-06 - MSE: 3.8224e-06 - val_loss: 2.7904e-06 - val_MSE: 2.7904e-06\n",
      "Epoch 97/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 3.7984e-06 - MSE: 3.7984e-06 - val_loss: 2.8704e-06 - val_MSE: 2.8704e-06\n",
      "Epoch 98/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 3.7990e-06 - MSE: 3.7990e-06 - val_loss: 2.7903e-06 - val_MSE: 2.7903e-06\n",
      "Epoch 99/100\n",
      "199/199 [==============================] - 5s 24ms/step - loss: 3.7897e-06 - MSE: 3.7897e-06 - val_loss: 2.7778e-06 - val_MSE: 2.7778e-06\n",
      "Epoch 100/100\n",
      "199/199 [==============================] - 8s 39ms/step - loss: 3.7622e-06 - MSE: 3.7622e-06 - val_loss: 2.7833e-06 - val_MSE: 2.7833e-06\n",
      "INFO:tensorflow:Assets written to: saved_model/MLI_120_log_s302_sigmoid_4layers\\assets\n",
      "Dataset MLI has been trained and saved.\n",
      "(7, 120)\n",
      "(1,)\n",
      "Epoch 1/100\n",
      "449/449 [==============================] - 20s 37ms/step - loss: 0.1558 - MSE: 0.1558 - val_loss: 0.0353 - val_MSE: 0.0353\n",
      "Epoch 2/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 0.0310 - MSE: 0.0310 - val_loss: 0.0097 - val_MSE: 0.0097\n",
      "Epoch 3/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 0.0100 - MSE: 0.0100 - val_loss: 0.0030 - val_MSE: 0.0030\n",
      "Epoch 4/100\n",
      "449/449 [==============================] - 19s 43ms/step - loss: 0.0037 - MSE: 0.0037 - val_loss: 0.0012 - val_MSE: 0.0012\n",
      "Epoch 5/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 0.0017 - MSE: 0.0017 - val_loss: 5.4612e-04 - val_MSE: 5.4612e-04\n",
      "Epoch 6/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 8.8498e-04 - MSE: 8.8498e-04 - val_loss: 3.0243e-04 - val_MSE: 3.0243e-04\n",
      "Epoch 7/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 5.5200e-04 - MSE: 5.5200e-04 - val_loss: 1.8814e-04 - val_MSE: 1.8814e-04\n",
      "Epoch 8/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 3.5969e-04 - MSE: 3.5969e-04 - val_loss: 1.2645e-04 - val_MSE: 1.2645e-04\n",
      "Epoch 9/100\n",
      "449/449 [==============================] - 19s 42ms/step - loss: 2.4713e-04 - MSE: 2.4713e-04 - val_loss: 8.9195e-05 - val_MSE: 8.9195e-05\n",
      "Epoch 10/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 1.7956e-04 - MSE: 1.7956e-04 - val_loss: 6.6612e-05 - val_MSE: 6.6612e-05\n",
      "Epoch 11/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 1.3062e-04 - MSE: 1.3062e-04 - val_loss: 5.0059e-05 - val_MSE: 5.0059e-05\n",
      "Epoch 12/100\n",
      "449/449 [==============================] - 19s 43ms/step - loss: 9.8472e-05 - MSE: 9.8472e-05 - val_loss: 3.6584e-05 - val_MSE: 3.6584e-05\n",
      "Epoch 13/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.6436e-05 - MSE: 7.6436e-05 - val_loss: 2.8769e-05 - val_MSE: 2.8769e-05\n",
      "Epoch 14/100\n",
      "449/449 [==============================] - 15s 34ms/step - loss: 5.7365e-05 - MSE: 5.7365e-05 - val_loss: 2.2607e-05 - val_MSE: 2.2607e-05\n",
      "Epoch 15/100\n",
      "449/449 [==============================] - 12s 26ms/step - loss: 4.5947e-05 - MSE: 4.5947e-05 - val_loss: 1.9120e-05 - val_MSE: 1.9120e-05\n",
      "Epoch 16/100\n",
      "449/449 [==============================] - 26s 57ms/step - loss: 3.6409e-05 - MSE: 3.6409e-05 - val_loss: 1.5225e-05 - val_MSE: 1.5225e-05\n",
      "Epoch 17/100\n",
      "449/449 [==============================] - 35s 79ms/step - loss: 2.9653e-05 - MSE: 2.9653e-05 - val_loss: 1.3435e-05 - val_MSE: 1.3435e-05\n",
      "Epoch 18/100\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 2.5528e-05 - MSE: 2.5528e-05 - val_loss: 1.1510e-05 - val_MSE: 1.1510e-05\n",
      "Epoch 19/100\n",
      "449/449 [==============================] - 32s 71ms/step - loss: 2.0015e-05 - MSE: 2.0015e-05 - val_loss: 9.4822e-06 - val_MSE: 9.4822e-06\n",
      "Epoch 20/100\n",
      "449/449 [==============================] - 32s 72ms/step - loss: 1.7768e-05 - MSE: 1.7768e-05 - val_loss: 8.5048e-06 - val_MSE: 8.5048e-06\n",
      "Epoch 21/100\n",
      "449/449 [==============================] - 28s 63ms/step - loss: 1.5734e-05 - MSE: 1.5734e-05 - val_loss: 8.1690e-06 - val_MSE: 8.1690e-06\n",
      "Epoch 22/100\n",
      "449/449 [==============================] - 25s 56ms/step - loss: 1.3543e-05 - MSE: 1.3543e-05 - val_loss: 7.2196e-06 - val_MSE: 7.2196e-06\n",
      "Epoch 23/100\n",
      "449/449 [==============================] - 25s 56ms/step - loss: 1.2085e-05 - MSE: 1.2085e-05 - val_loss: 6.8777e-06 - val_MSE: 6.8777e-06\n",
      "Epoch 24/100\n",
      "449/449 [==============================] - 25s 56ms/step - loss: 1.1069e-05 - MSE: 1.1069e-05 - val_loss: 6.7024e-06 - val_MSE: 6.7024e-06\n",
      "Epoch 25/100\n",
      "449/449 [==============================] - 32s 71ms/step - loss: 1.0223e-05 - MSE: 1.0223e-05 - val_loss: 6.4384e-06 - val_MSE: 6.4384e-06\n",
      "Epoch 26/100\n",
      "449/449 [==============================] - 35s 79ms/step - loss: 9.5745e-06 - MSE: 9.5745e-06 - val_loss: 6.8858e-06 - val_MSE: 6.8858e-06\n",
      "Epoch 27/100\n",
      "449/449 [==============================] - 29s 64ms/step - loss: 8.9882e-06 - MSE: 8.9882e-06 - val_loss: 6.0866e-06 - val_MSE: 6.0866e-06\n",
      "Epoch 28/100\n",
      "449/449 [==============================] - 32s 71ms/step - loss: 8.6602e-06 - MSE: 8.6602e-06 - val_loss: 6.0231e-06 - val_MSE: 6.0231e-06\n",
      "Epoch 29/100\n",
      "449/449 [==============================] - 22s 50ms/step - loss: 8.2999e-06 - MSE: 8.2999e-06 - val_loss: 5.6687e-06 - val_MSE: 5.6687e-06\n",
      "Epoch 30/100\n",
      "449/449 [==============================] - 18s 41ms/step - loss: 8.0585e-06 - MSE: 8.0585e-06 - val_loss: 5.4023e-06 - val_MSE: 5.4023e-06\n",
      "Epoch 31/100\n",
      "449/449 [==============================] - 20s 45ms/step - loss: 7.8550e-06 - MSE: 7.8550e-06 - val_loss: 5.3471e-06 - val_MSE: 5.3471e-06\n",
      "Epoch 32/100\n",
      "449/449 [==============================] - 25s 56ms/step - loss: 7.6932e-06 - MSE: 7.6932e-06 - val_loss: 5.3317e-06 - val_MSE: 5.3317e-06\n",
      "Epoch 33/100\n",
      "449/449 [==============================] - 25s 56ms/step - loss: 7.6097e-06 - MSE: 7.6097e-06 - val_loss: 5.1720e-06 - val_MSE: 5.1720e-06\n",
      "Epoch 34/100\n",
      "449/449 [==============================] - 25s 56ms/step - loss: 7.5024e-06 - MSE: 7.5024e-06 - val_loss: 5.1578e-06 - val_MSE: 5.1578e-06\n",
      "Epoch 35/100\n",
      "449/449 [==============================] - 28s 62ms/step - loss: 7.4217e-06 - MSE: 7.4217e-06 - val_loss: 5.1402e-06 - val_MSE: 5.1402e-06\n",
      "Epoch 36/100\n",
      "449/449 [==============================] - 22s 49ms/step - loss: 7.3184e-06 - MSE: 7.3184e-06 - val_loss: 5.2223e-06 - val_MSE: 5.2223e-06\n",
      "Epoch 37/100\n",
      "449/449 [==============================] - 22s 49ms/step - loss: 7.3149e-06 - MSE: 7.3149e-06 - val_loss: 5.0460e-06 - val_MSE: 5.0460e-06\n",
      "Epoch 38/100\n",
      "449/449 [==============================] - 25s 56ms/step - loss: 7.2579e-06 - MSE: 7.2579e-06 - val_loss: 5.0118e-06 - val_MSE: 5.0118e-06\n",
      "Epoch 39/100\n",
      "449/449 [==============================] - 19s 42ms/step - loss: 7.2196e-06 - MSE: 7.2196e-06 - val_loss: 4.9903e-06 - val_MSE: 4.9903e-06\n",
      "Epoch 40/100\n",
      "449/449 [==============================] - 29s 64ms/step - loss: 7.2190e-06 - MSE: 7.2190e-06 - val_loss: 4.9695e-06 - val_MSE: 4.9695e-06\n",
      "Epoch 41/100\n",
      "449/449 [==============================] - 25s 56ms/step - loss: 7.2043e-06 - MSE: 7.2043e-06 - val_loss: 4.9650e-06 - val_MSE: 4.9650e-06\n",
      "Epoch 42/100\n",
      "449/449 [==============================] - 28s 63ms/step - loss: 7.1681e-06 - MSE: 7.1681e-06 - val_loss: 4.9698e-06 - val_MSE: 4.9698e-06\n",
      "Epoch 43/100\n",
      "449/449 [==============================] - 28s 63ms/step - loss: 7.1403e-06 - MSE: 7.1403e-06 - val_loss: 4.9414e-06 - val_MSE: 4.9414e-06\n",
      "Epoch 44/100\n",
      "449/449 [==============================] - 29s 64ms/step - loss: 7.1490e-06 - MSE: 7.1490e-06 - val_loss: 4.9185e-06 - val_MSE: 4.9185e-06\n",
      "Epoch 45/100\n",
      "449/449 [==============================] - 32s 71ms/step - loss: 7.1414e-06 - MSE: 7.1414e-06 - val_loss: 4.8926e-06 - val_MSE: 4.8926e-06\n",
      "Epoch 46/100\n",
      "449/449 [==============================] - 25s 55ms/step - loss: 7.1145e-06 - MSE: 7.1145e-06 - val_loss: 4.8857e-06 - val_MSE: 4.8857e-06\n",
      "Epoch 47/100\n",
      "449/449 [==============================] - 29s 64ms/step - loss: 7.1071e-06 - MSE: 7.1071e-06 - val_loss: 4.8750e-06 - val_MSE: 4.8750e-06\n",
      "Epoch 48/100\n",
      "449/449 [==============================] - 31s 69ms/step - loss: 7.0993e-06 - MSE: 7.0993e-06 - val_loss: 4.8782e-06 - val_MSE: 4.8782e-06\n",
      "Epoch 49/100\n",
      "449/449 [==============================] - 28s 63ms/step - loss: 7.0988e-06 - MSE: 7.0988e-06 - val_loss: 4.8629e-06 - val_MSE: 4.8629e-06\n",
      "Epoch 50/100\n",
      "449/449 [==============================] - 39s 87ms/step - loss: 7.0906e-06 - MSE: 7.0906e-06 - val_loss: 4.8759e-06 - val_MSE: 4.8759e-06\n",
      "Epoch 51/100\n",
      "449/449 [==============================] - 29s 64ms/step - loss: 7.0844e-06 - MSE: 7.0844e-06 - val_loss: 4.8531e-06 - val_MSE: 4.8531e-06\n",
      "Epoch 52/100\n",
      "449/449 [==============================] - 32s 71ms/step - loss: 7.0879e-06 - MSE: 7.0879e-06 - val_loss: 4.8425e-06 - val_MSE: 4.8425e-06\n",
      "Epoch 53/100\n",
      "449/449 [==============================] - 32s 71ms/step - loss: 7.0943e-06 - MSE: 7.0943e-06 - val_loss: 4.8334e-06 - val_MSE: 4.8334e-06\n",
      "Epoch 54/100\n",
      "449/449 [==============================] - 32s 71ms/step - loss: 7.0715e-06 - MSE: 7.0715e-06 - val_loss: 4.8308e-06 - val_MSE: 4.8308e-06\n",
      "Epoch 55/100\n",
      "449/449 [==============================] - 39s 86ms/step - loss: 7.0899e-06 - MSE: 7.0899e-06 - val_loss: 4.8869e-06 - val_MSE: 4.8869e-06\n",
      "Epoch 56/100\n",
      "449/449 [==============================] - 32s 71ms/step - loss: 7.0869e-06 - MSE: 7.0869e-06 - val_loss: 4.9033e-06 - val_MSE: 4.9033e-06\n",
      "Epoch 57/100\n",
      "449/449 [==============================] - 32s 71ms/step - loss: 7.0743e-06 - MSE: 7.0743e-06 - val_loss: 4.8253e-06 - val_MSE: 4.8253e-06\n",
      "Epoch 58/100\n",
      "449/449 [==============================] - 39s 87ms/step - loss: 7.0658e-06 - MSE: 7.0658e-06 - val_loss: 4.8225e-06 - val_MSE: 4.8225e-06\n",
      "Epoch 59/100\n",
      "449/449 [==============================] - 29s 64ms/step - loss: 7.0776e-06 - MSE: 7.0776e-06 - val_loss: 4.8204e-06 - val_MSE: 4.8204e-06\n",
      "Epoch 60/100\n",
      "449/449 [==============================] - 29s 64ms/step - loss: 7.0772e-06 - MSE: 7.0772e-06 - val_loss: 4.8198e-06 - val_MSE: 4.8198e-06\n",
      "Epoch 61/100\n",
      "449/449 [==============================] - 32s 71ms/step - loss: 7.0807e-06 - MSE: 7.0807e-06 - val_loss: 4.8149e-06 - val_MSE: 4.8149e-06\n",
      "Epoch 62/100\n",
      "449/449 [==============================] - 31s 70ms/step - loss: 7.0780e-06 - MSE: 7.0780e-06 - val_loss: 4.8188e-06 - val_MSE: 4.8188e-06\n",
      "Epoch 63/100\n",
      "449/449 [==============================] - 32s 71ms/step - loss: 7.0838e-06 - MSE: 7.0838e-06 - val_loss: 4.8256e-06 - val_MSE: 4.8256e-06\n",
      "Epoch 64/100\n",
      "449/449 [==============================] - 32s 71ms/step - loss: 7.0791e-06 - MSE: 7.0791e-06 - val_loss: 4.8113e-06 - val_MSE: 4.8113e-06\n",
      "Epoch 65/100\n",
      "449/449 [==============================] - 28s 62ms/step - loss: 7.0722e-06 - MSE: 7.0722e-06 - val_loss: 4.8125e-06 - val_MSE: 4.8125e-06\n",
      "Epoch 66/100\n",
      "449/449 [==============================] - 29s 65ms/step - loss: 7.0595e-06 - MSE: 7.0595e-06 - val_loss: 4.8135e-06 - val_MSE: 4.8135e-06\n",
      "Epoch 67/100\n",
      "449/449 [==============================] - 29s 65ms/step - loss: 7.0686e-06 - MSE: 7.0686e-06 - val_loss: 4.8119e-06 - val_MSE: 4.8119e-06\n",
      "Epoch 68/100\n",
      "449/449 [==============================] - 14s 31ms/step - loss: 7.0662e-06 - MSE: 7.0662e-06 - val_loss: 4.8119e-06 - val_MSE: 4.8119e-06\n",
      "Epoch 69/100\n",
      "449/449 [==============================] - 25s 55ms/step - loss: 7.0620e-06 - MSE: 7.0620e-06 - val_loss: 4.8100e-06 - val_MSE: 4.8100e-06\n",
      "Epoch 70/100\n",
      "449/449 [==============================] - 25s 55ms/step - loss: 7.0648e-06 - MSE: 7.0648e-06 - val_loss: 4.8043e-06 - val_MSE: 4.8043e-06\n",
      "Epoch 71/100\n",
      "449/449 [==============================] - 28s 63ms/step - loss: 7.0748e-06 - MSE: 7.0748e-06 - val_loss: 4.8036e-06 - val_MSE: 4.8036e-06\n",
      "Epoch 72/100\n",
      "449/449 [==============================] - 35s 78ms/step - loss: 7.0588e-06 - MSE: 7.0588e-06 - val_loss: 4.8018e-06 - val_MSE: 4.8018e-06\n",
      "Epoch 73/100\n",
      "449/449 [==============================] - 28s 63ms/step - loss: 7.0592e-06 - MSE: 7.0592e-06 - val_loss: 4.8023e-06 - val_MSE: 4.8023e-06\n",
      "Epoch 74/100\n",
      "449/449 [==============================] - 17s 38ms/step - loss: 7.0712e-06 - MSE: 7.0712e-06 - val_loss: 4.8020e-06 - val_MSE: 4.8020e-06\n",
      "Epoch 75/100\n",
      "449/449 [==============================] - 14s 31ms/step - loss: 7.0615e-06 - MSE: 7.0615e-06 - val_loss: 4.8005e-06 - val_MSE: 4.8005e-06\n",
      "Epoch 76/100\n",
      "449/449 [==============================] - 16s 35ms/step - loss: 7.0722e-06 - MSE: 7.0722e-06 - val_loss: 4.8029e-06 - val_MSE: 4.8029e-06\n",
      "Epoch 77/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0620e-06 - MSE: 7.0620e-06 - val_loss: 4.8035e-06 - val_MSE: 4.8035e-06\n",
      "Epoch 78/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0565e-06 - MSE: 7.0565e-06 - val_loss: 4.7990e-06 - val_MSE: 4.7990e-06\n",
      "Epoch 79/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0629e-06 - MSE: 7.0629e-06 - val_loss: 4.8005e-06 - val_MSE: 4.8005e-06\n",
      "Epoch 80/100\n",
      "449/449 [==============================] - 13s 29ms/step - loss: 7.0652e-06 - MSE: 7.0652e-06 - val_loss: 4.8006e-06 - val_MSE: 4.8006e-06\n",
      "Epoch 81/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0594e-06 - MSE: 7.0594e-06 - val_loss: 4.7983e-06 - val_MSE: 4.7983e-06\n",
      "Epoch 82/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0655e-06 - MSE: 7.0655e-06 - val_loss: 4.8009e-06 - val_MSE: 4.8009e-06\n",
      "Epoch 83/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0682e-06 - MSE: 7.0682e-06 - val_loss: 4.7990e-06 - val_MSE: 4.7990e-06\n",
      "Epoch 84/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0673e-06 - MSE: 7.0673e-06 - val_loss: 4.7987e-06 - val_MSE: 4.7987e-06\n",
      "Epoch 85/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0617e-06 - MSE: 7.0617e-06 - val_loss: 4.8002e-06 - val_MSE: 4.8002e-06\n",
      "Epoch 86/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0587e-06 - MSE: 7.0587e-06 - val_loss: 4.7982e-06 - val_MSE: 4.7982e-06\n",
      "Epoch 87/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0658e-06 - MSE: 7.0658e-06 - val_loss: 4.8004e-06 - val_MSE: 4.8004e-06\n",
      "Epoch 88/100\n",
      "449/449 [==============================] - 19s 43ms/step - loss: 7.0661e-06 - MSE: 7.0661e-06 - val_loss: 4.7992e-06 - val_MSE: 4.7992e-06\n",
      "Epoch 89/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0615e-06 - MSE: 7.0615e-06 - val_loss: 4.8020e-06 - val_MSE: 4.8020e-06\n",
      "Epoch 90/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0783e-06 - MSE: 7.0783e-06 - val_loss: 4.7982e-06 - val_MSE: 4.7982e-06\n",
      "Epoch 91/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0568e-06 - MSE: 7.0568e-06 - val_loss: 4.7993e-06 - val_MSE: 4.7993e-06\n",
      "Epoch 92/100\n",
      "449/449 [==============================] - 19s 42ms/step - loss: 7.0653e-06 - MSE: 7.0653e-06 - val_loss: 4.7982e-06 - val_MSE: 4.7982e-06\n",
      "Epoch 93/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0604e-06 - MSE: 7.0604e-06 - val_loss: 4.8028e-06 - val_MSE: 4.8028e-06\n",
      "Epoch 94/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0612e-06 - MSE: 7.0612e-06 - val_loss: 4.8019e-06 - val_MSE: 4.8019e-06\n",
      "Epoch 95/100\n",
      "449/449 [==============================] - 16s 36ms/step - loss: 7.0543e-06 - MSE: 7.0543e-06 - val_loss: 4.7980e-06 - val_MSE: 4.7980e-06\n",
      "Epoch 96/100\n",
      "449/449 [==============================] - 22s 50ms/step - loss: 7.0653e-06 - MSE: 7.0653e-06 - val_loss: 4.8024e-06 - val_MSE: 4.8024e-06\n",
      "Epoch 97/100\n",
      "449/449 [==============================] - 25s 56ms/step - loss: 7.0635e-06 - MSE: 7.0635e-06 - val_loss: 4.7972e-06 - val_MSE: 4.7972e-06\n",
      "Epoch 98/100\n",
      "449/449 [==============================] - 32s 71ms/step - loss: 7.0726e-06 - MSE: 7.0726e-06 - val_loss: 4.8270e-06 - val_MSE: 4.8270e-06\n",
      "Epoch 99/100\n",
      "449/449 [==============================] - 25s 56ms/step - loss: 7.0663e-06 - MSE: 7.0663e-06 - val_loss: 4.7966e-06 - val_MSE: 4.7966e-06\n",
      "Epoch 100/100\n",
      "449/449 [==============================] - 22s 49ms/step - loss: 7.0636e-06 - MSE: 7.0636e-06 - val_loss: 4.7981e-06 - val_MSE: 4.7981e-06\n",
      "INFO:tensorflow:Assets written to: saved_model/NFE_120_log_s302_sigmoid_4layers\\assets\n",
      "Dataset NFE has been trained and saved.\n",
      "(7, 120)\n",
      "(1,)\n",
      "Epoch 1/100\n",
      "348/348 [==============================] - 15s 35ms/step - loss: 0.3135 - MSE: 0.3135 - val_loss: 0.0683 - val_MSE: 0.0683\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 0.0489 - MSE: 0.0489 - val_loss: 0.0175 - val_MSE: 0.0175\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 19s 56ms/step - loss: 0.0172 - MSE: 0.0172 - val_loss: 0.0070 - val_MSE: 0.0070\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 24s 69ms/step - loss: 0.0083 - MSE: 0.0083 - val_loss: 0.0036 - val_MSE: 0.0036\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 0.0049 - MSE: 0.0049 - val_loss: 0.0020 - val_MSE: 0.0020\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 0.0031 - MSE: 0.0031 - val_loss: 0.0013 - val_MSE: 0.0013\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 0.0021 - MSE: 0.0021 - val_loss: 8.3092e-04 - val_MSE: 8.3092e-04\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 16s 47ms/step - loss: 0.0015 - MSE: 0.0015 - val_loss: 5.6772e-04 - val_MSE: 5.6772e-04\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 24s 70ms/step - loss: 0.0011 - MSE: 0.0011 - val_loss: 3.9568e-04 - val_MSE: 3.9568e-04\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 12s 34ms/step - loss: 8.2567e-04 - MSE: 8.2567e-04 - val_loss: 2.7896e-04 - val_MSE: 2.7896e-04\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 6.1526e-04 - MSE: 6.1526e-04 - val_loss: 2.0243e-04 - val_MSE: 2.0243e-04\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 15s 34ms/step - loss: 4.7445e-04 - MSE: 4.7445e-04 - val_loss: 1.5133e-04 - val_MSE: 1.5133e-04\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 3.5665e-04 - MSE: 3.5665e-04 - val_loss: 1.1386e-04 - val_MSE: 1.1386e-04\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 2.7655e-04 - MSE: 2.7655e-04 - val_loss: 8.4727e-05 - val_MSE: 8.4727e-05\n",
      "Epoch 15/100\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 2.1368e-04 - MSE: 2.1368e-04 - val_loss: 6.5066e-05 - val_MSE: 6.5066e-05\n",
      "Epoch 16/100\n",
      "348/348 [==============================] - 15s 44ms/step - loss: 1.7321e-04 - MSE: 1.7321e-04 - val_loss: 5.0706e-05 - val_MSE: 5.0706e-05\n",
      "Epoch 17/100\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 1.4229e-04 - MSE: 1.4229e-04 - val_loss: 3.8735e-05 - val_MSE: 3.8735e-05\n",
      "Epoch 18/100\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 1.1550e-04 - MSE: 1.1550e-04 - val_loss: 3.0327e-05 - val_MSE: 3.0327e-05\n",
      "Epoch 19/100\n",
      "348/348 [==============================] - 16s 45ms/step - loss: 8.8741e-05 - MSE: 8.8741e-05 - val_loss: 2.4730e-05 - val_MSE: 2.4730e-05\n",
      "Epoch 20/100\n",
      "348/348 [==============================] - 36s 104ms/step - loss: 7.2004e-05 - MSE: 7.2004e-05 - val_loss: 2.0085e-05 - val_MSE: 2.0085e-05\n",
      "Epoch 21/100\n",
      "348/348 [==============================] - 31s 88ms/step - loss: 6.0638e-05 - MSE: 6.0638e-05 - val_loss: 1.6199e-05 - val_MSE: 1.6199e-05\n",
      "Epoch 22/100\n",
      "348/348 [==============================] - 24s 68ms/step - loss: 4.6267e-05 - MSE: 4.6267e-05 - val_loss: 1.2782e-05 - val_MSE: 1.2782e-05\n",
      "Epoch 23/100\n",
      "348/348 [==============================] - 30s 88ms/step - loss: 4.0095e-05 - MSE: 4.0095e-05 - val_loss: 1.1675e-05 - val_MSE: 1.1675e-05\n",
      "Epoch 24/100\n",
      "348/348 [==============================] - 20s 57ms/step - loss: 3.2357e-05 - MSE: 3.2357e-05 - val_loss: 9.1096e-06 - val_MSE: 9.1096e-06\n",
      "Epoch 25/100\n",
      "348/348 [==============================] - 12s 36ms/step - loss: 2.7587e-05 - MSE: 2.7587e-05 - val_loss: 7.5536e-06 - val_MSE: 7.5536e-06\n",
      "Epoch 26/100\n",
      "348/348 [==============================] - 12s 36ms/step - loss: 2.2675e-05 - MSE: 2.2675e-05 - val_loss: 6.5790e-06 - val_MSE: 6.5790e-06\n",
      "Epoch 27/100\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 1.9077e-05 - MSE: 1.9077e-05 - val_loss: 5.5691e-06 - val_MSE: 5.5691e-06\n",
      "Epoch 28/100\n",
      "348/348 [==============================] - 15s 44ms/step - loss: 1.6667e-05 - MSE: 1.6667e-05 - val_loss: 4.5761e-06 - val_MSE: 4.5761e-06\n",
      "Epoch 29/100\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 1.3479e-05 - MSE: 1.3479e-05 - val_loss: 3.9455e-06 - val_MSE: 3.9455e-06\n",
      "Epoch 30/100\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 1.2270e-05 - MSE: 1.2270e-05 - val_loss: 3.4563e-06 - val_MSE: 3.4563e-06\n",
      "Epoch 31/100\n",
      "348/348 [==============================] - 15s 43ms/step - loss: 1.0136e-05 - MSE: 1.0136e-05 - val_loss: 3.1576e-06 - val_MSE: 3.1576e-06\n",
      "Epoch 32/100\n",
      "348/348 [==============================] - 11s 30ms/step - loss: 8.6477e-06 - MSE: 8.6477e-06 - val_loss: 2.7098e-06 - val_MSE: 2.7098e-06\n",
      "Epoch 33/100\n",
      "348/348 [==============================] - 10s 29ms/step - loss: 7.2762e-06 - MSE: 7.2762e-06 - val_loss: 2.3908e-06 - val_MSE: 2.3908e-06\n",
      "Epoch 34/100\n",
      "348/348 [==============================] - 10s 30ms/step - loss: 6.4985e-06 - MSE: 6.4985e-06 - val_loss: 2.2128e-06 - val_MSE: 2.2128e-06\n",
      "Epoch 35/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 5.5509e-06 - MSE: 5.5509e-06 - val_loss: 1.9223e-06 - val_MSE: 1.9223e-06\n",
      "Epoch 36/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 4.8349e-06 - MSE: 4.8349e-06 - val_loss: 1.6905e-06 - val_MSE: 1.6905e-06\n",
      "Epoch 37/100\n",
      "348/348 [==============================] - 16s 47ms/step - loss: 4.2043e-06 - MSE: 4.2043e-06 - val_loss: 1.6218e-06 - val_MSE: 1.6218e-06\n",
      "Epoch 38/100\n",
      "348/348 [==============================] - 23s 59ms/step - loss: 3.8970e-06 - MSE: 3.8970e-06 - val_loss: 1.4477e-06 - val_MSE: 1.4477e-06\n",
      "Epoch 39/100\n",
      "348/348 [==============================] - 14s 40ms/step - loss: 3.4787e-06 - MSE: 3.4787e-06 - val_loss: 1.3601e-06 - val_MSE: 1.3601e-06\n",
      "Epoch 40/100\n",
      "348/348 [==============================] - 9s 27ms/step - loss: 3.0001e-06 - MSE: 3.0001e-06 - val_loss: 1.4086e-06 - val_MSE: 1.4086e-06\n",
      "Epoch 41/100\n",
      "348/348 [==============================] - 10s 27ms/step - loss: 2.7613e-06 - MSE: 2.7613e-06 - val_loss: 1.2207e-06 - val_MSE: 1.2207e-06\n",
      "Epoch 42/100\n",
      "348/348 [==============================] - 19s 56ms/step - loss: 2.5825e-06 - MSE: 2.5825e-06 - val_loss: 1.2061e-06 - val_MSE: 1.2061e-06\n",
      "Epoch 43/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 2.4673e-06 - MSE: 2.4673e-06 - val_loss: 1.1305e-06 - val_MSE: 1.1305e-06\n",
      "Epoch 44/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 2.2834e-06 - MSE: 2.2834e-06 - val_loss: 1.1603e-06 - val_MSE: 1.1603e-06\n",
      "Epoch 45/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 2.1904e-06 - MSE: 2.1904e-06 - val_loss: 1.0715e-06 - val_MSE: 1.0715e-06\n",
      "Epoch 46/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 2.0323e-06 - MSE: 2.0323e-06 - val_loss: 1.1051e-06 - val_MSE: 1.1051e-06\n",
      "Epoch 47/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.9246e-06 - MSE: 1.9246e-06 - val_loss: 1.0422e-06 - val_MSE: 1.0422e-06\n",
      "Epoch 48/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.8891e-06 - MSE: 1.8891e-06 - val_loss: 1.0270e-06 - val_MSE: 1.0270e-06\n",
      "Epoch 49/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.8291e-06 - MSE: 1.8291e-06 - val_loss: 1.0756e-06 - val_MSE: 1.0756e-06\n",
      "Epoch 50/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.7877e-06 - MSE: 1.7877e-06 - val_loss: 1.0111e-06 - val_MSE: 1.0111e-06\n",
      "Epoch 51/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.7713e-06 - MSE: 1.7713e-06 - val_loss: 1.0094e-06 - val_MSE: 1.0094e-06\n",
      "Epoch 52/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.7271e-06 - MSE: 1.7271e-06 - val_loss: 1.0224e-06 - val_MSE: 1.0224e-06\n",
      "Epoch 53/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.7083e-06 - MSE: 1.7083e-06 - val_loss: 9.9422e-07 - val_MSE: 9.9422e-07\n",
      "Epoch 54/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.6907e-06 - MSE: 1.6907e-06 - val_loss: 9.8670e-07 - val_MSE: 9.8670e-07\n",
      "Epoch 55/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.6435e-06 - MSE: 1.6435e-06 - val_loss: 9.7915e-07 - val_MSE: 9.7915e-07\n",
      "Epoch 56/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.6375e-06 - MSE: 1.6375e-06 - val_loss: 9.9788e-07 - val_MSE: 9.9788e-07\n",
      "Epoch 57/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.6089e-06 - MSE: 1.6089e-06 - val_loss: 9.7253e-07 - val_MSE: 9.7253e-07\n",
      "Epoch 58/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.6213e-06 - MSE: 1.6213e-06 - val_loss: 9.9496e-07 - val_MSE: 9.9496e-07\n",
      "Epoch 59/100\n",
      "348/348 [==============================] - 10s 29ms/step - loss: 1.6074e-06 - MSE: 1.6074e-06 - val_loss: 1.0164e-06 - val_MSE: 1.0164e-06\n",
      "Epoch 60/100\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.5888e-06 - MSE: 1.5888e-06 - val_loss: 9.9175e-07 - val_MSE: 9.9175e-07\n",
      "Epoch 61/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.5915e-06 - MSE: 1.5915e-06 - val_loss: 9.7723e-07 - val_MSE: 9.7723e-07\n",
      "Epoch 62/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.5782e-06 - MSE: 1.5782e-06 - val_loss: 9.6161e-07 - val_MSE: 9.6161e-07\n",
      "Epoch 63/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.5728e-06 - MSE: 1.5728e-06 - val_loss: 1.0173e-06 - val_MSE: 1.0173e-06\n",
      "Epoch 64/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.5742e-06 - MSE: 1.5742e-06 - val_loss: 1.0039e-06 - val_MSE: 1.0039e-06\n",
      "Epoch 65/100\n",
      "348/348 [==============================] - 10s 29ms/step - loss: 1.5640e-06 - MSE: 1.5640e-06 - val_loss: 9.8123e-07 - val_MSE: 9.8123e-07\n",
      "Epoch 66/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.5534e-06 - MSE: 1.5534e-06 - val_loss: 9.7290e-07 - val_MSE: 9.7290e-07\n",
      "Epoch 67/100\n",
      "348/348 [==============================] - 10s 28ms/step - loss: 1.5580e-06 - MSE: 1.5580e-06 - val_loss: 9.6378e-07 - val_MSE: 9.6378e-07\n",
      "INFO:tensorflow:Assets written to: saved_model/NOC_120_log_s302_sigmoid_4layers\\assets\n",
      "Dataset NOC has been trained and saved.\n",
      "(7, 120)\n",
      "(1,)\n",
      "Epoch 1/100\n",
      "530/530 [==============================] - 17s 26ms/step - loss: 2.2227 - MSE: 2.2227 - val_loss: 0.0849 - val_MSE: 0.0849\n",
      "Epoch 2/100\n",
      "530/530 [==============================] - 16s 30ms/step - loss: 0.0846 - MSE: 0.0846 - val_loss: 0.0504 - val_MSE: 0.0504\n",
      "Epoch 3/100\n",
      "530/530 [==============================] - 14s 26ms/step - loss: 0.0489 - MSE: 0.0489 - val_loss: 0.0291 - val_MSE: 0.0291\n",
      "Epoch 4/100\n",
      "530/530 [==============================] - 7s 13ms/step - loss: 0.0301 - MSE: 0.0301 - val_loss: 0.0173 - val_MSE: 0.0173\n",
      "Epoch 5/100\n",
      "530/530 [==============================] - 7s 13ms/step - loss: 0.0191 - MSE: 0.0191 - val_loss: 0.0097 - val_MSE: 0.0097\n",
      "Epoch 6/100\n",
      "530/530 [==============================] - 9s 17ms/step - loss: 0.0115 - MSE: 0.0115 - val_loss: 0.0052 - val_MSE: 0.0052\n",
      "Epoch 7/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.0074 - MSE: 0.0074 - val_loss: 0.0029 - val_MSE: 0.0029\n",
      "Epoch 8/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.0045 - MSE: 0.0045 - val_loss: 0.0015 - val_MSE: 0.0015\n",
      "Epoch 9/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.0029 - MSE: 0.0029 - val_loss: 8.9943e-04 - val_MSE: 8.9943e-04\n",
      "Epoch 10/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.0019 - MSE: 0.0019 - val_loss: 5.8507e-04 - val_MSE: 5.8507e-04\n",
      "Epoch 11/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.0013 - MSE: 0.0013 - val_loss: 3.9785e-04 - val_MSE: 3.9785e-04\n",
      "Epoch 12/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 9.1540e-04 - MSE: 9.1540e-04 - val_loss: 2.9724e-04 - val_MSE: 2.9724e-04\n",
      "Epoch 13/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 6.5067e-04 - MSE: 6.5067e-04 - val_loss: 2.1840e-04 - val_MSE: 2.1840e-04\n",
      "Epoch 14/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 4.6864e-04 - MSE: 4.6864e-04 - val_loss: 1.5847e-04 - val_MSE: 1.5847e-04\n",
      "Epoch 15/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.3383e-04 - MSE: 3.3383e-04 - val_loss: 1.2364e-04 - val_MSE: 1.2364e-04\n",
      "Epoch 16/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 2.4532e-04 - MSE: 2.4532e-04 - val_loss: 8.5463e-05 - val_MSE: 8.5463e-05\n",
      "Epoch 17/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 1.6907e-04 - MSE: 1.6907e-04 - val_loss: 5.8074e-05 - val_MSE: 5.8074e-05\n",
      "Epoch 18/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 1.2008e-04 - MSE: 1.2008e-04 - val_loss: 3.8712e-05 - val_MSE: 3.8712e-05\n",
      "Epoch 19/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 8.9882e-05 - MSE: 8.9882e-05 - val_loss: 2.7091e-05 - val_MSE: 2.7091e-05\n",
      "Epoch 20/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 6.2563e-05 - MSE: 6.2563e-05 - val_loss: 1.9852e-05 - val_MSE: 1.9852e-05\n",
      "Epoch 21/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 4.7349e-05 - MSE: 4.7349e-05 - val_loss: 1.6093e-05 - val_MSE: 1.6093e-05\n",
      "Epoch 22/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.6952e-05 - MSE: 3.6952e-05 - val_loss: 1.2919e-05 - val_MSE: 1.2919e-05\n",
      "Epoch 23/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 2.5397e-05 - MSE: 2.5397e-05 - val_loss: 1.0427e-05 - val_MSE: 1.0427e-05\n",
      "Epoch 24/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 1.8941e-05 - MSE: 1.8941e-05 - val_loss: 8.9766e-06 - val_MSE: 8.9766e-06\n",
      "Epoch 25/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 1.5738e-05 - MSE: 1.5738e-05 - val_loss: 7.7736e-06 - val_MSE: 7.7736e-06\n",
      "Epoch 26/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 1.2293e-05 - MSE: 1.2293e-05 - val_loss: 7.3240e-06 - val_MSE: 7.3240e-06\n",
      "Epoch 27/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 1.0063e-05 - MSE: 1.0063e-05 - val_loss: 6.8655e-06 - val_MSE: 6.8655e-06\n",
      "Epoch 28/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 8.4372e-06 - MSE: 8.4372e-06 - val_loss: 6.2013e-06 - val_MSE: 6.2013e-06\n",
      "Epoch 29/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 7.0306e-06 - MSE: 7.0306e-06 - val_loss: 6.0205e-06 - val_MSE: 6.0205e-06\n",
      "Epoch 30/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 6.0325e-06 - MSE: 6.0325e-06 - val_loss: 5.7614e-06 - val_MSE: 5.7614e-06\n",
      "Epoch 31/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 5.3216e-06 - MSE: 5.3216e-06 - val_loss: 5.6287e-06 - val_MSE: 5.6287e-06\n",
      "Epoch 32/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 4.6418e-06 - MSE: 4.6418e-06 - val_loss: 5.6365e-06 - val_MSE: 5.6365e-06\n",
      "Epoch 33/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 4.4250e-06 - MSE: 4.4250e-06 - val_loss: 5.6229e-06 - val_MSE: 5.6229e-06\n",
      "Epoch 34/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 4.2947e-06 - MSE: 4.2947e-06 - val_loss: 5.4863e-06 - val_MSE: 5.4863e-06\n",
      "Epoch 35/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.7371e-06 - MSE: 3.7371e-06 - val_loss: 5.4109e-06 - val_MSE: 5.4109e-06\n",
      "Epoch 36/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.6411e-06 - MSE: 3.6411e-06 - val_loss: 5.6393e-06 - val_MSE: 5.6393e-06\n",
      "Epoch 37/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.5060e-06 - MSE: 3.5060e-06 - val_loss: 5.3643e-06 - val_MSE: 5.3643e-06\n",
      "Epoch 38/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.3824e-06 - MSE: 3.3824e-06 - val_loss: 5.3448e-06 - val_MSE: 5.3448e-06\n",
      "Epoch 39/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.3592e-06 - MSE: 3.3592e-06 - val_loss: 5.3637e-06 - val_MSE: 5.3637e-06\n",
      "Epoch 40/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.3388e-06 - MSE: 3.3388e-06 - val_loss: 5.3360e-06 - val_MSE: 5.3360e-06\n",
      "Epoch 41/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.2516e-06 - MSE: 3.2516e-06 - val_loss: 5.3748e-06 - val_MSE: 5.3748e-06\n",
      "Epoch 42/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.2140e-06 - MSE: 3.2140e-06 - val_loss: 5.3204e-06 - val_MSE: 5.3204e-06\n",
      "Epoch 43/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.1933e-06 - MSE: 3.1933e-06 - val_loss: 5.3501e-06 - val_MSE: 5.3501e-06\n",
      "Epoch 44/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.1658e-06 - MSE: 3.1658e-06 - val_loss: 5.3291e-06 - val_MSE: 5.3291e-06\n",
      "Epoch 45/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.1424e-06 - MSE: 3.1424e-06 - val_loss: 5.3215e-06 - val_MSE: 5.3215e-06\n",
      "Epoch 46/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.1465e-06 - MSE: 3.1465e-06 - val_loss: 5.3712e-06 - val_MSE: 5.3712e-06\n",
      "Epoch 47/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.1170e-06 - MSE: 3.1170e-06 - val_loss: 5.3086e-06 - val_MSE: 5.3086e-06\n",
      "Epoch 48/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.1163e-06 - MSE: 3.1163e-06 - val_loss: 5.3316e-06 - val_MSE: 5.3316e-06\n",
      "Epoch 49/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.1278e-06 - MSE: 3.1278e-06 - val_loss: 5.3114e-06 - val_MSE: 5.3114e-06\n",
      "Epoch 50/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.0971e-06 - MSE: 3.0971e-06 - val_loss: 5.3306e-06 - val_MSE: 5.3306e-06\n",
      "Epoch 51/100\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 3.1070e-06 - MSE: 3.1070e-06 - val_loss: 5.3241e-06 - val_MSE: 5.3241e-06\n",
      "Epoch 52/100\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 3.0949e-06 - MSE: 3.0949e-06 - val_loss: 5.3364e-06 - val_MSE: 5.3364e-06\n",
      "INFO:tensorflow:Assets written to: saved_model/PBR_120_log_s302_sigmoid_4layers\\assets\n",
      "Dataset PBR has been trained and saved.\n",
      "(7, 120)\n",
      "(1,)\n",
      "Epoch 1/100\n",
      "561/561 [==============================] - 7s 9ms/step - loss: 0.3911 - MSE: 0.3911 - val_loss: 0.0802 - val_MSE: 0.0802\n",
      "Epoch 2/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.0594 - MSE: 0.0594 - val_loss: 0.0211 - val_MSE: 0.0211\n",
      "Epoch 3/100\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 0.0169 - MSE: 0.0169 - val_loss: 0.0052 - val_MSE: 0.0052\n",
      "Epoch 4/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.0059 - MSE: 0.0059 - val_loss: 0.0028 - val_MSE: 0.0028\n",
      "Epoch 5/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.0031 - MSE: 0.0031 - val_loss: 0.0017 - val_MSE: 0.0017\n",
      "Epoch 6/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.0018 - MSE: 0.0018 - val_loss: 0.0010 - val_MSE: 0.0010\n",
      "Epoch 7/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.0011 - MSE: 0.0011 - val_loss: 6.2101e-04 - val_MSE: 6.2101e-04\n",
      "Epoch 8/100\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 7.4004e-04 - MSE: 7.4004e-04 - val_loss: 3.6620e-04 - val_MSE: 3.6620e-04\n",
      "Epoch 9/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 4.8215e-04 - MSE: 4.8215e-04 - val_loss: 2.3471e-04 - val_MSE: 2.3471e-04\n",
      "Epoch 10/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1810e-04 - MSE: 3.1810e-04 - val_loss: 1.5732e-04 - val_MSE: 1.5732e-04\n",
      "Epoch 11/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 2.1877e-04 - MSE: 2.1877e-04 - val_loss: 1.1127e-04 - val_MSE: 1.1127e-04\n",
      "Epoch 12/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 1.5164e-04 - MSE: 1.5164e-04 - val_loss: 6.7115e-05 - val_MSE: 6.7115e-05\n",
      "Epoch 13/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 1.0547e-04 - MSE: 1.0547e-04 - val_loss: 4.7914e-05 - val_MSE: 4.7914e-05\n",
      "Epoch 14/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 7.3723e-05 - MSE: 7.3723e-05 - val_loss: 3.6426e-05 - val_MSE: 3.6426e-05\n",
      "Epoch 15/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 5.6738e-05 - MSE: 5.6738e-05 - val_loss: 2.1836e-05 - val_MSE: 2.1836e-05\n",
      "Epoch 16/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 4.0662e-05 - MSE: 4.0662e-05 - val_loss: 1.9461e-05 - val_MSE: 1.9461e-05\n",
      "Epoch 17/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 2.9791e-05 - MSE: 2.9791e-05 - val_loss: 1.3856e-05 - val_MSE: 1.3856e-05\n",
      "Epoch 18/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 2.2335e-05 - MSE: 2.2335e-05 - val_loss: 1.0229e-05 - val_MSE: 1.0229e-05\n",
      "Epoch 19/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 1.7081e-05 - MSE: 1.7081e-05 - val_loss: 7.8581e-06 - val_MSE: 7.8581e-06\n",
      "Epoch 20/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 1.4081e-05 - MSE: 1.4081e-05 - val_loss: 6.3153e-06 - val_MSE: 6.3153e-06\n",
      "Epoch 21/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 1.1024e-05 - MSE: 1.1024e-05 - val_loss: 5.2302e-06 - val_MSE: 5.2302e-06\n",
      "Epoch 22/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 8.9434e-06 - MSE: 8.9434e-06 - val_loss: 4.2424e-06 - val_MSE: 4.2424e-06\n",
      "Epoch 23/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 7.4553e-06 - MSE: 7.4553e-06 - val_loss: 3.6921e-06 - val_MSE: 3.6921e-06\n",
      "Epoch 24/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 6.3128e-06 - MSE: 6.3128e-06 - val_loss: 3.5646e-06 - val_MSE: 3.5646e-06\n",
      "Epoch 25/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 5.5558e-06 - MSE: 5.5558e-06 - val_loss: 3.3576e-06 - val_MSE: 3.3576e-06\n",
      "Epoch 26/100\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 5.0486e-06 - MSE: 5.0486e-06 - val_loss: 2.9078e-06 - val_MSE: 2.9078e-06\n",
      "Epoch 27/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 4.5471e-06 - MSE: 4.5471e-06 - val_loss: 2.7823e-06 - val_MSE: 2.7823e-06\n",
      "Epoch 28/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 4.1615e-06 - MSE: 4.1615e-06 - val_loss: 2.6879e-06 - val_MSE: 2.6879e-06\n",
      "Epoch 29/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.9735e-06 - MSE: 3.9735e-06 - val_loss: 2.6054e-06 - val_MSE: 2.6054e-06\n",
      "Epoch 30/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.8011e-06 - MSE: 3.8011e-06 - val_loss: 2.6271e-06 - val_MSE: 2.6271e-06\n",
      "Epoch 31/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.6281e-06 - MSE: 3.6281e-06 - val_loss: 2.5012e-06 - val_MSE: 2.5012e-06\n",
      "Epoch 32/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.5358e-06 - MSE: 3.5358e-06 - val_loss: 2.4958e-06 - val_MSE: 2.4958e-06\n",
      "Epoch 33/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.4173e-06 - MSE: 3.4173e-06 - val_loss: 2.4386e-06 - val_MSE: 2.4386e-06\n",
      "Epoch 34/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.3868e-06 - MSE: 3.3868e-06 - val_loss: 2.4211e-06 - val_MSE: 2.4211e-06\n",
      "Epoch 35/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.3307e-06 - MSE: 3.3307e-06 - val_loss: 2.4260e-06 - val_MSE: 2.4260e-06\n",
      "Epoch 36/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.2858e-06 - MSE: 3.2858e-06 - val_loss: 2.3912e-06 - val_MSE: 2.3912e-06\n",
      "Epoch 37/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.2708e-06 - MSE: 3.2708e-06 - val_loss: 2.3911e-06 - val_MSE: 2.3911e-06\n",
      "Epoch 38/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.2457e-06 - MSE: 3.2457e-06 - val_loss: 2.3736e-06 - val_MSE: 2.3736e-06\n",
      "Epoch 39/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.2159e-06 - MSE: 3.2159e-06 - val_loss: 2.3685e-06 - val_MSE: 2.3685e-06\n",
      "Epoch 40/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.2084e-06 - MSE: 3.2084e-06 - val_loss: 2.3629e-06 - val_MSE: 2.3629e-06\n",
      "Epoch 41/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.2044e-06 - MSE: 3.2044e-06 - val_loss: 2.3711e-06 - val_MSE: 2.3711e-06\n",
      "Epoch 42/100\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 3.1951e-06 - MSE: 3.1951e-06 - val_loss: 2.3566e-06 - val_MSE: 2.3566e-06\n",
      "Epoch 43/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1991e-06 - MSE: 3.1991e-06 - val_loss: 2.3567e-06 - val_MSE: 2.3567e-06\n",
      "Epoch 44/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1798e-06 - MSE: 3.1798e-06 - val_loss: 2.3547e-06 - val_MSE: 2.3547e-06\n",
      "Epoch 45/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1792e-06 - MSE: 3.1792e-06 - val_loss: 2.3528e-06 - val_MSE: 2.3528e-06\n",
      "Epoch 46/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1702e-06 - MSE: 3.1702e-06 - val_loss: 2.3494e-06 - val_MSE: 2.3494e-06\n",
      "Epoch 47/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1652e-06 - MSE: 3.1652e-06 - val_loss: 2.3475e-06 - val_MSE: 2.3475e-06\n",
      "Epoch 48/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1650e-06 - MSE: 3.1650e-06 - val_loss: 2.3504e-06 - val_MSE: 2.3504e-06\n",
      "Epoch 49/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1629e-06 - MSE: 3.1629e-06 - val_loss: 2.3471e-06 - val_MSE: 2.3471e-06\n",
      "Epoch 50/100\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 3.1601e-06 - MSE: 3.1601e-06 - val_loss: 2.3452e-06 - val_MSE: 2.3452e-06\n",
      "Epoch 51/100\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 3.1612e-06 - MSE: 3.1612e-06 - val_loss: 2.3444e-06 - val_MSE: 2.3444e-06\n",
      "Epoch 52/100\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 3.1580e-06 - MSE: 3.1580e-06 - val_loss: 2.3440e-06 - val_MSE: 2.3440e-06\n",
      "Epoch 53/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1564e-06 - MSE: 3.1564e-06 - val_loss: 2.3435e-06 - val_MSE: 2.3435e-06\n",
      "Epoch 54/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1584e-06 - MSE: 3.1584e-06 - val_loss: 2.3450e-06 - val_MSE: 2.3450e-06\n",
      "Epoch 55/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1570e-06 - MSE: 3.1570e-06 - val_loss: 2.3430e-06 - val_MSE: 2.3430e-06\n",
      "Epoch 56/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1528e-06 - MSE: 3.1528e-06 - val_loss: 2.3432e-06 - val_MSE: 2.3432e-06\n",
      "Epoch 57/100\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 3.1534e-06 - MSE: 3.1534e-06 - val_loss: 2.3429e-06 - val_MSE: 2.3429e-06\n",
      "Epoch 58/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1552e-06 - MSE: 3.1552e-06 - val_loss: 2.3443e-06 - val_MSE: 2.3443e-06\n",
      "Epoch 59/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1578e-06 - MSE: 3.1578e-06 - val_loss: 2.3422e-06 - val_MSE: 2.3422e-06\n",
      "Epoch 60/100\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 3.1528e-06 - MSE: 3.1528e-06 - val_loss: 2.3423e-06 - val_MSE: 2.3423e-06\n",
      "Epoch 61/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1559e-06 - MSE: 3.1559e-06 - val_loss: 2.3427e-06 - val_MSE: 2.3427e-06\n",
      "Epoch 62/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1490e-06 - MSE: 3.1490e-06 - val_loss: 2.3416e-06 - val_MSE: 2.3416e-06\n",
      "Epoch 63/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1511e-06 - MSE: 3.1511e-06 - val_loss: 2.3413e-06 - val_MSE: 2.3413e-06\n",
      "Epoch 64/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1508e-06 - MSE: 3.1508e-06 - val_loss: 2.3418e-06 - val_MSE: 2.3418e-06\n",
      "Epoch 65/100\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 3.1502e-06 - MSE: 3.1502e-06 - val_loss: 2.3418e-06 - val_MSE: 2.3418e-06\n",
      "Epoch 66/100\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 3.1494e-06 - MSE: 3.1494e-06 - val_loss: 2.3413e-06 - val_MSE: 2.3413e-06\n",
      "Epoch 67/100\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 3.1466e-06 - MSE: 3.1466e-06 - val_loss: 2.3411e-06 - val_MSE: 2.3411e-06\n",
      "Epoch 68/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1497e-06 - MSE: 3.1497e-06 - val_loss: 2.3409e-06 - val_MSE: 2.3409e-06\n",
      "Epoch 69/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1506e-06 - MSE: 3.1506e-06 - val_loss: 2.3429e-06 - val_MSE: 2.3429e-06\n",
      "Epoch 70/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1498e-06 - MSE: 3.1498e-06 - val_loss: 2.3409e-06 - val_MSE: 2.3409e-06\n",
      "Epoch 71/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1501e-06 - MSE: 3.1501e-06 - val_loss: 2.3433e-06 - val_MSE: 2.3433e-06\n",
      "Epoch 72/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1481e-06 - MSE: 3.1481e-06 - val_loss: 2.3422e-06 - val_MSE: 2.3422e-06\n",
      "Epoch 73/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1480e-06 - MSE: 3.1480e-06 - val_loss: 2.3410e-06 - val_MSE: 2.3410e-06\n",
      "Epoch 74/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1475e-06 - MSE: 3.1475e-06 - val_loss: 2.3409e-06 - val_MSE: 2.3409e-06\n",
      "Epoch 75/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1484e-06 - MSE: 3.1484e-06 - val_loss: 2.3417e-06 - val_MSE: 2.3417e-06\n",
      "Epoch 76/100\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 3.1461e-06 - MSE: 3.1461e-06 - val_loss: 2.3405e-06 - val_MSE: 2.3405e-06\n",
      "Epoch 77/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1472e-06 - MSE: 3.1472e-06 - val_loss: 2.3410e-06 - val_MSE: 2.3410e-06\n",
      "Epoch 78/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1473e-06 - MSE: 3.1473e-06 - val_loss: 2.3405e-06 - val_MSE: 2.3405e-06\n",
      "Epoch 79/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1446e-06 - MSE: 3.1446e-06 - val_loss: 2.3405e-06 - val_MSE: 2.3405e-06\n",
      "Epoch 80/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1478e-06 - MSE: 3.1478e-06 - val_loss: 2.3400e-06 - val_MSE: 2.3400e-06\n",
      "Epoch 81/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1472e-06 - MSE: 3.1472e-06 - val_loss: 2.3413e-06 - val_MSE: 2.3413e-06\n",
      "Epoch 82/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1459e-06 - MSE: 3.1459e-06 - val_loss: 2.3399e-06 - val_MSE: 2.3399e-06\n",
      "Epoch 83/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1470e-06 - MSE: 3.1470e-06 - val_loss: 2.3409e-06 - val_MSE: 2.3409e-06\n",
      "Epoch 84/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1490e-06 - MSE: 3.1490e-06 - val_loss: 2.3413e-06 - val_MSE: 2.3413e-06\n",
      "Epoch 85/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1471e-06 - MSE: 3.1471e-06 - val_loss: 2.3413e-06 - val_MSE: 2.3413e-06\n",
      "Epoch 86/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1456e-06 - MSE: 3.1456e-06 - val_loss: 2.3408e-06 - val_MSE: 2.3408e-06\n",
      "Epoch 87/100\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 3.1441e-06 - MSE: 3.1441e-06 - val_loss: 2.3404e-06 - val_MSE: 2.3404e-06\n",
      "INFO:tensorflow:Assets written to: saved_model/STLD_120_log_s302_sigmoid_4layers\\assets\n",
      "Dataset STLD has been trained and saved.\n"
     ]
    }
   ],
   "source": [
    "STRIDE = (30, 2)    # stride for feature block\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "CALLBACKS = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "all_loss = list()\n",
    "for name, train_dt, valid_dt in load_dataset(TRAIN_DIR, VALID_DIR):\n",
    "    feature = alphanet_feature_Block(STRIDE)\n",
    "    print(train_dt.element_spec[0].shape)\n",
    "    print(train_dt.element_spec[1].shape)\n",
    "    model = build_AlphaNet(\n",
    "        feature, \n",
    "        input_shape=tuple(train_dt.element_spec[0].shape), \n",
    "        output_shape=tuple(train_dt.element_spec[1].shape)[0]\n",
    "    )\n",
    "    train_dt = train_dt.batch(BATCH_SIZE)\n",
    "    valid_dt = valid_dt.batch(BATCH_SIZE)\n",
    "    history = model.fit(\n",
    "        train_dt,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=valid_dt,\n",
    "        callbacks=CALLBACKS,\n",
    "        shuffle=True,\n",
    "        use_multiprocessing=True,\n",
    "        workers=10\n",
    "    )\n",
    "    all_loss.append({\n",
    "        \"stock\": name,\n",
    "        \"epoch\": len(history.history[\"loss\"]),\n",
    "        \"train_loss\": history.history[\"loss\"][-1],\n",
    "        \"valid_loss\": history.history[\"val_loss\"][-1]\n",
    "    })\n",
    "    model.save(f\"{SAVE_DIR}/{name}_120_log_s302_sigmoid_4layers\")\n",
    "    print(f\"Dataset {name} has been trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4Bm_yzw7pVDB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1666277042056,
     "user": {
      "displayName": "Junhao Huang",
      "userId": "04740354632925395357"
     },
     "user_tz": -480
    },
    "id": "4Bm_yzw7pVDB",
    "outputId": "e44cce44-1361-4ec0-8a83-b4814d78e8cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  stock  epoch  train_loss    valid_loss\n0   LMT     71    0.000001  6.829630e-07\n1   NOC     67    0.000002  9.637770e-07\n2   HII     86    0.000002  2.065070e-06\n3   BAH     50    0.000002  2.149203e-06\n4  STLD     87    0.000003  2.340352e-06\n5   MLI    100    0.000004  2.783340e-06\n6    AR     55    0.000006  4.615395e-06\n7   NFE    100    0.000007  4.798133e-06\n8   PBR     52    0.000003  5.336437e-06\n9   FTI     44    0.000007  5.718620e-06",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stock</th>\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LMT</td>\n      <td>71</td>\n      <td>0.000001</td>\n      <td>6.829630e-07</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NOC</td>\n      <td>67</td>\n      <td>0.000002</td>\n      <td>9.637770e-07</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HII</td>\n      <td>86</td>\n      <td>0.000002</td>\n      <td>2.065070e-06</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>BAH</td>\n      <td>50</td>\n      <td>0.000002</td>\n      <td>2.149203e-06</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>STLD</td>\n      <td>87</td>\n      <td>0.000003</td>\n      <td>2.340352e-06</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>MLI</td>\n      <td>100</td>\n      <td>0.000004</td>\n      <td>2.783340e-06</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>AR</td>\n      <td>55</td>\n      <td>0.000006</td>\n      <td>4.615395e-06</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NFE</td>\n      <td>100</td>\n      <td>0.000007</td>\n      <td>4.798133e-06</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>PBR</td>\n      <td>52</td>\n      <td>0.000003</td>\n      <td>5.336437e-06</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>FTI</td>\n      <td>44</td>\n      <td>0.000007</td>\n      <td>5.718620e-06</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.DataFrame(all_loss).sort_values(\"valid_loss\").reset_index(drop=True)\n",
    "temp.to_csv(f\"{OUTPUT_DIR}/all_loss_120_log_s302_sigmoid_4layers.csv\")\n",
    "temp"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "31b9f103-0a81-4cc1-8e8c-226d40f546cf",
    "98797082-b05f-4949-97e3-02b52bb03d79",
    "cyfLh4DI-5it"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "99ea4bddf300909f6d049ff7715ecae0b1516d9d48b79ec655d91803e59750be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}